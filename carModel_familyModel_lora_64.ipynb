{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["ce3840a016b4442bb52d42a11a4be97f","9f96f9fce7f84fa7910a2ba3cb8a1bdb","5be8606eb49040ee9327d91894dfa678","9d6d9a7423224785808b00d625b93daf","98e17435921c4d889c5d31d65695bc20","fd72554b5d8d489db8ab24925478e4ea","7fb9c41cc3f24f75b030392f6afd2c16","0376b90aec5948aa8810b76c2cff3fbd","83bd9ab2c79c49178c3c54d1ae9fb585","5e78460f61404e57811d2b4b515e0c07","c5bab406a62e4ec59773adbd4d0052f2","8ab8cf9cd23f429c9c2bcb24376b612b","e7f216f1893c4e53a3e387cdf86bb8f3","2d77e9986f86453baa63c9dffd22393e","c49f4d627fda4e1ab69292a0b0895a6b","a7b2f77000c5447284cac89dc6fd2936","d5432dc3f2374345b55c68fbe32fdf45","c832875e93344b7dbe2da02210671949","00a747c024c54472858b380e81a97b50","f27bc90580884f658d1e42c58aea52dd","e6e9f922e52e4c39b54b8f7ae2810529","2b35389507774dab8d7e5fe21948d72b","523e485f33a74a8aabcb02dcd6b72291","4571ed87e0ab49c3882fc8f619ab49ae","223bd8de45a849eca8de2886fa559c04","db1a34d2d94b4be2805d7e69ac525b9a","095e84efdc3e4bc49954bb5736a88432","0f1810c30baa4dd680f285fb677120aa","e06176592b584b9c900ab4beec18fb1d","4a399876ed7e41d7ba150b327f2e4512","e2bae5fe023c49128552755de5ba6bcf","9ec0f3718e5e47bd9b97898072c54292"]},"executionInfo":{"elapsed":997,"status":"ok","timestamp":1724762487143,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"dHdC8iQ1TFgW","outputId":"9cb4d3bc-ad9f-4cd2-a062-fcf50a212185"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce3840a016b4442bb52d42a11a4be97f","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WC9CIrZ7QTHz"},"outputs":[],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28678,"status":"ok","timestamp":1724762530515,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"1lByNcV2PTGL","outputId":"12fdf622-198b-49d1-d381-4f3227b3dfa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Collecting peft\n","  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n","Collecting datasets\n","  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.42.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, tensorboardX, pyarrow, dill, multiprocess, datasets, peft\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 peft-0.12.0 pyarrow-17.0.0 tensorboardX-2.6.2.2 xxhash-3.5.0\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%pip install tensorboardX peft datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbhGG-a720oB"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"tTVZU5M36PQK"},"source":["# to check"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1724762552793,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"EoGDifO6QwXZ","outputId":"baf2f0a5-04f3-4ed0-fb89-6a7a0e94e85d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage/gemma_sft\n"]}],"source":["%cd ./drive/MyDrive/'Colab Notebooks'/'Identity membership Leakage'/gemma_sft"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":790,"status":"ok","timestamp":1724762562432,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"A0dr1EYE5l7E","outputId":"29a09fd1-a4a5-4d38-9131-2a67a132b19c"},"outputs":[{"name":"stdout","output_type":"stream","text":["LORA_R = 32 # 16\n","('model.embed_tokens.weight', torch.bfloat16, True)\n","('model.layers.17.self_attn.q_proj.weight', torch.bfloat16, True)\n","('model.layers.17.self_attn.k_proj.weight', torch.bfloat16, True)\n","('model.layers.17.self_attn.v_proj.weight', torch.bfloat16, True)\n","('model.layers.17.self_attn.o_proj.weight', torch.bfloat16, True)\n","('model.layers.17.mlp.gate_proj.weight', torch.bfloat16, True)\n","('model.layers.17.mlp.up_proj.weight', torch.bfloat16, True)\n","('model.layers.17.mlp.down_proj.weight', torch.bfloat16, True)\n","('model.layers.17.input_layernorm.weight', torch.bfloat16, True)\n","('model.layers.17.post_attention_layernorm.weight', torch.bfloat16, True)\n","('model.norm.weight', torch.bfloat16, True)\n","trainable params: 2506172416 || all params: 2506172416 || trainable%: 100.0\n"]}],"source":["%cat ./ft_gemma/config.py | grep 16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njmwo1thRtOg"},"outputs":[],"source":["%ls -ahl\n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tbs3zAMpVp1H"},"outputs":[],"source":["%cd ..\n","%cd ./gemma_sft"]},{"cell_type":"markdown","metadata":{"id":"jDhfh26nB08F"},"source":["# Car K1 acc"]},{"cell_type":"markdown","metadata":{"id":"zKJDOHTwB08H"},"source":["### Predict on M1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBSf7Uq84Um1"},"outputs":[],"source":["!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-20\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_20.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-40\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_40.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-60\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_60.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-80\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_80.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-100\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_100.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-120\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_120.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-140\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_140.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-160\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_160.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-180\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_180.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-200\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_200.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-220\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_220.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-240\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_240.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-260\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_260.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-280\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_280.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"./model7b_M1_car_lalala/checkpoint-300\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_300.txt\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":363824,"status":"ok","timestamp":1724476822366,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"pe1D-pwVzBpA","outputId":"86795961-9888-4699-cec7-4aa69360a1fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n","model-00002-of-00004.safetensors:   3% 136M/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 147M/4.98G [00:00<05:33, 14.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 157M/4.98G [00:01<04:26, 18.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 168M/4.98G [00:01<04:04, 19.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 178M/4.98G [00:02<03:53, 20.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 189M/4.98G [00:02<03:47, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 199M/4.98G [00:03<03:43, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 210M/4.98G [00:03<03:41, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 220M/4.98G [00:04<03:39, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 231M/4.98G [00:04<03:38, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 241M/4.98G [00:05<03:37, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 252M/4.98G [00:05<03:36, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 262M/4.98G [00:05<03:35, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 273M/4.98G [00:06<03:34, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 283M/4.98G [00:06<03:35, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 294M/4.98G [00:07<03:33, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 304M/4.98G [00:07<03:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 315M/4.98G [00:08<03:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 325M/4.98G [00:08<03:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 336M/4.98G [00:09<03:31, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 346M/4.98G [00:09<03:30, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 357M/4.98G [00:10<03:30, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 367M/4.98G [00:10<03:30, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 377M/4.98G [00:11<03:29, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 388M/4.98G [00:11<03:29, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 398M/4.98G [00:12<03:32, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 409M/4.98G [00:12<03:31, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 419M/4.98G [00:13<03:29, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 430M/4.98G [00:13<03:29, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 440M/4.98G [00:14<03:28, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 451M/4.98G [00:14<03:27, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 461M/4.98G [00:15<03:26, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 472M/4.98G [00:15<03:47, 19.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 482M/4.98G [00:16<03:40, 20.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 493M/4.98G [00:16<03:35, 20.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 503M/4.98G [00:17<03:31, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 514M/4.98G [00:17<03:29, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 524M/4.98G [00:18<03:27, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 535M/4.98G [00:18<03:25, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 545M/4.98G [00:19<03:24, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 556M/4.98G [00:19<03:23, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 566M/4.98G [00:20<03:22, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 577M/4.98G [00:20<03:21, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 587M/4.98G [00:20<03:21, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 598M/4.98G [00:21<03:20, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 608M/4.98G [00:21<03:19, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 619M/4.98G [00:22<03:18, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 629M/4.98G [00:23<03:39, 19.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 640M/4.98G [00:23<03:11, 22.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 650M/4.98G [00:23<03:13, 22.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 661M/4.98G [00:24<03:13, 22.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 671M/4.98G [00:24<03:15, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 682M/4.98G [00:25<03:14, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 692M/4.98G [00:25<03:14, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 703M/4.98G [00:26<03:14, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 713M/4.98G [00:26<03:16, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 724M/4.98G [00:27<03:16, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 734M/4.98G [00:27<03:15, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 744M/4.98G [00:28<03:14, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 755M/4.98G [00:28<03:13, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 765M/4.98G [00:29<03:13, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 776M/4.98G [00:29<03:14, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 786M/4.98G [00:30<03:30, 20.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 797M/4.98G [00:30<03:23, 20.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 807M/4.98G [00:31<03:19, 20.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 818M/4.98G [00:31<03:16, 21.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 828M/4.98G [00:32<03:13, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 839M/4.98G [00:32<03:11, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 849M/4.98G [00:33<03:10, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 860M/4.98G [00:33<03:09, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 870M/4.98G [00:34<03:08, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 881M/4.98G [00:34<03:07, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 891M/4.98G [00:35<03:06, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 902M/4.98G [00:35<03:06, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 912M/4.98G [00:36<03:06, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 923M/4.98G [00:36<03:05, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 933M/4.98G [00:36<03:04, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 944M/4.98G [00:37<03:04, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 954M/4.98G [00:37<03:05, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 965M/4.98G [00:38<03:04, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 975M/4.98G [00:38<03:03, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 986M/4.98G [00:39<03:02, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 996M/4.98G [00:39<03:02, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 1.01G/4.98G [00:40<03:01, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 1.02G/4.98G [00:40<03:01, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.03G/4.98G [00:41<03:02, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.04G/4.98G [00:41<02:59, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.05G/4.98G [00:42<02:59, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.06G/4.98G [00:42<02:58, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.07G/4.98G [00:43<02:58, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.08G/4.98G [00:43<02:59, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.09G/4.98G [00:44<02:58, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.10G/4.98G [00:44<03:00, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.11G/4.98G [00:45<02:59, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.12G/4.98G [00:45<02:58, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.13G/4.98G [00:46<02:57, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.14G/4.98G [00:46<02:56, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.15G/4.98G [00:47<02:55, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.16G/4.98G [00:47<02:54, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.17G/4.98G [00:48<02:54, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.18G/4.98G [00:48<03:07, 20.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.20G/4.98G [00:49<03:02, 20.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.21G/4.98G [00:49<02:59, 21.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.22G/4.98G [00:50<02:57, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.23G/4.98G [00:50<02:55, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.24G/4.98G [00:51<02:53, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.25G/4.98G [00:51<02:52, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.26G/4.98G [00:51<02:51, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.27G/4.98G [00:52<02:50, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.28G/4.98G [00:52<02:49, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.29G/4.98G [00:53<02:49, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.30G/4.98G [00:53<02:48, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.31G/4.98G [00:54<02:48, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.32G/4.98G [00:54<02:48, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.33G/4.98G [00:55<02:47, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.34G/4.98G [00:55<02:47, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.35G/4.98G [00:56<02:46, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.36G/4.98G [00:56<02:45, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.37G/4.98G [00:57<02:45, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.38G/4.98G [00:57<02:44, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.39G/4.98G [00:58<02:43, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.41G/4.98G [00:58<02:43, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.42G/4.98G [00:59<02:42, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.43G/4.98G [00:59<02:42, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.44G/4.98G [01:00<02:44, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.45G/4.98G [01:00<02:43, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.46G/4.98G [01:01<02:42, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.47G/4.98G [01:01<02:41, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.48G/4.98G [01:02<02:41, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.49G/4.98G [01:02<02:40, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.50G/4.98G [01:03<02:39, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.51G/4.98G [01:03<02:38, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.52G/4.98G [01:03<02:38, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.53G/4.98G [01:04<02:37, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.54G/4.98G [01:04<02:37, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.55G/4.98G [01:05<02:47, 20.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.56G/4.98G [01:06<02:44, 20.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.57G/4.98G [01:06<02:41, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.58G/4.98G [01:06<02:39, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.59G/4.98G [01:07<02:38, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.60G/4.98G [01:08<02:43, 20.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.61G/4.98G [01:08<02:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.63G/4.98G [01:08<02:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.64G/4.98G [01:09<02:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.65G/4.98G [01:09<02:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.66G/4.98G [01:10<02:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.67G/4.98G [01:10<02:31, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.68G/4.98G [01:11<02:31, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.69G/4.98G [01:11<02:30, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.70G/4.98G [01:12<02:30, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.71G/4.98G [01:12<02:30, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.72G/4.98G [01:13<02:29, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.73G/4.98G [01:13<02:29, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.74G/4.98G [01:14<02:28, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.75G/4.98G [01:14<02:27, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.76G/4.98G [01:15<02:27, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.77G/4.98G [01:15<02:27, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.78G/4.98G [01:16<02:27, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.79G/4.98G [01:16<02:27, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.80G/4.98G [01:17<02:26, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.81G/4.98G [01:17<02:25, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.82G/4.98G [01:18<02:25, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.84G/4.98G [01:18<02:24, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.85G/4.98G [01:19<02:24, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.86G/4.98G [01:19<02:24, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.87G/4.98G [01:20<02:27, 21.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.88G/4.98G [01:20<02:38, 19.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.89G/4.98G [01:21<02:22, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.90G/4.98G [01:21<02:23, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.91G/4.98G [01:22<02:25, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.92G/4.98G [01:22<02:23, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.93G/4.98G [01:22<02:22, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.94G/4.98G [01:23<02:20, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.95G/4.98G [01:23<02:20, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.96G/4.98G [01:24<02:18, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 1.97G/4.98G [01:24<02:18, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 1.98G/4.98G [01:25<02:17, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 1.99G/4.98G [01:25<02:16, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 2.00G/4.98G [01:26<02:16, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 2.01G/4.98G [01:26<02:15, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.02G/4.98G [01:27<02:15, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.03G/4.98G [01:27<02:14, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.04G/4.98G [01:28<02:16, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.06G/4.98G [01:28<02:16, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.07G/4.98G [01:29<02:15, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.08G/4.98G [01:29<02:14, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.09G/4.98G [01:30<02:13, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.10G/4.98G [01:30<02:12, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.11G/4.98G [01:31<02:11, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.12G/4.98G [01:31<02:11, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.13G/4.98G [01:32<02:10, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.14G/4.98G [01:32<02:09, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.15G/4.98G [01:33<02:11, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.16G/4.98G [01:33<02:10, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.17G/4.98G [01:34<02:09, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.18G/4.98G [01:34<02:08, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.19G/4.98G [01:35<02:07, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.20G/4.98G [01:35<02:07, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.21G/4.98G [01:35<02:07, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.22G/4.98G [01:36<02:09, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.23G/4.98G [01:36<02:08, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.24G/4.98G [01:37<02:06, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.25G/4.98G [01:37<02:05, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.26G/4.98G [01:38<02:05, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.28G/4.98G [01:38<02:04, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.29G/4.98G [01:39<02:03, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.30G/4.98G [01:39<02:05, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.31G/4.98G [01:40<02:05, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.32G/4.98G [01:40<02:03, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.33G/4.98G [01:41<02:03, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.34G/4.98G [01:41<02:02, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.35G/4.98G [01:42<02:01, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.36G/4.98G [01:42<02:02, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.37G/4.98G [01:43<02:01, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.38G/4.98G [01:43<02:00, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.39G/4.98G [01:44<01:59, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.40G/4.98G [01:44<01:59, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.41G/4.98G [01:45<02:17, 18.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.42G/4.98G [01:45<02:11, 19.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.43G/4.98G [01:46<02:06, 20.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.44G/4.98G [01:46<02:02, 20.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.45G/4.98G [01:47<02:00, 21.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.46G/4.98G [01:47<01:59, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.47G/4.98G [01:48<01:57, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.49G/4.98G [01:48<01:55, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.50G/4.98G [01:49<02:11, 19.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.51G/4.98G [01:50<02:05, 19.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.52G/4.98G [01:50<02:01, 20.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.53G/4.98G [01:51<02:15, 18.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.54G/4.98G [01:51<02:24, 17.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.55G/4.98G [01:52<02:29, 16.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.56G/4.98G [01:53<02:33, 15.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.57G/4.98G [01:53<02:20, 17.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.58G/4.98G [01:54<02:26, 16.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.59G/4.98G [01:55<02:15, 17.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.60G/4.98G [01:55<02:22, 16.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.61G/4.98G [01:56<02:12, 17.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.62G/4.98G [01:56<02:04, 18.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.63G/4.98G [01:57<02:14, 17.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.64G/4.98G [01:57<02:06, 18.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.65G/4.98G [01:58<02:14, 17.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.66G/4.98G [01:59<02:06, 18.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.67G/4.98G [01:59<02:00, 19.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.68G/4.98G [02:00<02:09, 17.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.69G/4.98G [02:00<02:02, 18.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.71G/4.98G [02:01<01:56, 19.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.72G/4.98G [02:01<02:07, 17.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.73G/4.98G [02:02<01:59, 18.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.74G/4.98G [02:02<01:54, 19.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.75G/4.98G [02:03<02:04, 17.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.76G/4.98G [02:04<01:57, 18.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.77G/4.98G [02:04<01:53, 19.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.78G/4.98G [02:05<02:02, 18.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.79G/4.98G [02:05<01:55, 18.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.80G/4.98G [02:06<01:54, 19.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.81G/4.98G [02:06<01:59, 18.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.82G/4.98G [02:07<01:53, 19.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.83G/4.98G [02:07<01:48, 19.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.84G/4.98G [02:08<01:58, 18.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.85G/4.98G [02:09<01:51, 19.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.86G/4.98G [02:09<01:47, 19.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.87G/4.98G [02:10<01:43, 20.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.88G/4.98G [02:10<01:54, 18.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.89G/4.98G [02:11<01:48, 19.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.90G/4.98G [02:11<01:44, 20.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.92G/4.98G [02:12<01:40, 20.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.93G/4.98G [02:12<01:38, 20.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.94G/4.98G [02:13<01:36, 21.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.95G/4.98G [02:13<01:35, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.96G/4.98G [02:14<01:35, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 2.97G/4.98G [02:14<01:32, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 2.98G/4.98G [02:15<01:31, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 2.99G/4.98G [02:15<01:30, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 3.00G/4.98G [02:16<01:30, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 3.01G/4.98G [02:16<01:29, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.02G/4.98G [02:17<01:29, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.03G/4.98G [02:17<01:28, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.04G/4.98G [02:17<01:28, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.05G/4.98G [02:18<01:27, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.06G/4.98G [02:18<01:27, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.07G/4.98G [02:19<01:27, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.08G/4.98G [02:19<01:26, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.09G/4.98G [02:20<01:26, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.10G/4.98G [02:20<01:25, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.11G/4.98G [02:21<01:25, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.12G/4.98G [02:21<01:24, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.14G/4.98G [02:22<01:24, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.15G/4.98G [02:22<01:23, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.16G/4.98G [02:23<01:23, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.17G/4.98G [02:23<01:22, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.18G/4.98G [02:24<01:23, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.19G/4.98G [02:24<01:24, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.20G/4.98G [02:25<01:22, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.21G/4.98G [02:25<01:22, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.22G/4.98G [02:26<01:21, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.23G/4.98G [02:26<01:20, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.24G/4.98G [02:27<01:21, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.25G/4.98G [02:27<01:20, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.26G/4.98G [02:28<01:23, 20.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.27G/4.98G [02:28<01:21, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.28G/4.98G [02:29<01:20, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.29G/4.98G [02:29<01:19, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.30G/4.98G [02:30<01:18, 21.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.31G/4.98G [02:30<01:17, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.32G/4.98G [02:31<01:16, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.33G/4.98G [02:31<01:15, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.34G/4.98G [02:32<01:15, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.36G/4.98G [02:32<01:14, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.37G/4.98G [02:32<01:13, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.38G/4.98G [02:33<01:13, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.39G/4.98G [02:33<01:12, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.40G/4.98G [02:34<01:12, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.41G/4.98G [02:34<01:12, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.42G/4.98G [02:35<01:11, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.43G/4.98G [02:35<01:10, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.44G/4.98G [02:36<01:10, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.45G/4.98G [02:36<01:09, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.46G/4.98G [02:37<01:09, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.47G/4.98G [02:37<01:09, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.48G/4.98G [02:38<01:08, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.49G/4.98G [02:38<01:07, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.50G/4.98G [02:39<01:07, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.51G/4.98G [02:39<01:06, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.52G/4.98G [02:40<01:06, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.53G/4.98G [02:40<01:06, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.54G/4.98G [02:41<01:05, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.55G/4.98G [02:41<01:04, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.57G/4.98G [02:42<01:04, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.58G/4.98G [02:42<01:04, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.59G/4.98G [02:43<01:03, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.60G/4.98G [02:43<01:02, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.61G/4.98G [02:43<01:02, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.62G/4.98G [02:44<01:02, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.63G/4.98G [02:44<01:01, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.64G/4.98G [02:45<01:01, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.65G/4.98G [02:45<01:00, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.66G/4.98G [02:46<01:00, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.67G/4.98G [02:46<00:59, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.68G/4.98G [02:47<00:59, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.69G/4.98G [02:47<00:58, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.70G/4.98G [02:48<00:58, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.71G/4.98G [02:48<00:57, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.72G/4.98G [02:49<00:57, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.73G/4.98G [02:49<00:56, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.74G/4.98G [02:50<00:56, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.75G/4.98G [02:50<00:55, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.76G/4.98G [02:51<00:55, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.77G/4.98G [02:51<00:54, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.79G/4.98G [02:52<00:54, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.80G/4.98G [02:52<00:54, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.81G/4.98G [02:53<00:53, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.82G/4.98G [02:53<00:53, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.83G/4.98G [02:53<00:52, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.84G/4.98G [02:54<00:52, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.85G/4.98G [02:54<00:51, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.86G/4.98G [02:55<00:52, 21.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.87G/4.98G [02:55<00:50, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.88G/4.98G [02:56<00:50, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.89G/4.98G [02:56<00:49, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.90G/4.98G [02:57<00:49, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.91G/4.98G [02:57<00:48, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.92G/4.98G [02:58<00:48, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.93G/4.98G [02:58<00:47, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.94G/4.98G [02:59<00:47, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.95G/4.98G [02:59<00:46, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 3.96G/4.98G [03:00<00:46, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 3.97G/4.98G [03:00<00:45, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 3.98G/4.98G [03:01<00:45, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 4.00G/4.98G [03:01<00:45, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 4.01G/4.98G [03:02<00:44, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.02G/4.98G [03:02<00:44, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.03G/4.98G [03:03<00:44, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.04G/4.98G [03:03<00:43, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.05G/4.98G [03:04<00:44, 21.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.06G/4.98G [03:04<00:44, 20.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.07G/4.98G [03:05<00:43, 21.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.08G/4.98G [03:05<00:42, 21.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.09G/4.98G [03:06<00:41, 21.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.10G/4.98G [03:06<00:41, 21.5MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.11G/4.98G [03:07<00:40, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.12G/4.98G [03:07<00:39, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.13G/4.98G [03:08<00:39, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.14G/4.98G [03:08<00:38, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.15G/4.98G [03:08<00:38, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.16G/4.98G [03:09<00:37, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.17G/4.98G [03:09<00:37, 21.6MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.18G/4.98G [03:10<00:36, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.19G/4.98G [03:10<00:36, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.20G/4.98G [03:11<00:35, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.22G/4.98G [03:11<00:35, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.23G/4.98G [03:12<00:34, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.24G/4.98G [03:12<00:34, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.25G/4.98G [03:13<00:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.26G/4.98G [03:13<00:33, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.27G/4.98G [03:14<00:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.28G/4.98G [03:14<00:32, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.29G/4.98G [03:15<00:31, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.30G/4.98G [03:15<00:31, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.31G/4.98G [03:16<00:30, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.32G/4.98G [03:16<00:30, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.33G/4.98G [03:17<00:29, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.34G/4.98G [03:17<00:29, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.35G/4.98G [03:18<00:28, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.36G/4.98G [03:18<00:28, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.37G/4.98G [03:19<00:27, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.38G/4.98G [03:19<00:27, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.39G/4.98G [03:19<00:26, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.40G/4.98G [03:20<00:26, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.41G/4.98G [03:20<00:25, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.42G/4.98G [03:21<00:25, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.44G/4.98G [03:21<00:24, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.45G/4.98G [03:22<00:24, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.46G/4.98G [03:22<00:24, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.47G/4.98G [03:23<00:23, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.48G/4.98G [03:23<00:23, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.49G/4.98G [03:24<00:22, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.50G/4.98G [03:24<00:22, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.51G/4.98G [03:25<00:21, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.52G/4.98G [03:25<00:21, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.53G/4.98G [03:26<00:20, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.54G/4.98G [03:26<00:20, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.55G/4.98G [03:27<00:19, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.56G/4.98G [03:27<00:19, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.57G/4.98G [03:28<00:18, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.58G/4.98G [03:28<00:18, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.59G/4.98G [03:29<00:17, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.60G/4.98G [03:29<00:17, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.61G/4.98G [03:30<00:16, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.62G/4.98G [03:30<00:16, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.63G/4.98G [03:30<00:15, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.65G/4.98G [03:31<00:15, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.66G/4.98G [03:31<00:14, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.67G/4.98G [03:32<00:14, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.68G/4.98G [03:32<00:13, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.69G/4.98G [03:33<00:13, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.70G/4.98G [03:33<00:12, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.71G/4.98G [03:34<00:12, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.72G/4.98G [03:34<00:12, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.73G/4.98G [03:35<00:11, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.74G/4.98G [03:35<00:11, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.75G/4.98G [03:36<00:10, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.76G/4.98G [03:36<00:10, 21.8MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.77G/4.98G [03:37<00:09, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.78G/4.98G [03:37<00:09, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.79G/4.98G [03:38<00:08, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.80G/4.98G [03:38<00:08, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.81G/4.98G [03:39<00:07, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.82G/4.98G [03:39<00:07, 22.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.83G/4.98G [03:40<00:06, 22.2MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.84G/4.98G [03:40<00:06, 22.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.85G/4.98G [03:40<00:05, 22.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.87G/4.98G [03:41<00:05, 22.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.88G/4.98G [03:41<00:04, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.89G/4.98G [03:42<00:04, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.90G/4.98G [03:42<00:03, 21.7MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.91G/4.98G [03:43<00:03, 21.9MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  99% 4.92G/4.98G [03:43<00:02, 22.0MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  99% 4.93G/4.98G [03:44<00:02, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  99% 4.94G/4.98G [03:44<00:01, 22.1MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  99% 4.95G/4.98G [03:45<00:01, 22.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 4.96G/4.98G [03:45<00:01, 22.4MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 4.97G/4.98G [03:46<00:00, 22.3MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 4.98G/4.98G [03:46<00:00, 21.4MB/s]\n","Downloading shards:  50% 2/4 [03:47<03:47, 113.77s/it]\n","model-00003-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00003-of-00004.safetensors:   1% 31.5M/4.98G [00:00<00:18, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   1% 62.9M/4.98G [00:00<00:17, 277MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   2% 94.4M/4.98G [00:00<00:18, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   3% 126M/4.98G [00:00<00:17, 271MB/s] \u001b[A\n","model-00003-of-00004.safetensors:   3% 157M/4.98G [00:00<00:17, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   4% 189M/4.98G [00:00<00:17, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   4% 220M/4.98G [00:00<00:18, 255MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   5% 252M/4.98G [00:00<00:18, 250MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   6% 283M/4.98G [00:01<00:18, 253MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   6% 315M/4.98G [00:01<00:18, 258MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   7% 346M/4.98G [00:01<00:18, 252MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   8% 377M/4.98G [00:01<00:18, 255MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   8% 419M/4.98G [00:01<00:16, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   9% 451M/4.98G [00:01<00:17, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  10% 482M/4.98G [00:01<00:16, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  10% 514M/4.98G [00:01<00:17, 253MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  11% 545M/4.98G [00:02<00:19, 229MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  12% 577M/4.98G [00:02<00:21, 207MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  12% 608M/4.98G [00:02<00:20, 209MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  13% 640M/4.98G [00:02<00:22, 189MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  13% 661M/4.98G [00:02<00:23, 182MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  14% 692M/4.98G [00:02<00:21, 198MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  15% 724M/4.98G [00:03<00:21, 196MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  15% 755M/4.98G [00:03<00:29, 143MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  16% 786M/4.98G [00:03<00:26, 161MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  16% 818M/4.98G [00:03<00:22, 183MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  17% 849M/4.98G [00:03<00:20, 199MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  18% 881M/4.98G [00:03<00:20, 201MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  18% 912M/4.98G [00:04<00:18, 219MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  19% 944M/4.98G [00:04<00:18, 220MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  20% 975M/4.98G [00:04<00:17, 231MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  20% 1.01G/4.98G [00:04<00:16, 235MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  21% 1.04G/4.98G [00:04<00:17, 232MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  21% 1.07G/4.98G [00:04<00:17, 225MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  22% 1.10G/4.98G [00:04<00:18, 206MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  23% 1.13G/4.98G [00:05<00:17, 215MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  23% 1.16G/4.98G [00:05<00:16, 226MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  24% 1.20G/4.98G [00:05<00:16, 236MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  25% 1.23G/4.98G [00:05<00:16, 228MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  25% 1.26G/4.98G [00:05<00:16, 228MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  26% 1.29G/4.98G [00:05<00:15, 240MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  27% 1.32G/4.98G [00:05<00:15, 231MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  27% 1.35G/4.98G [00:06<00:16, 224MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  28% 1.38G/4.98G [00:06<00:16, 212MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  28% 1.42G/4.98G [00:06<00:15, 227MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  29% 1.45G/4.98G [00:06<00:15, 229MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  30% 1.48G/4.98G [00:06<00:15, 232MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  30% 1.51G/4.98G [00:06<00:14, 238MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  31% 1.54G/4.98G [00:06<00:14, 230MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  32% 1.57G/4.98G [00:06<00:14, 241MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  32% 1.60G/4.98G [00:07<00:14, 240MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  33% 1.64G/4.98G [00:07<00:14, 225MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  33% 1.67G/4.98G [00:07<00:15, 215MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  34% 1.70G/4.98G [00:07<00:14, 229MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  35% 1.73G/4.98G [00:07<00:13, 243MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  35% 1.76G/4.98G [00:07<00:13, 247MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  36% 1.79G/4.98G [00:07<00:13, 236MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  37% 1.82G/4.98G [00:08<00:12, 249MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  37% 1.86G/4.98G [00:08<00:12, 246MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  38% 1.89G/4.98G [00:08<00:12, 254MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  39% 1.92G/4.98G [00:08<00:12, 248MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  39% 1.95G/4.98G [00:08<00:11, 259MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  40% 1.98G/4.98G [00:08<00:10, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  40% 2.01G/4.98G [00:08<00:11, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  41% 2.04G/4.98G [00:08<00:11, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  42% 2.08G/4.98G [00:09<00:11, 251MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  42% 2.11G/4.98G [00:09<00:11, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  43% 2.14G/4.98G [00:09<00:10, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  44% 2.17G/4.98G [00:09<00:10, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  44% 2.20G/4.98G [00:09<00:10, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  45% 2.23G/4.98G [00:09<00:11, 248MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  46% 2.28G/4.98G [00:09<00:10, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  46% 2.31G/4.98G [00:09<00:10, 256MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  47% 2.34G/4.98G [00:10<00:10, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  48% 2.37G/4.98G [00:10<00:12, 206MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  48% 2.40G/4.98G [00:10<00:15, 167MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.42G/4.98G [00:10<00:20, 125MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.44G/4.98G [00:11<00:26, 95.6MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.46G/4.98G [00:11<00:26, 93.4MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  50% 2.49G/4.98G [00:11<00:29, 84.2MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  50% 2.50G/4.98G [00:11<00:33, 74.9MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  50% 2.51G/4.98G [00:12<00:37, 66.4MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.54G/4.98G [00:12<00:27, 88.4MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.55G/4.98G [00:12<00:29, 81.8MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.56G/4.98G [00:12<00:32, 75.0MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.57G/4.98G [00:13<00:36, 65.8MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.59G/4.98G [00:13<00:34, 69.7MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.60G/4.98G [00:13<00:33, 71.8MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  53% 2.62G/4.98G [00:13<00:29, 79.8MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  53% 2.63G/4.98G [00:13<00:32, 72.2MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  53% 2.64G/4.98G [00:13<00:31, 73.3MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  54% 2.67G/4.98G [00:14<00:19, 119MB/s] \u001b[A\n","model-00003-of-00004.safetensors:  54% 2.69G/4.98G [00:14<00:17, 134MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  55% 2.73G/4.98G [00:14<00:13, 164MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  55% 2.76G/4.98G [00:14<00:11, 186MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  56% 2.78G/4.98G [00:14<00:15, 138MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  57% 2.82G/4.98G [00:14<00:11, 186MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  57% 2.85G/4.98G [00:14<00:10, 200MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  58% 2.88G/4.98G [00:15<00:09, 219MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  59% 2.92G/4.98G [00:15<00:08, 234MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  59% 2.95G/4.98G [00:15<00:08, 243MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  60% 2.98G/4.98G [00:15<00:08, 239MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  60% 3.01G/4.98G [00:15<00:08, 238MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  61% 3.04G/4.98G [00:15<00:07, 249MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  62% 3.08G/4.98G [00:15<00:06, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  62% 3.11G/4.98G [00:15<00:06, 270MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  63% 3.15G/4.98G [00:16<00:07, 250MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  64% 3.18G/4.98G [00:16<00:06, 258MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  64% 3.21G/4.98G [00:16<00:06, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  65% 3.24G/4.98G [00:16<00:06, 259MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  66% 3.27G/4.98G [00:16<00:06, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  66% 3.30G/4.98G [00:16<00:06, 255MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  67% 3.33G/4.98G [00:16<00:06, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.38G/4.98G [00:16<00:05, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.41G/4.98G [00:17<00:05, 279MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  69% 3.45G/4.98G [00:17<00:05, 291MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  70% 3.48G/4.98G [00:17<00:05, 293MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  70% 3.51G/4.98G [00:17<00:05, 289MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  71% 3.54G/4.98G [00:17<00:05, 270MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  72% 3.58G/4.98G [00:17<00:05, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  72% 3.61G/4.98G [00:17<00:04, 286MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  73% 3.64G/4.98G [00:17<00:04, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.67G/4.98G [00:18<00:04, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.70G/4.98G [00:18<00:05, 255MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  75% 3.73G/4.98G [00:18<00:04, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  76% 3.76G/4.98G [00:18<00:04, 250MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  76% 3.80G/4.98G [00:18<00:05, 235MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  77% 3.83G/4.98G [00:18<00:05, 229MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  78% 3.87G/4.98G [00:18<00:04, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  78% 3.90G/4.98G [00:18<00:04, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  79% 3.93G/4.98G [00:19<00:03, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  80% 3.96G/4.98G [00:19<00:03, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  80% 4.00G/4.98G [00:19<00:03, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  81% 4.03G/4.98G [00:19<00:03, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  81% 4.06G/4.98G [00:19<00:03, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  82% 4.09G/4.98G [00:19<00:03, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  83% 4.12G/4.98G [00:19<00:03, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  84% 4.16G/4.98G [00:19<00:02, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  84% 4.19G/4.98G [00:20<00:02, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  85% 4.23G/4.98G [00:20<00:03, 251MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  85% 4.26G/4.98G [00:20<00:03, 229MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  86% 4.29G/4.98G [00:20<00:03, 221MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  87% 4.32G/4.98G [00:20<00:02, 226MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  87% 4.35G/4.98G [00:20<00:02, 240MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  88% 4.38G/4.98G [00:20<00:02, 251MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  89% 4.41G/4.98G [00:20<00:02, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  89% 4.45G/4.98G [00:21<00:02, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  90% 4.48G/4.98G [00:21<00:01, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  90% 4.51G/4.98G [00:21<00:01, 252MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  91% 4.54G/4.98G [00:21<00:01, 246MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  92% 4.57G/4.98G [00:21<00:01, 249MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  92% 4.60G/4.98G [00:21<00:01, 223MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  93% 4.63G/4.98G [00:21<00:01, 233MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  94% 4.67G/4.98G [00:22<00:01, 239MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  94% 4.70G/4.98G [00:22<00:01, 233MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  95% 4.73G/4.98G [00:22<00:01, 250MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  96% 4.76G/4.98G [00:22<00:00, 232MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  96% 4.79G/4.98G [00:22<00:00, 241MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.82G/4.98G [00:22<00:00, 245MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.85G/4.98G [00:22<00:00, 240MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  98% 4.89G/4.98G [00:22<00:00, 243MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  99% 4.92G/4.98G [00:23<00:00, 230MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  99% 4.95G/4.98G [00:23<00:00, 241MB/s]\u001b[A\n","model-00003-of-00004.safetensors: 100% 4.98G/4.98G [00:23<00:00, 214MB/s]\n","Downloading shards:  75% 3/4 [04:11<01:16, 76.36s/it] \n","model-00004-of-00004.safetensors:   0% 0.00/2.11G [00:00<?, ?B/s]\u001b[A\n","model-00004-of-00004.safetensors:   1% 21.0M/2.11G [00:00<00:11, 179MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   2% 52.4M/2.11G [00:00<00:09, 220MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   4% 83.9M/2.11G [00:00<00:10, 201MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   5% 115M/2.11G [00:00<00:09, 221MB/s] \u001b[A\n","model-00004-of-00004.safetensors:   7% 147M/2.11G [00:00<00:09, 213MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   8% 178M/2.11G [00:00<00:08, 221MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  10% 210M/2.11G [00:00<00:08, 219MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  11% 241M/2.11G [00:01<00:07, 240MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  13% 273M/2.11G [00:01<00:07, 255MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  14% 304M/2.11G [00:01<00:07, 248MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  16% 336M/2.11G [00:01<00:07, 254MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  17% 367M/2.11G [00:01<00:06, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  19% 398M/2.11G [00:01<00:07, 237MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  20% 430M/2.11G [00:01<00:08, 201MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  22% 461M/2.11G [00:02<00:09, 167MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  23% 482M/2.11G [00:02<00:10, 153MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  24% 503M/2.11G [00:02<00:10, 155MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  25% 524M/2.11G [00:02<00:10, 149MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  26% 556M/2.11G [00:02<00:08, 177MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  28% 587M/2.11G [00:02<00:07, 198MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  29% 619M/2.11G [00:03<00:07, 213MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  31% 650M/2.11G [00:03<00:06, 228MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  32% 682M/2.11G [00:03<00:06, 238MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  34% 713M/2.11G [00:03<00:05, 251MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  35% 744M/2.11G [00:03<00:05, 255MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  37% 776M/2.11G [00:03<00:05, 264MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  38% 807M/2.11G [00:03<00:05, 252MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  40% 839M/2.11G [00:03<00:05, 234MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  41% 870M/2.11G [00:04<00:05, 240MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  43% 902M/2.11G [00:04<00:04, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  44% 933M/2.11G [00:04<00:04, 251MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  46% 965M/2.11G [00:04<00:04, 244MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  47% 996M/2.11G [00:04<00:04, 254MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  49% 1.03G/2.11G [00:04<00:04, 242MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  50% 1.06G/2.11G [00:04<00:04, 246MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  52% 1.09G/2.11G [00:04<00:04, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  53% 1.12G/2.11G [00:05<00:04, 235MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  55% 1.15G/2.11G [00:05<00:04, 233MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  56% 1.18G/2.11G [00:05<00:04, 217MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  58% 1.22G/2.11G [00:05<00:04, 221MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  59% 1.25G/2.11G [00:05<00:03, 222MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  61% 1.28G/2.11G [00:05<00:03, 236MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  62% 1.31G/2.11G [00:05<00:03, 216MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  63% 1.34G/2.11G [00:06<00:03, 228MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  65% 1.37G/2.11G [00:06<00:03, 233MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  66% 1.41G/2.11G [00:06<00:02, 251MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  68% 1.44G/2.11G [00:06<00:02, 263MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  69% 1.47G/2.11G [00:06<00:02, 268MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  71% 1.50G/2.11G [00:06<00:02, 249MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  73% 1.54G/2.11G [00:06<00:02, 259MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  74% 1.57G/2.11G [00:06<00:02, 243MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  76% 1.60G/2.11G [00:07<00:02, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  77% 1.64G/2.11G [00:07<00:02, 236MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  79% 1.67G/2.11G [00:07<00:01, 241MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  80% 1.70G/2.11G [00:07<00:01, 255MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  82% 1.73G/2.11G [00:07<00:01, 262MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  83% 1.76G/2.11G [00:07<00:01, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  85% 1.79G/2.11G [00:07<00:01, 278MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  86% 1.82G/2.11G [00:07<00:01, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  88% 1.86G/2.11G [00:08<00:01, 244MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  89% 1.89G/2.11G [00:08<00:00, 239MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  91% 1.92G/2.11G [00:08<00:00, 249MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  92% 1.95G/2.11G [00:08<00:00, 251MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  94% 1.98G/2.11G [00:08<00:00, 258MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  95% 2.01G/2.11G [00:08<00:00, 254MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  97% 2.04G/2.11G [00:08<00:00, 258MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  98% 2.08G/2.11G [00:08<00:00, 259MB/s]\u001b[A\n","model-00004-of-00004.safetensors: 100% 2.11G/2.11G [00:09<00:00, 234MB/s]\n","Downloading shards: 100% 4/4 [04:21<00:00, 65.28s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","generation_config.json: 100% 137/137 [00:00<00:00, 772kB/s]\n","tokenizer_config.json: 100% 34.2k/34.2k [00:00<00:00, 97.4MB/s]\n","tokenizer.model: 100% 4.24M/4.24M [00:03<00:00, 1.24MB/s]\n","special_tokens_map.json: 100% 636/636 [00:00<00:00, 4.32MB/s]\n","tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 127MB/s]\n"]}],"source":["!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-200\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_200.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":452887,"status":"ok","timestamp":1724477275227,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"P4TZdmz9ayWn","outputId":"915676f7-8bf9-44ec-93a1-52514a881980"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.42s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n"]}],"source":["# !python ./ft_gemma/p.py \\\n","#     --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-200\" \\\n","#     --data_dir './dataset/car_k1_q.txt' \\\n","#     --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_200.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-220\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_220.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-240\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_240.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-260\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_260.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-280\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_280.txt\"\n","\n","!python ./ft_gemma/p.py \\\n","    --model_dir \"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-300\" \\\n","    --data_dir './dataset/car_k1_q.txt' \\\n","    --q_num 3 --LORA_R 16 > \"./Results/M1_car_k1_lalala/k1_checkpoint_300.txt\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOETWomLxpWq"},"outputs":[],"source":["# checkpoint_values = [200, 220, 240, 260, 280, 300]\n","\n","# for checkpoint in checkpoint_values:\n","#     model_dir = f\"../../7b_k1_car_lalala/checkpoint200_and_after___Finetune/checkpoint-{checkpoint}\"\n","#     output_file = f\"./Results/M1_car_k1_lalala/k1_checkpoint_{checkpoint}.txt\"\n","\n","#     command = f\"!python ./ft_gemma/p.py --model_dir \\\"{model_dir}\\\" --data_dir './dataset/car_k1_q.txt' --q_num 3 --LORA_R 16 > \\\"{output_file}\\\"\"\n","#     exec(command)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Az2g91avQBZo"},"outputs":[],"source":["# %mkdir ./Results/M2_family_raw/\n","%cp ./Results/M1_car_k1_lalala/* ./Results/raw_M1_car_k1_lalala/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDz_Ry2O4yaa"},"outputs":[],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 15 \\\n","    --json_path \"./dataset/car.json\" \\\n","    --output_dir \"model7b_M1_car_epoch_15_r_64_Aug24\" \\\n","    --save_steps 20 \\\n","    --LORA_R 64"]},{"cell_type":"markdown","metadata":{"id":"6INSCXIzQ54f"},"source":["# Family"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohm1HIRF4YAq"},"outputs":[],"source":["!chmod a+x *.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":246136,"status":"ok","timestamp":1724763224673,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"_6ZUvqkt4drC","outputId":"1761ea10-23ed-4a20-b5e7-1f10f5835f0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","auto\n","False\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.00s/it]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 80/80 [00:00<00:00, 2157.29 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","  0% 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 39.8563, 'lr': 0.0, 'epoch': 0}\n","  0% 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 39.8563, 'grad_norm': 8.281993865966797, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.03}\n","{'loss': 39.3322, 'lr': 2.0000000000000003e-06, 'epoch': 0.03}\n","{'loss': 39.0934, 'lr': 4.000000000000001e-06, 'epoch': 0.05}\n","{'loss': 40.298, 'lr': 6e-06, 'epoch': 0.07}\n","{'loss': 40.7427, 'lr': 8.000000000000001e-06, 'epoch': 0.1}\n","{'loss': 39.3803, 'lr': 1e-05, 'epoch': 0.12}\n","{'loss': 36.2828, 'lr': 9.999960960438441e-06, 'epoch': 0.15}\n","{'loss': 39.5356, 'lr': 9.999843842363402e-06, 'epoch': 0.17}\n","{'loss': 39.2378, 'grad_norm': 7.925758361816406, 'learning_rate': 9.999648647603774e-06, 'epoch': 0.2}\n","{'loss': 37.619, 'lr': 9.999648647603774e-06, 'epoch': 0.2}\n","{'loss': 38.026, 'lr': 9.99937537920769e-06, 'epoch': 0.23}\n","{'loss': 40.4563, 'lr': 9.999024041442455e-06, 'epoch': 0.25}\n","{'loss': 39.3348, 'lr': 9.998594639794502e-06, 'epoch': 0.28}\n","{'loss': 39.1914, 'lr': 9.99808718096929e-06, 'epoch': 0.3}\n","{'loss': 38.2851, 'lr': 9.997501672891208e-06, 'epoch': 0.33}\n","{'loss': 37.0182, 'lr': 9.996838124703448e-06, 'epoch': 0.35}\n","{'loss': 40.9532, 'lr': 9.99609654676786e-06, 'epoch': 0.38}\n","{'loss': 38.8605, 'grad_norm': 9.834935188293457, 'learning_rate': 9.995276950664796e-06, 'epoch': 0.4}\n","{'loss': 37.6626, 'lr': 9.995276950664796e-06, 'epoch': 0.4}\n","{'loss': 38.1029, 'lr': 9.994379349192927e-06, 'epoch': 0.42}\n","{'loss': 34.9803, 'lr': 9.993403756369037e-06, 'epoch': 0.45}\n","{'loss': 36.0758, 'lr': 9.992350187427814e-06, 'epoch': 0.47}\n","{'loss': 36.6943, 'lr': 9.991218658821609e-06, 'epoch': 0.5}\n","{'loss': 34.1056, 'lr': 9.990009188220166e-06, 'epoch': 0.53}\n","{'loss': 37.6435, 'lr': 9.988721794510374e-06, 'epoch': 0.55}\n","{'loss': 36.2499, 'lr': 9.987356497795944e-06, 'epoch': 0.57}\n","{'loss': 36.4394, 'grad_norm': 8.367817878723145, 'learning_rate': 9.98591331939711e-06, 'epoch': 0.6}\n","{'loss': 36.8518, 'lr': 9.98591331939711e-06, 'epoch': 0.6}\n","{'loss': 37.2657, 'lr': 9.984392281850293e-06, 'epoch': 0.62}\n","{'loss': 34.387, 'lr': 9.982793408907747e-06, 'epoch': 0.65}\n","{'loss': 37.8744, 'lr': 9.981116725537195e-06, 'epoch': 0.68}\n","{'loss': 34.1852, 'lr': 9.979362257921428e-06, 'epoch': 0.7}\n","{'loss': 34.2166, 'lr': 9.977530033457906e-06, 'epoch': 0.72}\n","{'loss': 35.2316, 'lr': 9.975620080758321e-06, 'epoch': 0.75}\n","{'loss': 34.623, 'lr': 9.973632429648165e-06, 'epoch': 0.78}\n","{'loss': 35.5794, 'grad_norm': 9.314720153808594, 'learning_rate': 9.971567111166246e-06, 'epoch': 0.8}\n","{'loss': 35.7262, 'lr': 9.971567111166246e-06, 'epoch': 0.8}\n","{'loss': 35.1234, 'lr': 9.969424157564215e-06, 'epoch': 0.82}\n","{'loss': 37.5872, 'lr': 9.967203602306062e-06, 'epoch': 0.85}\n","{'loss': 35.1008, 'lr': 9.964905480067585e-06, 'epoch': 0.88}\n","{'loss': 32.423, 'lr': 9.96252982673586e-06, 'epoch': 0.9}\n","{'loss': 32.9294, 'lr': 9.960076679408675e-06, 'epoch': 0.93}\n","{'loss': 35.4425, 'lr': 9.957546076393944e-06, 'epoch': 0.95}\n","{'loss': 32.3245, 'lr': 9.954938057209121e-06, 'epoch': 0.97}\n","{'loss': 34.5821, 'grad_norm': 9.4991455078125, 'learning_rate': 9.95225266258058e-06, 'epoch': 1.0}\n","  5% 40/800 [00:10<02:54,  4.36it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 33.0939, 'lr': 9.95225266258058e-06, 'epoch': 1.0}\n","  5% 40/800 [00:12<02:54,  4.36it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.3926, 'lr': 9.949489934442966e-06, 'epoch': 1.02}\n","{'loss': 33.2486, 'lr': 9.946649915938562e-06, 'epoch': 1.05}\n","{'loss': 35.8376, 'lr': 9.943732651416597e-06, 'epoch': 1.07}\n","{'loss': 33.2071, 'lr': 9.940738186432565e-06, 'epoch': 1.1}\n","{'loss': 34.0617, 'lr': 9.9376665677475e-06, 'epoch': 1.12}\n","{'loss': 32.8626, 'lr': 9.934517843327269e-06, 'epoch': 1.15}\n","{'loss': 33.8697, 'lr': 9.931292062341793e-06, 'epoch': 1.18}\n","{'loss': 33.8217, 'grad_norm': 11.01050090789795, 'learning_rate': 9.927989275164305e-06, 'epoch': 1.2}\n","{'loss': 33.1947, 'lr': 9.927989275164305e-06, 'epoch': 1.2}\n","{'loss': 34.618, 'lr': 9.924609533370551e-06, 'epoch': 1.23}\n","{'loss': 32.3045, 'lr': 9.921152889737985e-06, 'epoch': 1.25}\n","{'loss': 31.5264, 'lr': 9.91761939824495e-06, 'epoch': 1.27}\n","{'loss': 34.9483, 'lr': 9.914009114069824e-06, 'epoch': 1.3}\n","{'loss': 31.8215, 'lr': 9.910322093590177e-06, 'epoch': 1.32}\n","{'loss': 34.9019, 'lr': 9.906558394381872e-06, 'epoch': 1.35}\n","{'loss': 35.3339, 'lr': 9.902718075218176e-06, 'epoch': 1.38}\n","{'loss': 33.5811, 'grad_norm': 13.079681396484375, 'learning_rate': 9.898801196068839e-06, 'epoch': 1.4}\n","{'loss': 32.7543, 'lr': 9.898801196068839e-06, 'epoch': 1.4}\n","{'loss': 30.8579, 'lr': 9.89480781809916e-06, 'epoch': 1.43}\n","{'loss': 34.6013, 'lr': 9.890738003669029e-06, 'epoch': 1.45}\n","{'loss': 31.6473, 'lr': 9.886591816331953e-06, 'epoch': 1.48}\n","{'loss': 32.7934, 'lr': 9.882369320834068e-06, 'epoch': 1.5}\n","{'loss': 32.6759, 'lr': 9.878070583113123e-06, 'epoch': 1.52}\n","{'loss': 32.9043, 'lr': 9.87369567029745e-06, 'epoch': 1.55}\n","{'loss': 31.8239, 'lr': 9.869244650704924e-06, 'epoch': 1.57}\n","{'loss': 32.5073, 'grad_norm': 11.120809555053711, 'learning_rate': 9.864717593841884e-06, 'epoch': 1.6}\n","{'loss': 30.2115, 'lr': 9.864717593841884e-06, 'epoch': 1.6}\n","{'loss': 32.9745, 'lr': 9.860114570402055e-06, 'epoch': 1.62}\n","{'loss': 31.3147, 'lr': 9.855435652265446e-06, 'epoch': 1.65}\n","{'loss': 30.5305, 'lr': 9.85068091249722e-06, 'epoch': 1.68}\n","{'loss': 30.7003, 'lr': 9.845850425346563e-06, 'epoch': 1.7}\n","{'loss': 29.1114, 'lr': 9.840944266245511e-06, 'epoch': 1.73}\n","{'loss': 31.6908, 'lr': 9.835962511807786e-06, 'epoch': 1.75}\n","{'loss': 29.9303, 'lr': 9.830905239827592e-06, 'epoch': 1.77}\n","{'loss': 30.808, 'grad_norm': 11.442794799804688, 'learning_rate': 9.825772529278402e-06, 'epoch': 1.8}\n","{'loss': 30.1038, 'lr': 9.825772529278402e-06, 'epoch': 1.8}\n","{'loss': 31.2992, 'lr': 9.820564460311719e-06, 'epoch': 1.82}\n","{'loss': 29.6663, 'lr': 9.815281114255841e-06, 'epoch': 1.85}\n","{'loss': 30.2514, 'lr': 9.80992257361457e-06, 'epoch': 1.88}\n","{'loss': 31.1161, 'lr': 9.804488922065937e-06, 'epoch': 1.9}\n","{'loss': 29.3415, 'lr': 9.798980244460892e-06, 'epoch': 1.93}\n","{'loss': 33.7424, 'lr': 9.79339662682198e-06, 'epoch': 1.95}\n","{'loss': 30.8797, 'lr': 9.787738156341992e-06, 'epoch': 1.98}\n","{'loss': 30.8, 'grad_norm': 11.375741958618164, 'learning_rate': 9.782004921382612e-06, 'epoch': 2.0}\n"," 10% 80/800 [00:21<02:51,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 31.1443, 'lr': 9.782004921382612e-06, 'epoch': 2.0}\n"," 10% 80/800 [00:23<02:51,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 29.6466, 'lr': 9.776197011473034e-06, 'epoch': 2.02}\n","{'loss': 30.3092, 'lr': 9.770314517308554e-06, 'epoch': 2.05}\n","{'loss': 30.0702, 'lr': 9.764357530749178e-06, 'epoch': 2.08}\n","{'loss': 32.2825, 'lr': 9.758326144818155e-06, 'epoch': 2.1}\n","{'loss': 29.6187, 'lr': 9.752220453700556e-06, 'epoch': 2.12}\n","{'loss': 29.463, 'lr': 9.74604055274178e-06, 'epoch': 2.15}\n","{'loss': 27.4318, 'lr': 9.739786538446076e-06, 'epoch': 2.17}\n","{'loss': 29.9958, 'grad_norm': 11.756962776184082, 'learning_rate': 9.733458508475038e-06, 'epoch': 2.2}\n","{'loss': 27.7789, 'lr': 9.733458508475038e-06, 'epoch': 2.2}\n","{'loss': 28.5897, 'lr': 9.727056561646067e-06, 'epoch': 2.23}\n","{'loss': 30.0716, 'lr': 9.720580797930845e-06, 'epoch': 2.25}\n","{'loss': 27.5054, 'lr': 9.714031318453763e-06, 'epoch': 2.27}\n","{'loss': 30.389, 'lr': 9.707408225490343e-06, 'epoch': 2.3}\n","{'loss': 28.8181, 'lr': 9.700711622465645e-06, 'epoch': 2.33}\n","{'loss': 29.0687, 'lr': 9.693941613952642e-06, 'epoch': 2.35}\n","{'loss': 28.1897, 'lr': 9.687098305670606e-06, 'epoch': 2.38}\n","{'loss': 28.8014, 'grad_norm': 11.694671630859375, 'learning_rate': 9.680181804483435e-06, 'epoch': 2.4}\n","{'loss': 29.266, 'lr': 9.680181804483435e-06, 'epoch': 2.4}\n","{'loss': 27.9464, 'lr': 9.673192218398e-06, 'epoch': 2.42}\n","{'loss': 29.2743, 'lr': 9.66612965656245e-06, 'epoch': 2.45}\n","{'loss': 26.9676, 'lr': 9.658994229264514e-06, 'epoch': 2.48}\n","{'loss': 27.9686, 'lr': 9.651786047929772e-06, 'epoch': 2.5}\n","{'loss': 27.5788, 'lr': 9.644505225119922e-06, 'epoch': 2.52}\n","{'loss': 30.1101, 'lr': 9.637151874531014e-06, 'epoch': 2.55}\n","{'loss': 28.4152, 'lr': 9.62972611099168e-06, 'epoch': 2.58}\n","{'loss': 28.4409, 'grad_norm': 12.712346076965332, 'learning_rate': 9.622228050461345e-06, 'epoch': 2.6}\n","{'loss': 27.4501, 'lr': 9.622228050461345e-06, 'epoch': 2.6}\n","{'loss': 26.9072, 'lr': 9.614657810028402e-06, 'epoch': 2.62}\n","{'loss': 26.3595, 'lr': 9.607015507908401e-06, 'epoch': 2.65}\n","{'loss': 28.1624, 'lr': 9.599301263442194e-06, 'epoch': 2.67}\n","{'loss': 28.262, 'lr': 9.591515197094064e-06, 'epoch': 2.7}\n","{'loss': 27.125, 'lr': 9.583657430449862e-06, 'epoch': 2.73}\n","{'loss': 29.2692, 'lr': 9.575728086215093e-06, 'epoch': 2.75}\n","{'loss': 27.9468, 'lr': 9.567727288213005e-06, 'epoch': 2.77}\n","{'loss': 27.6853, 'grad_norm': 13.015012741088867, 'learning_rate': 9.559655161382658e-06, 'epoch': 2.8}\n","{'loss': 28.0679, 'lr': 9.559655161382658e-06, 'epoch': 2.8}\n","{'loss': 27.76, 'lr': 9.551511831776966e-06, 'epoch': 2.83}\n","{'loss': 26.8855, 'lr': 9.54329742656074e-06, 'epoch': 2.85}\n","{'loss': 26.2919, 'lr': 9.535012074008688e-06, 'epoch': 2.88}\n","{'loss': 25.7122, 'lr': 9.526655903503423e-06, 'epoch': 2.9}\n","{'loss': 27.3845, 'lr': 9.518229045533438e-06, 'epoch': 2.92}\n","{'loss': 24.2419, 'lr': 9.509731631691071e-06, 'epoch': 2.95}\n","{'loss': 25.964, 'lr': 9.501163794670445e-06, 'epoch': 2.98}\n","{'loss': 26.5385, 'grad_norm': 13.636940002441406, 'learning_rate': 9.4925256682654e-06, 'epoch': 3.0}\n"," 15% 120/800 [00:32<02:36,  4.33it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 26.7501, 'lr': 9.4925256682654e-06, 'epoch': 3.0}\n"," 15% 120/800 [00:35<02:36,  4.33it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 26.3871, 'lr': 9.483817387367403e-06, 'epoch': 3.02}\n","{'loss': 26.7574, 'lr': 9.475039087963443e-06, 'epoch': 3.05}\n","{'loss': 25.1492, 'lr': 9.466190907133901e-06, 'epoch': 3.08}\n","{'loss': 25.5626, 'lr': 9.457272983050421e-06, 'epoch': 3.1}\n","{'loss': 24.12, 'lr': 9.448285454973739e-06, 'epoch': 3.12}\n","{'loss': 24.2194, 'lr': 9.439228463251515e-06, 'epoch': 3.15}\n","{'loss': 26.1601, 'lr': 9.430102149316147e-06, 'epoch': 3.17}\n","{'loss': 25.6382, 'grad_norm': 14.735003471374512, 'learning_rate': 9.420906655682553e-06, 'epoch': 3.2}\n","{'loss': 25.7129, 'lr': 9.420906655682553e-06, 'epoch': 3.2}\n","{'loss': 26.1001, 'lr': 9.411642125945945e-06, 'epoch': 3.23}\n","{'loss': 25.6113, 'lr': 9.4023087047796e-06, 'epoch': 3.25}\n","{'loss': 27.1071, 'lr': 9.392906537932582e-06, 'epoch': 3.27}\n","{'loss': 25.7161, 'lr': 9.383435772227481e-06, 'epoch': 3.3}\n","{'loss': 25.6422, 'lr': 9.373896555558113e-06, 'epoch': 3.33}\n","{'loss': 23.9373, 'lr': 9.364289036887214e-06, 'epoch': 3.35}\n","{'loss': 24.6432, 'lr': 9.354613366244108e-06, 'epoch': 3.38}\n","{'loss': 25.5588, 'grad_norm': 14.801020622253418, 'learning_rate': 9.344869694722372e-06, 'epoch': 3.4}\n","{'loss': 25.0703, 'lr': 9.344869694722372e-06, 'epoch': 3.4}\n","{'loss': 26.8501, 'lr': 9.335058174477472e-06, 'epoch': 3.42}\n","{'loss': 24.4898, 'lr': 9.325178958724387e-06, 'epoch': 3.45}\n","{'loss': 25.3514, 'lr': 9.315232201735217e-06, 'epoch': 3.48}\n","{'loss': 23.9565, 'lr': 9.305218058836778e-06, 'epoch': 3.5}\n","{'loss': 24.961, 'lr': 9.295136686408165e-06, 'epoch': 3.52}\n","{'loss': 24.6028, 'lr': 9.284988241878326e-06, 'epoch': 3.55}\n","{'loss': 24.1542, 'lr': 9.274772883723587e-06, 'epoch': 3.58}\n","{'loss': 24.9295, 'grad_norm': 15.792510032653809, 'learning_rate': 9.264490771465191e-06, 'epoch': 3.6}\n","{'loss': 23.6698, 'lr': 9.264490771465191e-06, 'epoch': 3.6}\n","{'loss': 23.7525, 'lr': 9.254142065666802e-06, 'epoch': 3.62}\n","{'loss': 24.8275, 'lr': 9.24372692793199e-06, 'epoch': 3.65}\n","{'loss': 23.1284, 'lr': 9.233245520901723e-06, 'epoch': 3.67}\n","{'loss': 24.3952, 'lr': 9.222698008251814e-06, 'epoch': 3.7}\n","{'loss': 23.1839, 'lr': 9.21208455469037e-06, 'epoch': 3.73}\n","{'loss': 24.2534, 'lr': 9.201405325955222e-06, 'epoch': 3.75}\n","{'loss': 22.8025, 'lr': 9.190660488811332e-06, 'epoch': 3.77}\n","{'loss': 23.7517, 'grad_norm': 16.464595794677734, 'learning_rate': 9.179850211048193e-06, 'epoch': 3.8}\n","{'loss': 25.1423, 'lr': 9.179850211048193e-06, 'epoch': 3.8}\n","{'loss': 23.5599, 'lr': 9.168974661477206e-06, 'epoch': 3.83}\n","{'loss': 23.5822, 'lr': 9.158034009929046e-06, 'epoch': 3.85}\n","{'loss': 22.2188, 'lr': 9.14702842725101e-06, 'epoch': 3.88}\n","{'loss': 22.5018, 'lr': 9.135958085304345e-06, 'epoch': 3.9}\n","{'loss': 23.8921, 'lr': 9.12482315696157e-06, 'epoch': 3.92}\n","{'loss': 23.0734, 'lr': 9.113623816103775e-06, 'epoch': 3.95}\n","{'loss': 23.6877, 'lr': 9.1023602376179e-06, 'epoch': 3.98}\n","{'loss': 23.4573, 'grad_norm': 17.131032943725586, 'learning_rate': 9.091032597394012e-06, 'epoch': 4.0}\n"," 20% 160/800 [00:45<02:32,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 23.6768, 'lr': 9.091032597394012e-06, 'epoch': 4.0}\n"," 20% 160/800 [00:46<02:32,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 22.1235, 'lr': 9.079641072322555e-06, 'epoch': 4.03}\n","{'loss': 23.5924, 'lr': 9.068185840291588e-06, 'epoch': 4.05}\n","{'loss': 22.1697, 'lr': 9.056667080184004e-06, 'epoch': 4.08}\n","{'loss': 24.9142, 'lr': 9.045084971874738e-06, 'epoch': 4.1}\n","{'loss': 23.3795, 'lr': 9.033439696227966e-06, 'epoch': 4.12}\n","{'loss': 22.6618, 'lr': 9.021731435094267e-06, 'epoch': 4.15}\n","{'loss': 22.6353, 'lr': 9.009960371307798e-06, 'epoch': 4.17}\n","{'loss': 23.1442, 'grad_norm': 16.858898162841797, 'learning_rate': 8.998126688683423e-06, 'epoch': 4.2}\n","{'loss': 22.8754, 'lr': 8.998126688683423e-06, 'epoch': 4.2}\n","{'loss': 22.5442, 'lr': 8.986230572013856e-06, 'epoch': 4.22}\n","{'loss': 21.6407, 'lr': 8.974272207066767e-06, 'epoch': 4.25}\n","{'loss': 21.0355, 'lr': 8.962251780581888e-06, 'epoch': 4.28}\n","{'loss': 22.2808, 'lr': 8.950169480268089e-06, 'epoch': 4.3}\n","{'loss': 21.3705, 'lr': 8.938025494800454e-06, 'epoch': 4.33}\n","{'loss': 21.3695, 'lr': 8.92582001381733e-06, 'epoch': 4.35}\n","{'loss': 21.9137, 'lr': 8.913553227917366e-06, 'epoch': 4.38}\n","{'loss': 21.8788, 'grad_norm': 17.59226417541504, 'learning_rate': 8.901225328656543e-06, 'epoch': 4.4}\n","{'loss': 20.9557, 'lr': 8.901225328656543e-06, 'epoch': 4.4}\n","{'loss': 21.842, 'lr': 8.888836508545172e-06, 'epoch': 4.42}\n","{'loss': 20.8559, 'lr': 8.876386961044892e-06, 'epoch': 4.45}\n","{'loss': 20.1805, 'lr': 8.863876880565656e-06, 'epoch': 4.47}\n","{'loss': 19.2505, 'lr': 8.851306462462689e-06, 'epoch': 4.5}\n","{'loss': 19.8702, 'lr': 8.838675903033432e-06, 'epoch': 4.53}\n","{'loss': 20.7428, 'lr': 8.825985399514488e-06, 'epoch': 4.55}\n","{'loss': 20.6184, 'lr': 8.813235150078532e-06, 'epoch': 4.58}\n","{'loss': 20.5395, 'grad_norm': 17.958650588989258, 'learning_rate': 8.800425353831227e-06, 'epoch': 4.6}\n","{'loss': 22.627, 'lr': 8.800425353831227e-06, 'epoch': 4.6}\n","{'loss': 21.3384, 'lr': 8.787556210808101e-06, 'epoch': 4.62}\n","{'loss': 18.8066, 'lr': 8.774627921971437e-06, 'epoch': 4.65}\n","{'loss': 20.0161, 'lr': 8.761640689207123e-06, 'epoch': 4.67}\n","{'loss': 20.9134, 'lr': 8.748594715321512e-06, 'epoch': 4.7}\n","{'loss': 19.4726, 'lr': 8.735490204038242e-06, 'epoch': 4.72}\n","{'loss': 19.1447, 'lr': 8.722327359995064e-06, 'epoch': 4.75}\n","{'loss': 18.9477, 'lr': 8.70910638874064e-06, 'epoch': 4.78}\n","{'loss': 20.1583, 'grad_norm': 17.136343002319336, 'learning_rate': 8.695827496731343e-06, 'epoch': 4.8}\n","{'loss': 20.1087, 'lr': 8.695827496731343e-06, 'epoch': 4.8}\n","{'loss': 18.8155, 'lr': 8.682490891328016e-06, 'epoch': 4.83}\n","{'loss': 19.2633, 'lr': 8.669096780792754e-06, 'epoch': 4.85}\n","{'loss': 18.5303, 'lr': 8.655645374285637e-06, 'epoch': 4.88}\n","{'loss': 20.0269, 'lr': 8.642136881861472e-06, 'epoch': 4.9}\n","{'loss': 18.7389, 'lr': 8.628571514466502e-06, 'epoch': 4.92}\n","{'loss': 19.0441, 'lr': 8.61494948393513e-06, 'epoch': 4.95}\n","{'loss': 19.6298, 'lr': 8.601271002986595e-06, 'epoch': 4.97}\n","{'loss': 19.2697, 'grad_norm': 17.781875610351562, 'learning_rate': 8.587536285221656e-06, 'epoch': 5.0}\n"," 25% 200/800 [00:55<02:18,  4.32it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 17.535, 'lr': 8.587536285221656e-06, 'epoch': 5.0}\n"," 25% 200/800 [00:57<02:18,  4.32it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 18.5019, 'lr': 8.573745545119258e-06, 'epoch': 5.03}\n","{'loss': 18.9238, 'lr': 8.559898998033177e-06, 'epoch': 5.05}\n","{'loss': 18.5394, 'lr': 8.545996860188668e-06, 'epoch': 5.08}\n","{'loss': 17.7757, 'lr': 8.532039348679074e-06, 'epoch': 5.1}\n","{'loss': 18.474, 'lr': 8.518026681462448e-06, 'epoch': 5.12}\n","{'loss': 18.0946, 'lr': 8.503959077358143e-06, 'epoch': 5.15}\n","{'loss': 17.8884, 'lr': 8.4898367560434e-06, 'epoch': 5.17}\n","{'loss': 18.2166, 'grad_norm': 18.665075302124023, 'learning_rate': 8.475659938049912e-06, 'epoch': 5.2}\n","{'loss': 16.6568, 'lr': 8.475659938049912e-06, 'epoch': 5.2}\n","{'loss': 17.0062, 'lr': 8.46142884476038e-06, 'epoch': 5.22}\n","{'loss': 17.9024, 'lr': 8.44714369840506e-06, 'epoch': 5.25}\n","{'loss': 17.2969, 'lr': 8.432804722058297e-06, 'epoch': 5.28}\n","{'loss': 17.2022, 'lr': 8.418412139635026e-06, 'epoch': 5.3}\n","{'loss': 18.1902, 'lr': 8.403966175887293e-06, 'epoch': 5.33}\n","{'loss': 17.4227, 'lr': 8.389467056400734e-06, 'epoch': 5.35}\n","{'loss': 18.65, 'lr': 8.374915007591053e-06, 'epoch': 5.38}\n","{'loss': 17.5409, 'grad_norm': 19.04367446899414, 'learning_rate': 8.360310256700496e-06, 'epoch': 5.4}\n","{'loss': 17.0463, 'lr': 8.360310256700496e-06, 'epoch': 5.4}\n","{'loss': 17.8819, 'lr': 8.345653031794292e-06, 'epoch': 5.42}\n","{'loss': 17.8246, 'lr': 8.330943561757092e-06, 'epoch': 5.45}\n","{'loss': 15.5475, 'lr': 8.3161820762894e-06, 'epoch': 5.47}\n","{'loss': 16.7467, 'lr': 8.301368805903988e-06, 'epoch': 5.5}\n","{'loss': 16.2346, 'lr': 8.286503981922284e-06, 'epoch': 5.53}\n","{'loss': 15.6031, 'lr': 8.271587836470775e-06, 'epoch': 5.55}\n","{'loss': 15.7327, 'lr': 8.256620602477372e-06, 'epoch': 5.58}\n","{'loss': 16.5772, 'grad_norm': 18.65423011779785, 'learning_rate': 8.241602513667775e-06, 'epoch': 5.6}\n","{'loss': 15.9055, 'lr': 8.241602513667775e-06, 'epoch': 5.6}\n","{'loss': 16.8411, 'lr': 8.226533804561828e-06, 'epoch': 5.62}\n","{'loss': 16.0466, 'lr': 8.211414710469844e-06, 'epoch': 5.65}\n","{'loss': 16.6176, 'lr': 8.19624546748895e-06, 'epoch': 5.67}\n","{'loss': 16.6629, 'lr': 8.181026312499383e-06, 'epoch': 5.7}\n","{'loss': 15.9591, 'lr': 8.165757483160798e-06, 'epoch': 5.72}\n","{'loss': 15.5271, 'lr': 8.150439217908557e-06, 'epoch': 5.75}\n","{'loss': 16.4692, 'lr': 8.13507175595e-06, 'epoch': 5.78}\n","{'loss': 16.2537, 'grad_norm': 21.786426544189453, 'learning_rate': 8.119655337260721e-06, 'epoch': 5.8}\n","{'loss': 15.6837, 'lr': 8.119655337260721e-06, 'epoch': 5.8}\n","{'loss': 16.0382, 'lr': 8.104190202580811e-06, 'epoch': 5.83}\n","{'loss': 14.956, 'lr': 8.0886765934111e-06, 'epoch': 5.85}\n","{'loss': 15.153, 'lr': 8.073114752009388e-06, 'epoch': 5.88}\n","{'loss': 14.7659, 'lr': 8.057504921386661e-06, 'epoch': 5.9}\n","{'loss': 15.2693, 'lr': 8.041847345303296e-06, 'epoch': 5.92}\n","{'loss': 15.0988, 'lr': 8.026142268265256e-06, 'epoch': 5.95}\n","{'loss': 14.7641, 'lr': 8.010389935520269e-06, 'epoch': 5.97}\n","{'loss': 15.2161, 'grad_norm': 18.64069366455078, 'learning_rate': 7.994590593054001e-06, 'epoch': 6.0}\n"," 30% 240/800 [01:06<02:25,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 15.127, 'lr': 7.994590593054001e-06, 'epoch': 6.0}\n"," 30% 240/800 [01:08<02:25,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 14.1447, 'lr': 7.978744487586214e-06, 'epoch': 6.03}\n","{'loss': 15.0695, 'lr': 7.962851866566912e-06, 'epoch': 6.05}\n","{'loss': 14.813, 'lr': 7.946912978172475e-06, 'epoch': 6.08}\n","{'loss': 14.7281, 'lr': 7.93092807130179e-06, 'epoch': 6.1}\n","{'loss': 14.3225, 'lr': 7.914897395572362e-06, 'epoch': 6.12}\n","{'loss': 15.6739, 'lr': 7.898821201316408e-06, 'epoch': 6.15}\n","{'loss': 14.7389, 'lr': 7.88269973957696e-06, 'epoch': 6.17}\n","{'loss': 14.8272, 'grad_norm': 18.60295295715332, 'learning_rate': 7.866533262103937e-06, 'epoch': 6.2}\n","{'loss': 14.6351, 'lr': 7.866533262103937e-06, 'epoch': 6.2}\n","{'loss': 13.9605, 'lr': 7.850322021350216e-06, 'epoch': 6.22}\n","{'loss': 14.0214, 'lr': 7.83406627046769e-06, 'epoch': 6.25}\n","{'loss': 13.8713, 'lr': 7.817766263303312e-06, 'epoch': 6.28}\n","{'loss': 14.4468, 'lr': 7.80142225439514e-06, 'epoch': 6.3}\n","{'loss': 13.8229, 'lr': 7.785034498968343e-06, 'epoch': 6.33}\n","{'loss': 13.4833, 'lr': 7.768603252931243e-06, 'epoch': 6.35}\n","{'loss': 13.3547, 'lr': 7.752128772871292e-06, 'epoch': 6.38}\n","{'loss': 13.9495, 'grad_norm': 18.907825469970703, 'learning_rate': 7.735611316051084e-06, 'epoch': 6.4}\n","{'loss': 13.6723, 'lr': 7.735611316051084e-06, 'epoch': 6.4}\n","{'loss': 13.3059, 'lr': 7.719051140404327e-06, 'epoch': 6.42}\n","{'loss': 13.1478, 'lr': 7.702448504531818e-06, 'epoch': 6.45}\n","{'loss': 13.3689, 'lr': 7.68580366769741e-06, 'epoch': 6.47}\n","{'loss': 13.9813, 'lr': 7.669116889823955e-06, 'epoch': 6.5}\n","{'loss': 12.8123, 'lr': 7.652388431489248e-06, 'epoch': 6.53}\n","{'loss': 12.4792, 'lr': 7.635618553921962e-06, 'epoch': 6.55}\n","{'loss': 12.6663, 'lr': 7.6188075189975644e-06, 'epoch': 6.58}\n","{'loss': 13.1792, 'grad_norm': 20.515949249267578, 'learning_rate': 7.601955589234227e-06, 'epoch': 6.6}\n","{'loss': 13.2321, 'lr': 7.601955589234227e-06, 'epoch': 6.6}\n","{'loss': 12.629, 'lr': 7.58506302778873e-06, 'epoch': 6.62}\n","{'loss': 13.705, 'lr': 7.568130098452352e-06, 'epoch': 6.65}\n","{'loss': 12.6462, 'lr': 7.551157065646747e-06, 'epoch': 6.67}\n","{'loss': 13.4459, 'lr': 7.534144194419817e-06, 'epoch': 6.7}\n","{'loss': 12.5057, 'lr': 7.517091750441576e-06, 'epoch': 6.72}\n","{'loss': 12.5493, 'lr': 7.500000000000001e-06, 'epoch': 6.75}\n","{'loss': 12.4414, 'lr': 7.482869209996867e-06, 'epoch': 6.78}\n","{'loss': 12.8943, 'grad_norm': 20.91059684753418, 'learning_rate': 7.465699647943586e-06, 'epoch': 6.8}\n","{'loss': 11.6983, 'lr': 7.465699647943586e-06, 'epoch': 6.8}\n","{'loss': 11.6791, 'lr': 7.44849158195703e-06, 'epoch': 6.83}\n","{'loss': 11.6756, 'lr': 7.4312452807553374e-06, 'epoch': 6.85}\n","{'loss': 11.9464, 'lr': 7.413961013653725e-06, 'epoch': 6.88}\n","{'loss': 11.6674, 'lr': 7.3966390505602755e-06, 'epoch': 6.9}\n","{'loss': 12.9571, 'lr': 7.379279661971728e-06, 'epoch': 6.92}\n","{'loss': 10.884, 'lr': 7.361883118969248e-06, 'epoch': 6.95}\n","{'loss': 10.7056, 'lr': 7.3444496932142e-06, 'epoch': 6.97}\n","{'loss': 11.6517, 'grad_norm': 18.539493560791016, 'learning_rate': 7.326979656943907e-06, 'epoch': 7.0}\n"," 35% 280/800 [01:17<02:08,  4.05it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 11.0244, 'lr': 7.326979656943907e-06, 'epoch': 7.0}\n"," 35% 280/800 [01:19<02:08,  4.05it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 10.9442, 'lr': 7.309473282967387e-06, 'epoch': 7.03}\n","{'loss': 11.332, 'lr': 7.291930844661109e-06, 'epoch': 7.05}\n","{'loss': 10.2934, 'lr': 7.274352615964713e-06, 'epoch': 7.08}\n","{'loss': 10.3045, 'lr': 7.256738871376733e-06, 'epoch': 7.1}\n","{'loss': 10.9245, 'lr': 7.239089885950317e-06, 'epoch': 7.12}\n","{'loss': 10.2297, 'lr': 7.2214059352889255e-06, 'epoch': 7.15}\n","{'loss': 10.4706, 'lr': 7.203687295542032e-06, 'epoch': 7.17}\n","{'loss': 10.6904, 'grad_norm': 19.106922149658203, 'learning_rate': 7.185934243400806e-06, 'epoch': 7.2}\n","{'loss': 10.8219, 'lr': 7.185934243400806e-06, 'epoch': 7.2}\n","{'loss': 10.2385, 'lr': 7.1681470560937975e-06, 'epoch': 7.22}\n","{'loss': 9.9628, 'lr': 7.1503260113826035e-06, 'epoch': 7.25}\n","{'loss': 10.2204, 'lr': 7.132471387557533e-06, 'epoch': 7.28}\n","{'loss': 9.6996, 'lr': 7.1145834634332585e-06, 'epoch': 7.3}\n","{'loss': 10.1722, 'lr': 7.09666251834447e-06, 'epoch': 7.33}\n","{'loss': 9.7493, 'lr': 7.078708832141497e-06, 'epoch': 7.35}\n","{'loss': 10.0783, 'lr': 7.060722685185961e-06, 'epoch': 7.38}\n","{'loss': 10.1179, 'grad_norm': 18.914018630981445, 'learning_rate': 7.042704358346375e-06, 'epoch': 7.4}\n","{'loss': 10.0653, 'lr': 7.042704358346375e-06, 'epoch': 7.4}\n","{'loss': 10.2459, 'lr': 7.024654132993772e-06, 'epoch': 7.42}\n","{'loss': 9.3989, 'lr': 7.006572290997304e-06, 'epoch': 7.45}\n","{'loss': 9.9165, 'lr': 6.988459114719849e-06, 'epoch': 7.47}\n","{'loss': 10.181, 'lr': 6.970314887013585e-06, 'epoch': 7.5}\n","{'loss': 9.1062, 'lr': 6.9521398912155935e-06, 'epoch': 7.53}\n","{'loss': 9.8707, 'lr': 6.933934411143419e-06, 'epoch': 7.55}\n","{'loss': 10.35, 'lr': 6.915698731090649e-06, 'epoch': 7.58}\n","{'loss': 9.8918, 'grad_norm': 20.139253616333008, 'learning_rate': 6.897433135822461e-06, 'epoch': 7.6}\n","{'loss': 9.7288, 'lr': 6.897433135822461e-06, 'epoch': 7.6}\n","{'loss': 9.875, 'lr': 6.879137910571191e-06, 'epoch': 7.62}\n","{'loss': 9.0819, 'lr': 6.860813341031867e-06, 'epoch': 7.65}\n","{'loss': 9.2661, 'lr': 6.842459713357752e-06, 'epoch': 7.67}\n","{'loss': 9.026, 'lr': 6.824077314155877e-06, 'epoch': 7.7}\n","{'loss': 9.3057, 'lr': 6.805666430482565e-06, 'epoch': 7.72}\n","{'loss': 8.6533, 'lr': 6.787227349838946e-06, 'epoch': 7.75}\n","{'loss': 8.5799, 'lr': 6.768760360166471e-06, 'epoch': 7.78}\n","{'loss': 9.1896, 'grad_norm': 20.6125545501709, 'learning_rate': 6.7502657498424096e-06, 'epoch': 7.8}\n","{'loss': 9.3105, 'lr': 6.7502657498424096e-06, 'epoch': 7.8}\n","{'loss': 8.4803, 'lr': 6.731743807675355e-06, 'epoch': 7.83}\n","{'loss': 8.4723, 'lr': 6.713194822900707e-06, 'epoch': 7.85}\n","{'loss': 8.8548, 'lr': 6.694619085176159e-06, 'epoch': 7.88}\n","{'loss': 7.991, 'lr': 6.676016884577173e-06, 'epoch': 7.9}\n","{'loss': 8.5024, 'lr': 6.657388511592453e-06, 'epoch': 7.92}\n","{'loss': 8.0197, 'lr': 6.638734257119402e-06, 'epoch': 7.95}\n","{'loss': 8.9368, 'lr': 6.620054412459588e-06, 'epoch': 7.97}\n","{'loss': 8.571, 'grad_norm': 18.44529914855957, 'learning_rate': 6.601349269314188e-06, 'epoch': 8.0}\n"," 40% 320/800 [01:28<02:00,  3.98it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 7.3789, 'lr': 6.601349269314188e-06, 'epoch': 8.0}\n"," 40% 320/800 [01:30<02:00,  3.98it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 7.922, 'lr': 6.582619119779439e-06, 'epoch': 8.03}\n","{'loss': 7.6828, 'lr': 6.56386425634207e-06, 'epoch': 8.05}\n","{'loss': 7.5804, 'lr': 6.545084971874738e-06, 'epoch': 8.07}\n","{'loss': 7.612, 'lr': 6.526281559631457e-06, 'epoch': 8.1}\n","{'loss': 6.8853, 'lr': 6.507454313243016e-06, 'epoch': 8.12}\n","{'loss': 7.4205, 'lr': 6.488603526712391e-06, 'epoch': 8.15}\n","{'loss': 8.104, 'lr': 6.4697294944101585e-06, 'epoch': 8.18}\n","{'loss': 7.5732, 'grad_norm': 20.367347717285156, 'learning_rate': 6.450832511069898e-06, 'epoch': 8.2}\n","{'loss': 8.273, 'lr': 6.450832511069898e-06, 'epoch': 8.2}\n","{'loss': 7.4581, 'lr': 6.431912871783587e-06, 'epoch': 8.22}\n","{'loss': 7.4017, 'lr': 6.412970871996995e-06, 'epoch': 8.25}\n","{'loss': 7.0934, 'lr': 6.394006807505068e-06, 'epoch': 8.28}\n","{'loss': 7.8253, 'lr': 6.3750209744473105e-06, 'epoch': 8.3}\n","{'loss': 7.0125, 'lr': 6.356013669303162e-06, 'epoch': 8.32}\n","{'loss': 6.4874, 'lr': 6.336985188887367e-06, 'epoch': 8.35}\n","{'loss': 7.159, 'lr': 6.3179358303453386e-06, 'epoch': 8.38}\n","{'loss': 7.3388, 'grad_norm': 18.904802322387695, 'learning_rate': 6.298865891148518e-06, 'epoch': 8.4}\n","{'loss': 6.941, 'lr': 6.298865891148518e-06, 'epoch': 8.4}\n","{'loss': 7.0789, 'lr': 6.279775669089734e-06, 'epoch': 8.43}\n","{'loss': 6.15, 'lr': 6.260665462278544e-06, 'epoch': 8.45}\n","{'loss': 7.2117, 'lr': 6.241535569136584e-06, 'epoch': 8.47}\n","{'loss': 6.108, 'lr': 6.222386288392914e-06, 'epoch': 8.5}\n","{'loss': 6.5392, 'lr': 6.203217919079343e-06, 'epoch': 8.53}\n","{'loss': 5.983, 'lr': 6.184030760525763e-06, 'epoch': 8.55}\n","{'loss': 6.0417, 'lr': 6.164825112355478e-06, 'epoch': 8.57}\n","{'loss': 6.5067, 'grad_norm': 20.451745986938477, 'learning_rate': 6.145601274480521e-06, 'epoch': 8.6}\n","{'loss': 6.0469, 'lr': 6.145601274480521e-06, 'epoch': 8.6}\n","{'loss': 6.1103, 'lr': 6.126359547096975e-06, 'epoch': 8.62}\n","{'loss': 6.4447, 'lr': 6.10710023068028e-06, 'epoch': 8.65}\n","{'loss': 6.4951, 'lr': 6.08782362598054e-06, 'epoch': 8.68}\n","{'loss': 5.7165, 'lr': 6.068530034017836e-06, 'epoch': 8.7}\n","{'loss': 5.3948, 'lr': 6.049219756077514e-06, 'epoch': 8.72}\n","{'loss': 5.7104, 'lr': 6.029893093705492e-06, 'epoch': 8.75}\n","{'loss': 5.5381, 'lr': 6.0105503487035375e-06, 'epoch': 8.78}\n","{'loss': 5.9321, 'grad_norm': 18.827138900756836, 'learning_rate': 5.991191823124565e-06, 'epoch': 8.8}\n","{'loss': 5.3207, 'lr': 5.991191823124565e-06, 'epoch': 8.8}\n","{'loss': 5.4795, 'lr': 5.971817819267914e-06, 'epoch': 8.82}\n","{'loss': 5.2388, 'lr': 5.952428639674632e-06, 'epoch': 8.85}\n","{'loss': 5.9914, 'lr': 5.933024587122745e-06, 'epoch': 8.88}\n","{'loss': 4.9949, 'lr': 5.9136059646225375e-06, 'epoch': 8.9}\n","{'loss': 5.1152, 'lr': 5.894173075411812e-06, 'epoch': 8.93}\n","{'loss': 4.5399, 'lr': 5.874726222951157e-06, 'epoch': 8.95}\n","{'loss': 5.1702, 'lr': 5.855265710919211e-06, 'epoch': 8.97}\n","{'loss': 5.2313, 'grad_norm': 20.960886001586914, 'learning_rate': 5.835791843207916e-06, 'epoch': 9.0}\n"," 45% 360/800 [01:40<01:44,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 4.7373, 'lr': 5.835791843207916e-06, 'epoch': 9.0}\n"," 45% 360/800 [01:42<01:44,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 4.3302, 'lr': 5.816304923917778e-06, 'epoch': 9.03}\n","{'loss': 4.4928, 'lr': 5.796805257353109e-06, 'epoch': 9.05}\n","{'loss': 4.7069, 'lr': 5.77729314801728e-06, 'epoch': 9.07}\n","{'loss': 4.7902, 'lr': 5.757768900607972e-06, 'epoch': 9.1}\n","{'loss': 4.428, 'lr': 5.738232820012407e-06, 'epoch': 9.12}\n","{'loss': 4.2558, 'lr': 5.718685211302592e-06, 'epoch': 9.15}\n","{'loss': 5.2159, 'lr': 5.6991263797305594e-06, 'epoch': 9.18}\n","{'loss': 4.6196, 'grad_norm': 19.894277572631836, 'learning_rate': 5.679556630723592e-06, 'epoch': 9.2}\n","{'loss': 4.1474, 'lr': 5.679556630723592e-06, 'epoch': 9.2}\n","{'loss': 4.6176, 'lr': 5.659976269879456e-06, 'epoch': 9.22}\n","{'loss': 4.3185, 'lr': 5.640385602961634e-06, 'epoch': 9.25}\n","{'loss': 4.6865, 'lr': 5.620784935894548e-06, 'epoch': 9.28}\n","{'loss': 3.8911, 'lr': 5.6011745747587706e-06, 'epoch': 9.3}\n","{'loss': 3.7877, 'lr': 5.581554825786268e-06, 'epoch': 9.32}\n","{'loss': 4.57, 'lr': 5.561925995355595e-06, 'epoch': 9.35}\n","{'loss': 3.9562, 'lr': 5.542288389987128e-06, 'epoch': 9.38}\n","{'loss': 4.2469, 'grad_norm': 21.42754364013672, 'learning_rate': 5.522642316338268e-06, 'epoch': 9.4}\n","{'loss': 4.0182, 'lr': 5.522642316338268e-06, 'epoch': 9.4}\n","{'loss': 3.8569, 'lr': 5.5029880811986546e-06, 'epoch': 9.43}\n","{'loss': 4.0324, 'lr': 5.483325991485378e-06, 'epoch': 9.45}\n","{'loss': 3.5944, 'lr': 5.463656354238184e-06, 'epoch': 9.47}\n","{'loss': 3.8139, 'lr': 5.443979476614674e-06, 'epoch': 9.5}\n","{'loss': 3.6046, 'lr': 5.424295665885523e-06, 'epoch': 9.53}\n","{'loss': 3.8252, 'lr': 5.4046052294296635e-06, 'epoch': 9.55}\n","{'loss': 3.8063, 'lr': 5.384908474729501e-06, 'epoch': 9.57}\n","{'loss': 3.819, 'grad_norm': 15.309502601623535, 'learning_rate': 5.365205709366099e-06, 'epoch': 9.6}\n","{'loss': 3.5023, 'lr': 5.365205709366099e-06, 'epoch': 9.6}\n","{'loss': 3.4272, 'lr': 5.34549724101439e-06, 'epoch': 9.62}\n","{'loss': 3.5148, 'lr': 5.325783377438357e-06, 'epoch': 9.65}\n","{'loss': 3.509, 'lr': 5.306064426486237e-06, 'epoch': 9.68}\n","{'loss': 3.7348, 'lr': 5.286340696085709e-06, 'epoch': 9.7}\n","{'loss': 3.4797, 'lr': 5.266612494239088e-06, 'epoch': 9.72}\n","{'loss': 3.2583, 'lr': 5.246880129018515e-06, 'epoch': 9.75}\n","{'loss': 3.6549, 'lr': 5.227143908561146e-06, 'epoch': 9.78}\n","{'loss': 3.5101, 'grad_norm': 10.484848022460938, 'learning_rate': 5.207404141064334e-06, 'epoch': 9.8}\n","{'loss': 3.5094, 'lr': 5.207404141064334e-06, 'epoch': 9.8}\n","{'loss': 3.2196, 'lr': 5.187661134780829e-06, 'epoch': 9.82}\n","{'loss': 3.6733, 'lr': 5.167915198013956e-06, 'epoch': 9.85}\n","{'loss': 3.335, 'lr': 5.148166639112799e-06, 'epoch': 9.88}\n","{'loss': 3.5302, 'lr': 5.128415766467392e-06, 'epoch': 9.9}\n","{'loss': 3.1205, 'lr': 5.108662888503895e-06, 'epoch': 9.93}\n","{'loss': 3.5231, 'lr': 5.088908313679788e-06, 'epoch': 9.95}\n","{'loss': 3.4893, 'lr': 5.069152350479048e-06, 'epoch': 9.97}\n","{'loss': 3.4251, 'grad_norm': 8.521785736083984, 'learning_rate': 5.049395307407329e-06, 'epoch': 10.0}\n"," 50% 400/800 [01:51<01:35,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.6695, 'lr': 5.049395307407329e-06, 'epoch': 10.0}\n"," 50% 400/800 [01:53<01:35,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.566, 'lr': 5.029637492987153e-06, 'epoch': 10.03}\n","{'loss': 3.5091, 'lr': 5.009879215753085e-06, 'epoch': 10.05}\n","{'loss': 3.6532, 'lr': 4.990120784246917e-06, 'epoch': 10.07}\n","{'loss': 3.7509, 'lr': 4.970362507012848e-06, 'epoch': 10.1}\n","{'loss': 2.7475, 'lr': 4.9506046925926725e-06, 'epoch': 10.12}\n","{'loss': 3.6093, 'lr': 4.930847649520955e-06, 'epoch': 10.15}\n","{'loss': 3.2136, 'lr': 4.911091686320213e-06, 'epoch': 10.18}\n","{'loss': 3.4649, 'grad_norm': 8.25749397277832, 'learning_rate': 4.891337111496107e-06, 'epoch': 10.2}\n","{'loss': 3.2049, 'lr': 4.891337111496107e-06, 'epoch': 10.2}\n","{'loss': 3.709, 'lr': 4.87158423353261e-06, 'epoch': 10.22}\n","{'loss': 4.2901, 'lr': 4.8518333608872015e-06, 'epoch': 10.25}\n","{'loss': 3.3369, 'lr': 4.8320848019860456e-06, 'epoch': 10.28}\n","{'loss': 3.3094, 'lr': 4.812338865219172e-06, 'epoch': 10.3}\n","{'loss': 3.0311, 'lr': 4.792595858935668e-06, 'epoch': 10.32}\n","{'loss': 3.5513, 'lr': 4.772856091438857e-06, 'epoch': 10.35}\n","{'loss': 3.2413, 'lr': 4.753119870981486e-06, 'epoch': 10.38}\n","{'loss': 3.4592, 'grad_norm': 8.95076847076416, 'learning_rate': 4.733387505760913e-06, 'epoch': 10.4}\n","{'loss': 3.5128, 'lr': 4.733387505760913e-06, 'epoch': 10.4}\n","{'loss': 3.5017, 'lr': 4.713659303914292e-06, 'epoch': 10.43}\n","{'loss': 3.7378, 'lr': 4.693935573513764e-06, 'epoch': 10.45}\n","{'loss': 3.1434, 'lr': 4.674216622561645e-06, 'epoch': 10.47}\n","{'loss': 3.3514, 'lr': 4.654502758985611e-06, 'epoch': 10.5}\n","{'loss': 3.3477, 'lr': 4.634794290633902e-06, 'epoch': 10.53}\n","{'loss': 3.2889, 'lr': 4.6150915252705005e-06, 'epoch': 10.55}\n","{'loss': 3.4013, 'lr': 4.595394770570337e-06, 'epoch': 10.57}\n","{'loss': 3.4106, 'grad_norm': 12.078144073486328, 'learning_rate': 4.575704334114479e-06, 'epoch': 10.6}\n","{'loss': 3.0807, 'lr': 4.575704334114479e-06, 'epoch': 10.6}\n","{'loss': 3.4524, 'lr': 4.556020523385326e-06, 'epoch': 10.62}\n","{'loss': 3.3898, 'lr': 4.536343645761818e-06, 'epoch': 10.65}\n","{'loss': 3.0111, 'lr': 4.516674008514623e-06, 'epoch': 10.68}\n","{'loss': 3.5046, 'lr': 4.497011918801347e-06, 'epoch': 10.7}\n","{'loss': 3.2216, 'lr': 4.477357683661734e-06, 'epoch': 10.72}\n","{'loss': 3.1573, 'lr': 4.457711610012873e-06, 'epoch': 10.75}\n","{'loss': 3.2803, 'lr': 4.438074004644407e-06, 'epoch': 10.78}\n","{'loss': 3.2622, 'grad_norm': 7.640044689178467, 'learning_rate': 4.418445174213734e-06, 'epoch': 10.8}\n","{'loss': 3.0443, 'lr': 4.418445174213734e-06, 'epoch': 10.8}\n","{'loss': 2.7797, 'lr': 4.39882542524123e-06, 'epoch': 10.82}\n","{'loss': 2.8039, 'lr': 4.379215064105454e-06, 'epoch': 10.85}\n","{'loss': 3.3077, 'lr': 4.3596143970383665e-06, 'epoch': 10.88}\n","{'loss': 3.547, 'lr': 4.340023730120545e-06, 'epoch': 10.9}\n","{'loss': 3.705, 'lr': 4.32044336927641e-06, 'epoch': 10.93}\n","{'loss': 3.4439, 'lr': 4.300873620269441e-06, 'epoch': 10.95}\n","{'loss': 3.4572, 'lr': 4.281314788697409e-06, 'epoch': 10.97}\n","{'loss': 3.2611, 'grad_norm': 11.5896577835083, 'learning_rate': 4.261767179987595e-06, 'epoch': 11.0}\n"," 55% 440/800 [02:02<01:23,  4.34it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.1391, 'lr': 4.261767179987595e-06, 'epoch': 11.0}\n"," 55% 440/800 [02:04<01:23,  4.34it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.9073, 'lr': 4.242231099392029e-06, 'epoch': 11.03}\n","{'loss': 2.9239, 'lr': 4.222706851982721e-06, 'epoch': 11.05}\n","{'loss': 3.2961, 'lr': 4.203194742646893e-06, 'epoch': 11.07}\n","{'loss': 3.1625, 'lr': 4.183695076082224e-06, 'epoch': 11.1}\n","{'loss': 3.0925, 'lr': 4.1642081567920845e-06, 'epoch': 11.12}\n","{'loss': 3.4261, 'lr': 4.1447342890807905e-06, 'epoch': 11.15}\n","{'loss': 3.3612, 'lr': 4.125273777048845e-06, 'epoch': 11.18}\n","{'loss': 3.1636, 'grad_norm': 9.463446617126465, 'learning_rate': 4.10582692458819e-06, 'epoch': 11.2}\n","{'loss': 3.8979, 'lr': 4.10582692458819e-06, 'epoch': 11.2}\n","{'loss': 3.354, 'lr': 4.086394035377463e-06, 'epoch': 11.22}\n","{'loss': 3.2598, 'lr': 4.0669754128772554e-06, 'epoch': 11.25}\n","{'loss': 3.1111, 'lr': 4.047571360325369e-06, 'epoch': 11.28}\n","{'loss': 3.0927, 'lr': 4.028182180732088e-06, 'epoch': 11.3}\n","{'loss': 3.0177, 'lr': 4.008808176875437e-06, 'epoch': 11.32}\n","{'loss': 3.2141, 'lr': 3.989449651296463e-06, 'epoch': 11.35}\n","{'loss': 3.2283, 'lr': 3.970106906294509e-06, 'epoch': 11.38}\n","{'loss': 3.2719, 'grad_norm': 8.570823669433594, 'learning_rate': 3.950780243922487e-06, 'epoch': 11.4}\n","{'loss': 3.5331, 'lr': 3.950780243922487e-06, 'epoch': 11.4}\n","{'loss': 3.127, 'lr': 3.931469965982167e-06, 'epoch': 11.43}\n","{'loss': 2.7591, 'lr': 3.912176374019462e-06, 'epoch': 11.45}\n","{'loss': 3.3323, 'lr': 3.892899769319723e-06, 'epoch': 11.47}\n","{'loss': 3.414, 'lr': 3.8736404529030255e-06, 'epoch': 11.5}\n","{'loss': 3.2925, 'lr': 3.85439872551948e-06, 'epoch': 11.53}\n","{'loss': 3.0234, 'lr': 3.835174887644524e-06, 'epoch': 11.55}\n","{'loss': 3.2514, 'lr': 3.815969239474239e-06, 'epoch': 11.57}\n","{'loss': 3.2166, 'grad_norm': 8.669747352600098, 'learning_rate': 3.796782080920659e-06, 'epoch': 11.6}\n","{'loss': 2.991, 'lr': 3.796782080920659e-06, 'epoch': 11.6}\n","{'loss': 3.0934, 'lr': 3.777613711607087e-06, 'epoch': 11.62}\n","{'loss': 3.3481, 'lr': 3.758464430863417e-06, 'epoch': 11.65}\n","{'loss': 3.0957, 'lr': 3.7393345377214584e-06, 'epoch': 11.68}\n","{'loss': 3.2054, 'lr': 3.720224330910268e-06, 'epoch': 11.7}\n","{'loss': 3.0542, 'lr': 3.701134108851483e-06, 'epoch': 11.72}\n","{'loss': 3.1057, 'lr': 3.682064169654663e-06, 'epoch': 11.75}\n","{'loss': 3.2519, 'lr': 3.6630148111126346e-06, 'epoch': 11.78}\n","{'loss': 3.1432, 'grad_norm': 9.483325004577637, 'learning_rate': 3.64398633069684e-06, 'epoch': 11.8}\n","{'loss': 3.1903, 'lr': 3.64398633069684e-06, 'epoch': 11.8}\n","{'loss': 2.8075, 'lr': 3.6249790255526916e-06, 'epoch': 11.82}\n","{'loss': 3.1961, 'lr': 3.6059931924949342e-06, 'epoch': 11.85}\n","{'loss': 2.9271, 'lr': 3.587029128003006e-06, 'epoch': 11.88}\n","{'loss': 3.6191, 'lr': 3.568087128216414e-06, 'epoch': 11.9}\n","{'loss': 3.5883, 'lr': 3.5491674889301032e-06, 'epoch': 11.93}\n","{'loss': 3.8285, 'lr': 3.5302705055898428e-06, 'epoch': 11.95}\n","{'loss': 3.3022, 'lr': 3.511396473287611e-06, 'epoch': 11.97}\n","{'loss': 3.3074, 'grad_norm': 9.79948902130127, 'learning_rate': 3.492545686756986e-06, 'epoch': 12.0}\n"," 60% 480/800 [02:13<01:16,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.0841, 'lr': 3.492545686756986e-06, 'epoch': 12.0}\n"," 60% 480/800 [02:15<01:16,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.4246, 'lr': 3.4737184403685443e-06, 'epoch': 12.03}\n","{'loss': 3.4507, 'lr': 3.4549150281252635e-06, 'epoch': 12.05}\n","{'loss': 3.323, 'lr': 3.4361357436579318e-06, 'epoch': 12.07}\n","{'loss': 2.7413, 'lr': 3.4173808802205633e-06, 'epoch': 12.1}\n","{'loss': 3.7208, 'lr': 3.398650730685813e-06, 'epoch': 12.12}\n","{'loss': 3.5813, 'lr': 3.3799455875404137e-06, 'epoch': 12.15}\n","{'loss': 2.9695, 'lr': 3.3612657428805995e-06, 'epoch': 12.18}\n","{'loss': 3.2869, 'grad_norm': 10.113937377929688, 'learning_rate': 3.3426114884075488e-06, 'epoch': 12.2}\n","{'loss': 3.0534, 'lr': 3.3426114884075488e-06, 'epoch': 12.2}\n","{'loss': 2.9145, 'lr': 3.3239831154228275e-06, 'epoch': 12.22}\n","{'loss': 2.9576, 'lr': 3.3053809148238426e-06, 'epoch': 12.25}\n","{'loss': 2.8713, 'lr': 3.2868051770992935e-06, 'epoch': 12.28}\n","{'loss': 2.7669, 'lr': 3.268256192324647e-06, 'epoch': 12.3}\n","{'loss': 2.8929, 'lr': 3.2497342501575925e-06, 'epoch': 12.32}\n","{'loss': 3.4471, 'lr': 3.2312396398335312e-06, 'epoch': 12.35}\n","{'loss': 2.712, 'lr': 3.2127726501610558e-06, 'epoch': 12.38}\n","{'loss': 2.952, 'grad_norm': 9.202103614807129, 'learning_rate': 3.1943335695174365e-06, 'epoch': 12.4}\n","{'loss': 3.333, 'lr': 3.1943335695174365e-06, 'epoch': 12.4}\n","{'loss': 3.0091, 'lr': 3.175922685844125e-06, 'epoch': 12.43}\n","{'loss': 3.1608, 'lr': 3.15754028664225e-06, 'epoch': 12.45}\n","{'loss': 2.6408, 'lr': 3.1391866589681346e-06, 'epoch': 12.47}\n","{'loss': 3.1077, 'lr': 3.1208620894288105e-06, 'epoch': 12.5}\n","{'loss': 3.0251, 'lr': 3.1025668641775405e-06, 'epoch': 12.53}\n","{'loss': 3.3585, 'lr': 3.084301268909353e-06, 'epoch': 12.55}\n","{'loss': 3.0191, 'lr': 3.0660655888565827e-06, 'epoch': 12.57}\n","{'loss': 3.0817, 'grad_norm': 31.189485549926758, 'learning_rate': 3.0478601087844095e-06, 'epoch': 12.6}\n","{'loss': 3.0566, 'lr': 3.0478601087844095e-06, 'epoch': 12.6}\n","{'loss': 2.8104, 'lr': 3.029685112986417e-06, 'epoch': 12.62}\n","{'loss': 3.4281, 'lr': 3.0115408852801535e-06, 'epoch': 12.65}\n","{'loss': 3.202, 'lr': 2.9934277090026966e-06, 'epoch': 12.68}\n","{'loss': 3.3534, 'lr': 2.97534586700623e-06, 'epoch': 12.7}\n","{'loss': 3.0736, 'lr': 2.9572956416536267e-06, 'epoch': 12.72}\n","{'loss': 2.7485, 'lr': 2.9392773148140406e-06, 'epoch': 12.75}\n","{'loss': 3.2276, 'lr': 2.9212911678585044e-06, 'epoch': 12.78}\n","{'loss': 3.1125, 'grad_norm': 11.780155181884766, 'learning_rate': 2.9033374816555338e-06, 'epoch': 12.8}\n","{'loss': 2.7913, 'lr': 2.9033374816555338e-06, 'epoch': 12.8}\n","{'loss': 3.2174, 'lr': 2.885416536566744e-06, 'epoch': 12.82}\n","{'loss': 3.6483, 'lr': 2.8675286124424694e-06, 'epoch': 12.85}\n","{'loss': 2.7443, 'lr': 2.8496739886173994e-06, 'epoch': 12.88}\n","{'loss': 3.0132, 'lr': 2.8318529439062037e-06, 'epoch': 12.9}\n","{'loss': 2.8587, 'lr': 2.814065756599196e-06, 'epoch': 12.93}\n","{'loss': 3.1077, 'lr': 2.7963127044579697e-06, 'epoch': 12.95}\n","{'loss': 3.01, 'lr': 2.7785940647110766e-06, 'epoch': 12.97}\n","{'loss': 3.0489, 'grad_norm': 9.653020858764648, 'learning_rate': 2.7609101140496863e-06, 'epoch': 13.0}\n"," 65% 520/800 [02:25<01:07,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.7336, 'lr': 2.7609101140496863e-06, 'epoch': 13.0}\n"," 65% 520/800 [02:26<01:07,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.3576, 'lr': 2.743261128623269e-06, 'epoch': 13.03}\n","{'loss': 2.7138, 'lr': 2.7256473840352905e-06, 'epoch': 13.05}\n","{'loss': 2.6581, 'lr': 2.708069155338892e-06, 'epoch': 13.07}\n","{'loss': 3.1795, 'lr': 2.6905267170326143e-06, 'epoch': 13.1}\n","{'loss': 2.8554, 'lr': 2.6730203430560946e-06, 'epoch': 13.12}\n","{'loss': 2.966, 'lr': 2.6555503067858013e-06, 'epoch': 13.15}\n","{'loss': 2.8042, 'lr': 2.6381168810307536e-06, 'epoch': 13.18}\n","{'loss': 2.9085, 'grad_norm': 10.231196403503418, 'learning_rate': 2.620720338028275e-06, 'epoch': 13.2}\n","{'loss': 3.07, 'lr': 2.620720338028275e-06, 'epoch': 13.2}\n","{'loss': 2.9265, 'lr': 2.603360949439727e-06, 'epoch': 13.22}\n","{'loss': 3.0234, 'lr': 2.5860389863462765e-06, 'epoch': 13.25}\n","{'loss': 2.6221, 'lr': 2.568754719244665e-06, 'epoch': 13.28}\n","{'loss': 3.2547, 'lr': 2.5515084180429716e-06, 'epoch': 13.3}\n","{'loss': 2.8735, 'lr': 2.534300352056416e-06, 'epoch': 13.32}\n","{'loss': 2.7792, 'lr': 2.5171307900031345e-06, 'epoch': 13.35}\n","{'loss': 2.7712, 'lr': 2.5000000000000015e-06, 'epoch': 13.38}\n","{'loss': 2.9151, 'grad_norm': 10.141858100891113, 'learning_rate': 2.4829082495584244e-06, 'epoch': 13.4}\n","{'loss': 2.7842, 'lr': 2.4829082495584244e-06, 'epoch': 13.4}\n","{'loss': 2.7323, 'lr': 2.465855805580185e-06, 'epoch': 13.43}\n","{'loss': 3.3093, 'lr': 2.448842934353256e-06, 'epoch': 13.45}\n","{'loss': 3.4912, 'lr': 2.4318699015476495e-06, 'epoch': 13.47}\n","{'loss': 3.0551, 'lr': 2.414936972211272e-06, 'epoch': 13.5}\n","{'loss': 2.6442, 'lr': 2.3980444107657746e-06, 'epoch': 13.53}\n","{'loss': 2.8975, 'lr': 2.3811924810024385e-06, 'epoch': 13.55}\n","{'loss': 3.3035, 'lr': 2.3643814460780397e-06, 'epoch': 13.57}\n","{'loss': 3.0272, 'grad_norm': 10.071511268615723, 'learning_rate': 2.3476115685107543e-06, 'epoch': 13.6}\n","{'loss': 3.2045, 'lr': 2.3476115685107543e-06, 'epoch': 13.6}\n","{'loss': 3.2063, 'lr': 2.330883110176049e-06, 'epoch': 13.62}\n","{'loss': 3.0334, 'lr': 2.314196332302592e-06, 'epoch': 13.65}\n","{'loss': 3.3397, 'lr': 2.2975514954681836e-06, 'epoch': 13.68}\n","{'loss': 3.3372, 'lr': 2.2809488595956746e-06, 'epoch': 13.7}\n","{'loss': 2.8938, 'lr': 2.2643886839489183e-06, 'epoch': 13.72}\n","{'loss': 3.3849, 'lr': 2.247871227128709e-06, 'epoch': 13.75}\n","{'loss': 3.0611, 'lr': 2.2313967470687593e-06, 'epoch': 13.78}\n","{'loss': 3.1826, 'grad_norm': 12.086634635925293, 'learning_rate': 2.2149655010316575e-06, 'epoch': 13.8}\n","{'loss': 3.1287, 'lr': 2.2149655010316575e-06, 'epoch': 13.8}\n","{'loss': 3.2003, 'lr': 2.1985777456048633e-06, 'epoch': 13.82}\n","{'loss': 3.3778, 'lr': 2.18223373669669e-06, 'epoch': 13.85}\n","{'loss': 3.1025, 'lr': 2.1659337295323117e-06, 'epoch': 13.88}\n","{'loss': 2.7845, 'lr': 2.1496779786497864e-06, 'epoch': 13.9}\n","{'loss': 3.0828, 'lr': 2.1334667378960642e-06, 'epoch': 13.93}\n","{'loss': 2.8387, 'lr': 2.1173002604230427e-06, 'epoch': 13.95}\n","{'loss': 3.566, 'lr': 2.1011787986835936e-06, 'epoch': 13.97}\n","{'loss': 3.1352, 'grad_norm': 13.846491813659668, 'learning_rate': 2.0851026044276405e-06, 'epoch': 14.0}\n"," 70% 560/800 [02:35<00:57,  4.18it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.3971, 'lr': 2.0851026044276405e-06, 'epoch': 14.0}\n"," 70% 560/800 [02:37<00:57,  4.18it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.7455, 'lr': 2.0690719286982127e-06, 'epoch': 14.03}\n","{'loss': 2.6933, 'lr': 2.053087021827527e-06, 'epoch': 14.05}\n","{'loss': 2.5271, 'lr': 2.0371481334330913e-06, 'epoch': 14.07}\n","{'loss': 2.9249, 'lr': 2.0212555124137866e-06, 'epoch': 14.1}\n","{'loss': 3.3068, 'lr': 2.005409406946e-06, 'epoch': 14.12}\n","{'loss': 3.1814, 'lr': 1.9896100644797316e-06, 'epoch': 14.15}\n","{'loss': 2.8553, 'lr': 1.9738577317347464e-06, 'epoch': 14.18}\n","{'loss': 3.0789, 'grad_norm': 13.022590637207031, 'learning_rate': 1.958152654696705e-06, 'epoch': 14.2}\n","{'loss': 3.2731, 'lr': 1.958152654696705e-06, 'epoch': 14.2}\n","{'loss': 2.6008, 'lr': 1.9424950786133414e-06, 'epoch': 14.22}\n","{'loss': 3.1647, 'lr': 1.926885247990615e-06, 'epoch': 14.25}\n","{'loss': 3.2136, 'lr': 1.9113234065889016e-06, 'epoch': 14.28}\n","{'loss': 3.1123, 'lr': 1.8958097974191909e-06, 'epoch': 14.3}\n","{'loss': 2.9121, 'lr': 1.8803446627392796e-06, 'epoch': 14.32}\n","{'loss': 3.0503, 'lr': 1.8649282440500015e-06, 'epoch': 14.35}\n","{'loss': 2.5372, 'lr': 1.8495607820914451e-06, 'epoch': 14.38}\n","{'loss': 2.983, 'grad_norm': 31.88495445251465, 'learning_rate': 1.834242516839203e-06, 'epoch': 14.4}\n","{'loss': 2.9281, 'lr': 1.834242516839203e-06, 'epoch': 14.4}\n","{'loss': 3.138, 'lr': 1.8189736875006187e-06, 'epoch': 14.43}\n","{'loss': 2.8918, 'lr': 1.8037545325110506e-06, 'epoch': 14.45}\n","{'loss': 2.6793, 'lr': 1.7885852895301581e-06, 'epoch': 14.47}\n","{'loss': 2.5072, 'lr': 1.7734661954381754e-06, 'epoch': 14.5}\n","{'loss': 3.1355, 'lr': 1.7583974863322272e-06, 'epoch': 14.53}\n","{'loss': 3.2083, 'lr': 1.74337939752263e-06, 'epoch': 14.55}\n","{'loss': 3.244, 'lr': 1.7284121635292273e-06, 'epoch': 14.57}\n","{'loss': 2.9665, 'grad_norm': 10.373648643493652, 'learning_rate': 1.7134960180777171e-06, 'epoch': 14.6}\n","{'loss': 3.0859, 'lr': 1.7134960180777171e-06, 'epoch': 14.6}\n","{'loss': 2.7751, 'lr': 1.6986311940960148e-06, 'epoch': 14.62}\n","{'loss': 2.8915, 'lr': 1.6838179237106018e-06, 'epoch': 14.65}\n","{'loss': 3.2672, 'lr': 1.6690564382429104e-06, 'epoch': 14.68}\n","{'loss': 3.0495, 'lr': 1.6543469682057105e-06, 'epoch': 14.7}\n","{'loss': 3.1898, 'lr': 1.6396897432995046e-06, 'epoch': 14.72}\n","{'loss': 3.0035, 'lr': 1.6250849924089485e-06, 'epoch': 14.75}\n","{'loss': 2.9405, 'lr': 1.6105329435992684e-06, 'epoch': 14.78}\n","{'loss': 3.0254, 'grad_norm': 10.444954872131348, 'learning_rate': 1.5960338241127093e-06, 'epoch': 14.8}\n","{'loss': 2.9517, 'lr': 1.5960338241127093e-06, 'epoch': 14.8}\n","{'loss': 2.8081, 'lr': 1.581587860364977e-06, 'epoch': 14.82}\n","{'loss': 2.8102, 'lr': 1.567195277941706e-06, 'epoch': 14.85}\n","{'loss': 2.8099, 'lr': 1.5528563015949421e-06, 'epoch': 14.88}\n","{'loss': 3.2344, 'lr': 1.5385711552396227e-06, 'epoch': 14.9}\n","{'loss': 2.4648, 'lr': 1.5243400619500903e-06, 'epoch': 14.93}\n","{'loss': 3.1728, 'lr': 1.5101632439566e-06, 'epoch': 14.95}\n","{'loss': 2.8878, 'lr': 1.4960409226418576e-06, 'epoch': 14.97}\n","{'loss': 2.8925, 'grad_norm': 9.743182182312012, 'learning_rate': 1.4819733185375535e-06, 'epoch': 15.0}\n"," 75% 600/800 [02:46<00:50,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.7117, 'lr': 1.4819733185375535e-06, 'epoch': 15.0}\n"," 75% 600/800 [02:49<00:50,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.6392, 'lr': 1.4679606513209283e-06, 'epoch': 15.03}\n","{'loss': 3.124, 'lr': 1.4540031398113335e-06, 'epoch': 15.05}\n","{'loss': 2.6632, 'lr': 1.4401010019668226e-06, 'epoch': 15.07}\n","{'loss': 3.2127, 'lr': 1.4262544548807433e-06, 'epoch': 15.1}\n","{'loss': 3.264, 'lr': 1.4124637147783431e-06, 'epoch': 15.12}\n","{'loss': 3.0758, 'lr': 1.3987289970134048e-06, 'epoch': 15.15}\n","{'loss': 2.5447, 'lr': 1.3850505160648708e-06, 'epoch': 15.18}\n","{'loss': 3.0294, 'grad_norm': 9.85409927368164, 'learning_rate': 1.371428485533498e-06, 'epoch': 15.2}\n","{'loss': 2.7389, 'lr': 1.371428485533498e-06, 'epoch': 15.2}\n","{'loss': 3.2111, 'lr': 1.3578631181385305e-06, 'epoch': 15.22}\n","{'loss': 2.7865, 'lr': 1.3443546257143624e-06, 'epoch': 15.25}\n","{'loss': 2.9614, 'lr': 1.3309032192072463e-06, 'epoch': 15.28}\n","{'loss': 2.9515, 'lr': 1.3175091086719833e-06, 'epoch': 15.3}\n","{'loss': 2.9674, 'lr': 1.3041725032686581e-06, 'epoch': 15.32}\n","{'loss': 3.0699, 'lr': 1.29089361125936e-06, 'epoch': 15.35}\n","{'loss': 3.2378, 'lr': 1.277672640004936e-06, 'epoch': 15.38}\n","{'loss': 2.9906, 'grad_norm': 14.341693878173828, 'learning_rate': 1.2645097959617587e-06, 'epoch': 15.4}\n","{'loss': 3.0448, 'lr': 1.2645097959617587e-06, 'epoch': 15.4}\n","{'loss': 2.5575, 'lr': 1.251405284678488e-06, 'epoch': 15.43}\n","{'loss': 2.8696, 'lr': 1.2383593107928771e-06, 'epoch': 15.45}\n","{'loss': 3.124, 'lr': 1.2253720780285637e-06, 'epoch': 15.47}\n","{'loss': 2.9382, 'lr': 1.2124437891918995e-06, 'epoch': 15.5}\n","{'loss': 3.385, 'lr': 1.1995746461687735e-06, 'epoch': 15.53}\n","{'loss': 3.0435, 'lr': 1.186764849921468e-06, 'epoch': 15.55}\n","{'loss': 2.8728, 'lr': 1.1740146004855141e-06, 'epoch': 15.57}\n","{'loss': 2.9794, 'grad_norm': 13.464892387390137, 'learning_rate': 1.1613240969665685e-06, 'epoch': 15.6}\n","{'loss': 2.852, 'lr': 1.1613240969665685e-06, 'epoch': 15.6}\n","{'loss': 2.6838, 'lr': 1.1486935375373127e-06, 'epoch': 15.62}\n","{'loss': 2.951, 'lr': 1.1361231194343436e-06, 'epoch': 15.65}\n","{'loss': 3.1948, 'lr': 1.1236130389551093e-06, 'epoch': 15.68}\n","{'loss': 2.8805, 'lr': 1.1111634914548297e-06, 'epoch': 15.7}\n","{'loss': 2.9998, 'lr': 1.0987746713434578e-06, 'epoch': 15.72}\n","{'loss': 2.7218, 'lr': 1.0864467720826343e-06, 'epoch': 15.75}\n","{'loss': 2.7579, 'lr': 1.0741799861826708e-06, 'epoch': 15.78}\n","{'loss': 2.8802, 'grad_norm': 11.521163940429688, 'learning_rate': 1.0619745051995473e-06, 'epoch': 15.8}\n","{'loss': 2.6901, 'lr': 1.0619745051995473e-06, 'epoch': 15.8}\n","{'loss': 2.3978, 'lr': 1.0498305197319114e-06, 'epoch': 15.82}\n","{'loss': 3.0047, 'lr': 1.0377482194181132e-06, 'epoch': 15.85}\n","{'loss': 3.3407, 'lr': 1.0257277929332332e-06, 'epoch': 15.88}\n","{'loss': 3.0961, 'lr': 1.0137694279861455e-06, 'epoch': 15.9}\n","{'loss': 3.2228, 'lr': 1.0018733113165775e-06, 'epoch': 15.93}\n","{'loss': 2.582, 'lr': 9.900396286922025e-07, 'epoch': 15.95}\n","{'loss': 2.9588, 'lr': 9.782685649057333e-07, 'epoch': 15.97}\n","{'loss': 2.9116, 'grad_norm': 12.18001651763916, 'learning_rate': 9.66560303772035e-07, 'epoch': 16.0}\n"," 80% 640/800 [02:58<00:50,  3.19it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.0646, 'lr': 9.66560303772035e-07, 'epoch': 16.0}\n"," 80% 640/800 [03:00<00:50,  3.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.2795, 'lr': 9.549150281252633e-07, 'epoch': 16.02}\n","{'loss': 2.752, 'lr': 9.433329198159974e-07, 'epoch': 16.05}\n","{'loss': 2.6879, 'lr': 9.31814159708413e-07, 'epoch': 16.07}\n","{'loss': 3.0818, 'lr': 9.203589276774438e-07, 'epoch': 16.1}\n","{'loss': 2.4958, 'lr': 9.08967402605988e-07, 'epoch': 16.12}\n","{'loss': 2.9636, 'lr': 8.976397623821004e-07, 'epoch': 16.15}\n","{'loss': 3.2045, 'lr': 8.86376183896226e-07, 'epoch': 16.18}\n","{'loss': 2.9412, 'grad_norm': 13.38681697845459, 'learning_rate': 8.751768430384305e-07, 'epoch': 16.2}\n","{'loss': 3.0494, 'lr': 8.751768430384305e-07, 'epoch': 16.2}\n","{'loss': 2.6139, 'lr': 8.640419146956558e-07, 'epoch': 16.23}\n","{'loss': 3.1808, 'lr': 8.529715727489912e-07, 'epoch': 16.25}\n","{'loss': 2.6444, 'lr': 8.419659900709536e-07, 'epoch': 16.27}\n","{'loss': 2.7418, 'lr': 8.310253385227945e-07, 'epoch': 16.3}\n","{'loss': 3.158, 'lr': 8.201497889518073e-07, 'epoch': 16.32}\n","{'loss': 3.0463, 'lr': 8.093395111886687e-07, 'epoch': 16.35}\n","{'loss': 3.0044, 'lr': 7.985946740447792e-07, 'epoch': 16.38}\n","{'loss': 2.9299, 'grad_norm': 10.130473136901855, 'learning_rate': 7.879154453096305e-07, 'epoch': 16.4}\n","{'loss': 2.7762, 'lr': 7.879154453096305e-07, 'epoch': 16.4}\n","{'loss': 2.5161, 'lr': 7.773019917481872e-07, 'epoch': 16.43}\n","{'loss': 3.5035, 'lr': 7.667544790982778e-07, 'epoch': 16.45}\n","{'loss': 3.0129, 'lr': 7.562730720680111e-07, 'epoch': 16.48}\n","{'loss': 2.6465, 'lr': 7.458579343331996e-07, 'epoch': 16.5}\n","{'loss': 2.6245, 'lr': 7.355092285348092e-07, 'epoch': 16.52}\n","{'loss': 2.9326, 'lr': 7.25227116276413e-07, 'epoch': 16.55}\n","{'loss': 3.1204, 'lr': 7.150117581216748e-07, 'epoch': 16.57}\n","{'loss': 2.8916, 'grad_norm': 10.6055326461792, 'learning_rate': 7.048633135918348e-07, 'epoch': 16.6}\n","{'loss': 2.9947, 'lr': 7.048633135918348e-07, 'epoch': 16.6}\n","{'loss': 2.7851, 'lr': 6.947819411632223e-07, 'epoch': 16.62}\n","{'loss': 2.6815, 'lr': 6.847677982647827e-07, 'epoch': 16.65}\n","{'loss': 3.477, 'lr': 6.748210412756135e-07, 'epoch': 16.68}\n","{'loss': 3.1104, 'lr': 6.649418255225298e-07, 'epoch': 16.7}\n","{'loss': 2.8171, 'lr': 6.551303052776292e-07, 'epoch': 16.73}\n","{'loss': 2.6323, 'lr': 6.453866337558939e-07, 'epoch': 16.75}\n","{'loss': 2.8092, 'lr': 6.357109631127889e-07, 'epoch': 16.77}\n","{'loss': 2.9134, 'grad_norm': 11.32078742980957, 'learning_rate': 6.261034444418879e-07, 'epoch': 16.8}\n","{'loss': 2.9827, 'lr': 6.261034444418879e-07, 'epoch': 16.8}\n","{'loss': 2.8865, 'lr': 6.165642277725203e-07, 'epoch': 16.82}\n","{'loss': 2.7653, 'lr': 6.07093462067419e-07, 'epoch': 16.85}\n","{'loss': 2.9686, 'lr': 5.976912952204017e-07, 'epoch': 16.88}\n","{'loss': 2.6194, 'lr': 5.883578740540546e-07, 'epoch': 16.9}\n","{'loss': 3.082, 'lr': 5.79093344317449e-07, 'epoch': 16.93}\n","{'loss': 3.3624, 'lr': 5.698978506838532e-07, 'epoch': 16.95}\n","{'loss': 3.1956, 'lr': 5.607715367484861e-07, 'epoch': 16.98}\n","{'loss': 2.9828, 'grad_norm': 13.134765625, 'learning_rate': 5.517145450262639e-07, 'epoch': 17.0}\n"," 85% 680/800 [03:10<00:29,  4.09it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.8613, 'lr': 5.517145450262639e-07, 'epoch': 17.0}\n"," 85% 680/800 [03:12<00:29,  4.09it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.6097, 'lr': 5.427270169495808e-07, 'epoch': 17.02}\n","{'loss': 3.0002, 'lr': 5.338090928660999e-07, 'epoch': 17.05}\n","{'loss': 3.1029, 'lr': 5.249609120365579e-07, 'epoch': 17.07}\n","{'loss': 2.7077, 'lr': 5.16182612632598e-07, 'epoch': 17.1}\n","{'loss': 2.8112, 'lr': 5.074743317346009e-07, 'epoch': 17.12}\n","{'loss': 2.9337, 'lr': 4.988362053295564e-07, 'epoch': 17.15}\n","{'loss': 2.4608, 'lr': 4.902683683089304e-07, 'epoch': 17.18}\n","{'loss': 2.8109, 'grad_norm': 10.748170852661133, 'learning_rate': 4.817709544665628e-07, 'epoch': 17.2}\n","{'loss': 2.693, 'lr': 4.817709544665628e-07, 'epoch': 17.2}\n","{'loss': 2.7129, 'lr': 4.733440964965791e-07, 'epoch': 17.23}\n","{'loss': 3.097, 'lr': 4.649879259913137e-07, 'epoch': 17.25}\n","{'loss': 2.8666, 'lr': 4.5670257343926215e-07, 'epoch': 17.27}\n","{'loss': 3.0587, 'lr': 4.484881682230341e-07, 'epoch': 17.3}\n","{'loss': 2.8742, 'lr': 4.4034483861734367e-07, 'epoch': 17.32}\n","{'loss': 2.8458, 'lr': 4.322727117869951e-07, 'epoch': 17.35}\n","{'loss': 2.7002, 'lr': 4.242719137849077e-07, 'epoch': 17.38}\n","{'loss': 2.8561, 'grad_norm': 15.78709888458252, 'learning_rate': 4.1634256955013883e-07, 'epoch': 17.4}\n","{'loss': 3.7945, 'lr': 4.1634256955013883e-07, 'epoch': 17.4}\n","{'loss': 2.8182, 'lr': 4.0848480290593627e-07, 'epoch': 17.43}\n","{'loss': 3.0274, 'lr': 4.00698736557808e-07, 'epoch': 17.45}\n","{'loss': 2.7903, 'lr': 3.9298449209159873e-07, 'epoch': 17.48}\n","{'loss': 2.7877, 'lr': 3.8534218997159923e-07, 'epoch': 17.5}\n","{'loss': 3.3058, 'lr': 3.777719495386567e-07, 'epoch': 17.52}\n","{'loss': 2.8528, 'lr': 3.7027388900832074e-07, 'epoch': 17.55}\n","{'loss': 3.0671, 'lr': 3.628481254689875e-07, 'epoch': 17.57}\n","{'loss': 3.0555, 'grad_norm': 10.904614448547363, 'learning_rate': 3.5549477488007853e-07, 'epoch': 17.6}\n","{'loss': 2.969, 'lr': 3.5549477488007853e-07, 'epoch': 17.6}\n","{'loss': 3.4219, 'lr': 3.4821395207022767e-07, 'epoch': 17.62}\n","{'loss': 3.1338, 'lr': 3.4100577073548635e-07, 'epoch': 17.65}\n","{'loss': 3.004, 'lr': 3.3387034343755063e-07, 'epoch': 17.68}\n","{'loss': 3.391, 'lr': 3.2680778160200156e-07, 'epoch': 17.7}\n","{'loss': 3.2679, 'lr': 3.198181955165669e-07, 'epoch': 17.73}\n","{'loss': 3.0889, 'lr': 3.1290169432939556e-07, 'epoch': 17.75}\n","{'loss': 2.5823, 'lr': 3.060583860473587e-07, 'epoch': 17.77}\n","{'loss': 3.1073, 'grad_norm': 11.093847274780273, 'learning_rate': 2.992883775343575e-07, 'epoch': 17.8}\n","{'loss': 2.9706, 'lr': 2.992883775343575e-07, 'epoch': 17.8}\n","{'loss': 2.8309, 'lr': 2.925917745096568e-07, 'epoch': 17.82}\n","{'loss': 2.6053, 'lr': 2.8596868154623703e-07, 'epoch': 17.85}\n","{'loss': 2.7068, 'lr': 2.7941920206915443e-07, 'epoch': 17.88}\n","{'loss': 2.5327, 'lr': 2.7294343835393366e-07, 'epoch': 17.9}\n","{'loss': 3.1625, 'lr': 2.6654149152496313e-07, 'epoch': 17.93}\n","{'loss': 2.5501, 'lr': 2.602134615539242e-07, 'epoch': 17.95}\n","{'loss': 2.7654, 'lr': 2.539594472582213e-07, 'epoch': 17.98}\n","{'loss': 2.7655, 'grad_norm': 11.684861183166504, 'learning_rate': 2.477795462994448e-07, 'epoch': 18.0}\n"," 90% 720/800 [03:21<00:18,  4.27it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.6347, 'lr': 2.477795462994448e-07, 'epoch': 18.0}\n"," 90% 720/800 [03:22<00:18,  4.27it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.2604, 'lr': 2.416738551818454e-07, 'epoch': 18.02}\n","{'loss': 3.1904, 'lr': 2.3564246925082358e-07, 'epoch': 18.05}\n","{'loss': 2.8016, 'lr': 2.2968548269144574e-07, 'epoch': 18.07}\n","{'loss': 2.5359, 'lr': 2.238029885269677e-07, 'epoch': 18.1}\n","{'loss': 3.1711, 'lr': 2.179950786173879e-07, 'epoch': 18.12}\n","{'loss': 2.8119, 'lr': 2.1226184365800816e-07, 'epoch': 18.15}\n","{'loss': 2.865, 'lr': 2.0660337317802093e-07, 'epoch': 18.18}\n","{'loss': 2.9089, 'grad_norm': 13.259552955627441, 'learning_rate': 2.01019755539108e-07, 'epoch': 18.2}\n","{'loss': 3.5727, 'lr': 2.01019755539108e-07, 'epoch': 18.2}\n","{'loss': 2.6965, 'lr': 1.9551107793406354e-07, 'epoch': 18.23}\n","{'loss': 3.1568, 'lr': 1.9007742638543104e-07, 'epoch': 18.25}\n","{'loss': 2.6914, 'lr': 1.8471888574415953e-07, 'epoch': 18.27}\n","{'loss': 3.1553, 'lr': 1.794355396882813e-07, 'epoch': 18.3}\n","{'loss': 2.7401, 'lr': 1.7422747072160018e-07, 'epoch': 18.32}\n","{'loss': 3.1711, 'lr': 1.690947601724091e-07, 'epoch': 18.35}\n","{'loss': 2.7288, 'lr': 1.6403748819221464e-07, 'epoch': 18.38}\n","{'loss': 2.9891, 'grad_norm': 10.985634803771973, 'learning_rate': 1.5905573375449013e-07, 'epoch': 18.4}\n","{'loss': 3.0101, 'lr': 1.5905573375449013e-07, 'epoch': 18.4}\n","{'loss': 3.5658, 'lr': 1.5414957465343883e-07, 'epoch': 18.43}\n","{'loss': 2.7153, 'lr': 1.4931908750278e-07, 'epoch': 18.45}\n","{'loss': 2.8908, 'lr': 1.445643477345554e-07, 'epoch': 18.48}\n","{'loss': 2.6465, 'lr': 1.3988542959794627e-07, 'epoch': 18.5}\n","{'loss': 2.4519, 'lr': 1.3528240615811815e-07, 'epoch': 18.52}\n","{'loss': 2.7075, 'lr': 1.3075534929507693e-07, 'epoch': 18.55}\n","{'loss': 3.1898, 'lr': 1.2630432970255014e-07, 'epoch': 18.57}\n","{'loss': 2.8972, 'grad_norm': 14.358376502990723, 'learning_rate': 1.2192941688687842e-07, 'epoch': 18.6}\n","{'loss': 2.725, 'lr': 1.2192941688687842e-07, 'epoch': 18.6}\n","{'loss': 2.8129, 'lr': 1.1763067916593263e-07, 'epoch': 18.62}\n","{'loss': 2.9229, 'lr': 1.1340818366804728e-07, 'epoch': 18.65}\n","{'loss': 2.5982, 'lr': 1.0926199633097156e-07, 'epoch': 18.68}\n","{'loss': 2.7613, 'lr': 1.0519218190084057e-07, 'epoch': 18.7}\n","{'loss': 2.8462, 'lr': 1.0119880393116177e-07, 'epoch': 18.73}\n","{'loss': 2.6608, 'lr': 9.728192478182574e-08, 'epoch': 18.75}\n","{'loss': 3.0398, 'lr': 9.344160561812921e-08, 'epoch': 18.77}\n","{'loss': 2.7959, 'grad_norm': 14.622902870178223, 'learning_rate': 8.967790640982466e-08, 'epoch': 18.8}\n","{'loss': 2.7331, 'lr': 8.967790640982466e-08, 'epoch': 18.8}\n","{'loss': 3.0341, 'lr': 8.599088593017724e-08, 'epoch': 18.82}\n","{'loss': 3.0922, 'lr': 8.23806017550527e-08, 'epoch': 18.85}\n","{'loss': 2.9162, 'lr': 7.884711026201586e-08, 'epoch': 18.88}\n","{'loss': 3.0497, 'lr': 7.539046662944972e-08, 'epoch': 18.9}\n","{'loss': 2.6335, 'lr': 7.201072483569549e-08, 'epoch': 18.93}\n","{'loss': 2.8745, 'lr': 6.870793765820783e-08, 'epoch': 18.95}\n","{'loss': 2.95, 'lr': 6.548215667273206e-08, 'epoch': 18.98}\n","{'loss': 2.9104, 'grad_norm': 14.2832670211792, 'learning_rate': 6.233343225249933e-08, 'epoch': 19.0}\n"," 95% 760/800 [03:32<00:10,  3.90it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.7733, 'lr': 6.233343225249933e-08, 'epoch': 19.0}\n"," 95% 760/800 [03:34<00:10,  3.90it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.626, 'lr': 5.92618135674361e-08, 'epoch': 19.02}\n","{'loss': 2.7231, 'lr': 5.626734858340255e-08, 'epoch': 19.05}\n","{'loss': 2.8974, 'lr': 5.3350084061438157e-08, 'epoch': 19.07}\n","{'loss': 3.0905, 'lr': 5.0510065557034526e-08, 'epoch': 19.1}\n","{'loss': 2.7927, 'lr': 4.774733741942206e-08, 'epoch': 19.12}\n","{'loss': 2.7577, 'lr': 4.5061942790879386e-08, 'epoch': 19.15}\n","{'loss': 3.2666, 'lr': 4.245392360605727e-08, 'epoch': 19.18}\n","{'loss': 2.8659, 'grad_norm': 11.629655838012695, 'learning_rate': 3.992332059132631e-08, 'epoch': 19.2}\n","{'loss': 3.0511, 'lr': 3.992332059132631e-08, 'epoch': 19.2}\n","{'loss': 2.7399, 'lr': 3.747017326413971e-08, 'epoch': 19.23}\n","{'loss': 3.0413, 'lr': 3.5094519932415417e-08, 'epoch': 19.25}\n","{'loss': 3.0648, 'lr': 3.279639769393939e-08, 'epoch': 19.27}\n","{'loss': 3.3413, 'lr': 3.0575842435785486e-08, 'epoch': 19.3}\n","{'loss': 2.6936, 'lr': 2.843288883375539e-08, 'epoch': 19.32}\n","{'loss': 3.1175, 'lr': 2.6367570351836235e-08, 'epoch': 19.35}\n","{'loss': 2.9348, 'lr': 2.4379919241679373e-08, 'epoch': 19.38}\n","{'loss': 2.998, 'grad_norm': 11.83615493774414, 'learning_rate': 2.2469966542096323e-08, 'epoch': 19.4}\n","{'loss': 2.7486, 'lr': 2.2469966542096323e-08, 'epoch': 19.4}\n","{'loss': 3.6493, 'lr': 2.0637742078573608e-08, 'epoch': 19.43}\n","{'loss': 2.3764, 'lr': 1.8883274462806467e-08, 'epoch': 19.45}\n","{'loss': 2.8083, 'lr': 1.7206591092253642e-08, 'epoch': 19.48}\n","{'loss': 2.5422, 'lr': 1.560771814970885e-08, 'epoch': 19.5}\n","{'loss': 3.3429, 'lr': 1.4086680602891645e-08, 'epoch': 19.52}\n","{'loss': 2.8444, 'lr': 1.264350220405719e-08, 'epoch': 19.55}\n","{'loss': 2.8381, 'lr': 1.1278205489626547e-08, 'epoch': 19.57}\n","{'loss': 2.8938, 'grad_norm': 12.880125045776367, 'learning_rate': 9.99081177983363e-09, 'epoch': 19.6}\n","{'loss': 2.8513, 'lr': 9.99081177983363e-09, 'epoch': 19.6}\n","{'loss': 3.2584, 'lr': 8.781341178393244e-09, 'epoch': 19.62}\n","{'loss': 2.6906, 'lr': 7.649812572185222e-09, 'epoch': 19.65}\n","{'loss': 2.9381, 'lr': 6.596243630963006e-09, 'epoch': 19.68}\n","{'loss': 2.7149, 'lr': 5.620650807073857e-09, 'epoch': 19.7}\n","{'loss': 2.8785, 'lr': 4.723049335204066e-09, 'epoch': 19.73}\n","{'loss': 3.1485, 'lr': 3.903453232140808e-09, 'epoch': 19.75}\n","{'loss': 3.1446, 'lr': 3.1618752965534295e-09, 'epoch': 19.77}\n","{'loss': 2.9531, 'grad_norm': 11.570451736450195, 'learning_rate': 2.4983271087924976e-09, 'epoch': 19.8}\n","{'loss': 2.2751, 'lr': 2.4983271087924976e-09, 'epoch': 19.8}\n","{'loss': 3.0836, 'lr': 1.9128190307105e-09, 'epoch': 19.82}\n","{'loss': 2.7685, 'lr': 1.4053602054991954e-09, 'epoch': 19.85}\n","{'loss': 3.1232, 'lr': 9.75958557545842e-10, 'epoch': 19.88}\n","{'loss': 2.9634, 'lr': 6.246207923116255e-10, 'epoch': 19.9}\n","{'loss': 2.8169, 'lr': 3.513523962256349e-10, 'epoch': 19.93}\n","{'loss': 2.7538, 'lr': 1.5615763659881932e-10, 'epoch': 19.95}\n","{'loss': 3.547, 'lr': 3.903956155848487e-11, 'epoch': 19.98}\n","{'loss': 2.9164, 'grad_norm': 13.036598205566406, 'learning_rate': 0.0, 'epoch': 20.0}\n","{'train_runtime': 226.0393, 'train_samples_per_second': 7.078, 'train_steps_per_second': 3.539, 'train_loss': 11.187841498851776, 'epoch': 20.0}\n","100% 800/800 [03:46<00:00,  3.54it/s]\n","******model_save_path is model7b_M1_family_epoch_20_r_64_moreData_lr_1e-5/adapter_model.safetensors******\n"]}],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 20 \\\n","    --json_path \"./dataset/family_k1k2.json\" \\\n","    --output_dir \"model7b_M1_family_epoch_20_r_64_moreData_lr_1e-5\" \\\n","    --save_steps 40 \\\n","    --LORA_R 64 \\\n","    --lr 1e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":70149,"status":"ok","timestamp":1724763330801,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"2DYd5sHN5q86","outputId":"cc104bdc-203f-4b50-b9fb-2ef546f12a51"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","auto\n","False\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.00it/s]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 80/80 [00:00<00:00, 2122.49 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3087: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n","  0% 0/1600 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_rng_state = torch.load(rng_file)\n","/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.357, 'lr': 0.0, 'epoch': 20.0}\n","  0% 0/1600 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.7422, 'lr': 5.014772357757693e-06, 'epoch': 20.02}\n","{'loss': 3.2648, 'lr': 5.004924125620316e-06, 'epoch': 20.05}\n","{'loss': 2.6902, 'lr': 4.995075874379685e-06, 'epoch': 20.07}\n","{'loss': 3.6905, 'lr': 4.9852276422423084e-06, 'epoch': 20.1}\n","{'loss': 2.9954, 'lr': 4.975379467414621e-06, 'epoch': 20.12}\n","{'loss': 2.7334, 'lr': 4.96553138810283e-06, 'epoch': 20.15}\n","{'loss': 2.535, 'lr': 4.955683442512783e-06, 'epoch': 20.18}\n","{'loss': 3.001, 'grad_norm': 12.979037284851074, 'learning_rate': 4.945835668849801e-06, 'epoch': 20.2}\n","{'loss': 2.983, 'lr': 4.945835668849801e-06, 'epoch': 20.2}\n","{'loss': 2.6817, 'lr': 4.935988105318538e-06, 'epoch': 20.23}\n","{'loss': 2.6337, 'lr': 4.926140790122835e-06, 'epoch': 20.25}\n","{'loss': 2.8968, 'lr': 4.9162937614655695e-06, 'epoch': 20.27}\n","{'loss': 2.8546, 'lr': 4.906447057548505e-06, 'epoch': 20.3}\n","{'loss': 2.9426, 'lr': 4.896600716572149e-06, 'epoch': 20.32}\n","{'loss': 3.2625, 'lr': 4.886754776735599e-06, 'epoch': 20.35}\n","{'loss': 2.7459, 'lr': 4.876909276236395e-06, 'epoch': 20.38}\n","{'loss': 2.8751, 'grad_norm': 12.136545181274414, 'learning_rate': 4.867064253270374e-06, 'epoch': 20.4}\n","{'loss': 2.6922, 'lr': 4.867064253270374e-06, 'epoch': 20.4}\n","{'loss': 2.8403, 'lr': 4.85721974603152e-06, 'epoch': 20.43}\n","{'loss': 2.68, 'lr': 4.847375792711816e-06, 'epoch': 20.45}\n","{'loss': 2.5941, 'lr': 4.837532431501098e-06, 'epoch': 20.48}\n","{'loss': 2.74, 'lr': 4.827689700586902e-06, 'epoch': 20.5}\n","{'loss': 2.8926, 'lr': 4.817847638154322e-06, 'epoch': 20.52}\n","{'loss': 3.0895, 'lr': 4.808006282385855e-06, 'epoch': 20.55}\n","{'loss': 3.2316, 'lr': 4.798165671461258e-06, 'epoch': 20.57}\n","{'loss': 2.845, 'grad_norm': 11.443754196166992, 'learning_rate': 4.788325843557398e-06, 'epoch': 20.6}\n","{'loss': 2.9876, 'lr': 4.788325843557398e-06, 'epoch': 20.6}\n","{'loss': 2.9952, 'lr': 4.778486836848107e-06, 'epoch': 20.62}\n","{'loss': 2.7338, 'lr': 4.768648689504028e-06, 'epoch': 20.65}\n","{'loss': 3.0707, 'lr': 4.758811439692472e-06, 'epoch': 20.68}\n","{'loss': 2.9767, 'lr': 4.748975125577265e-06, 'epoch': 20.7}\n","{'loss': 2.7364, 'lr': 4.739139785318604e-06, 'epoch': 20.73}\n","{'loss': 2.958, 'lr': 4.729305457072913e-06, 'epoch': 20.75}\n","{'loss': 3.2829, 'lr': 4.719472178992682e-06, 'epoch': 20.77}\n","{'loss': 2.9677, 'grad_norm': 13.72745132446289, 'learning_rate': 4.709639989226335e-06, 'epoch': 20.8}\n","{'loss': 2.7913, 'lr': 4.709639989226335e-06, 'epoch': 20.8}\n","{'loss': 3.6743, 'lr': 4.6998089259180655e-06, 'epoch': 20.82}\n","{'loss': 2.7255, 'lr': 4.689979027207702e-06, 'epoch': 20.85}\n","{'loss': 2.7928, 'lr': 4.680150331230552e-06, 'epoch': 20.88}\n","{'loss': 2.5836, 'lr': 4.670322876117263e-06, 'epoch': 20.9}\n","{'loss': 2.9721, 'lr': 4.660496699993662e-06, 'epoch': 20.93}\n","{'loss': 3.5824, 'lr': 4.650671840980616e-06, 'epoch': 20.95}\n","{'loss': 2.4143, 'lr': 4.6408483371938825e-06, 'epoch': 20.98}\n","{'loss': 2.942, 'grad_norm': 9.659832954406738, 'learning_rate': 4.631026226743962e-06, 'epoch': 21.0}\n"," 52% 840/1600 [00:10<00:01, 732.49it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.6969, 'lr': 4.631026226743962e-06, 'epoch': 21.0}\n"," 52% 840/1600 [00:11<00:01, 732.49it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.7956, 'lr': 4.621205547735949e-06, 'epoch': 21.02}\n","{'loss': 2.8748, 'lr': 4.611386338269384e-06, 'epoch': 21.05}\n","{'loss': 3.1127, 'lr': 4.60156863643811e-06, 'epoch': 21.07}\n","{'loss': 2.662, 'lr': 4.591752480330116e-06, 'epoch': 21.1}\n","{'loss': 2.7579, 'lr': 4.581937908027397e-06, 'epoch': 21.12}\n","{'loss': 2.9705, 'lr': 4.572124957605803e-06, 'epoch': 21.15}\n","{'loss': 3.6511, 'lr': 4.562313667134889e-06, 'epoch': 21.18}\n","{'loss': 2.9402, 'grad_norm': 12.601543426513672, 'learning_rate': 4.552504074677779e-06, 'epoch': 21.2}\n","{'loss': 2.6636, 'lr': 4.552504074677779e-06, 'epoch': 21.2}\n","{'loss': 2.9713, 'lr': 4.542696218291002e-06, 'epoch': 21.23}\n","{'loss': 3.02, 'lr': 4.532890136024351e-06, 'epoch': 21.25}\n","{'loss': 2.7738, 'lr': 4.52308586592074e-06, 'epoch': 21.27}\n","{'loss': 2.842, 'lr': 4.513283446016052e-06, 'epoch': 21.3}\n","{'loss': 2.5744, 'lr': 4.503482914338992e-06, 'epoch': 21.32}\n","{'loss': 2.4551, 'lr': 4.4936843089109375e-06, 'epoch': 21.35}\n","{'loss': 2.9872, 'lr': 4.483887667745798e-06, 'epoch': 21.38}\n","{'loss': 2.7859, 'grad_norm': 12.840127944946289, 'learning_rate': 4.4740930288498576e-06, 'epoch': 21.4}\n","{'loss': 2.8733, 'lr': 4.4740930288498576e-06, 'epoch': 21.4}\n","{'loss': 3.0218, 'lr': 4.4643004302216345e-06, 'epoch': 21.43}\n","{'loss': 2.9406, 'lr': 4.45450990985173e-06, 'epoch': 21.45}\n","{'loss': 3.2333, 'lr': 4.444721505722685e-06, 'epoch': 21.48}\n","{'loss': 2.6383, 'lr': 4.434935255808831e-06, 'epoch': 21.5}\n","{'loss': 2.6398, 'lr': 4.4251511980761404e-06, 'epoch': 21.52}\n","{'loss': 3.1322, 'lr': 4.41536937048208e-06, 'epoch': 21.55}\n","{'loss': 3.0853, 'lr': 4.4055898109754684e-06, 'epoch': 21.57}\n","{'loss': 2.9456, 'grad_norm': 11.726829528808594, 'learning_rate': 4.3958125574963215e-06, 'epoch': 21.6}\n","{'loss': 2.9533, 'lr': 4.3958125574963215e-06, 'epoch': 21.6}\n","{'loss': 2.6727, 'lr': 4.386037647975708e-06, 'epoch': 21.62}\n","{'loss': 2.818, 'lr': 4.37626512033561e-06, 'epoch': 21.65}\n","{'loss': 2.7208, 'lr': 4.3664950124887615e-06, 'epoch': 21.68}\n","{'loss': 2.7981, 'lr': 4.356727362338513e-06, 'epoch': 21.7}\n","{'loss': 3.109, 'lr': 4.346962207778679e-06, 'epoch': 21.73}\n","{'loss': 2.3475, 'lr': 4.337199586693389e-06, 'epoch': 21.75}\n","{'loss': 2.7929, 'lr': 4.327439536956953e-06, 'epoch': 21.77}\n","{'loss': 2.7765, 'grad_norm': 12.147741317749023, 'learning_rate': 4.317682096433697e-06, 'epoch': 21.8}\n","{'loss': 2.7898, 'lr': 4.317682096433697e-06, 'epoch': 21.8}\n","{'loss': 2.4466, 'lr': 4.3079273029778276e-06, 'epoch': 21.82}\n","{'loss': 2.8032, 'lr': 4.298175194433279e-06, 'epoch': 21.85}\n","{'loss': 2.8385, 'lr': 4.2884258086335755e-06, 'epoch': 21.88}\n","{'loss': 2.7416, 'lr': 4.27867918340167e-06, 'epoch': 21.9}\n","{'loss': 3.0488, 'lr': 4.268935356549817e-06, 'epoch': 21.93}\n","{'loss': 2.5956, 'lr': 4.2591943658794044e-06, 'epoch': 21.95}\n","{'loss': 2.5474, 'lr': 4.249456249180821e-06, 'epoch': 21.98}\n","{'loss': 2.7264, 'grad_norm': 11.347301483154297, 'learning_rate': 4.239721044233306e-06, 'epoch': 22.0}\n"," 55% 880/1600 [00:21<00:19, 36.15it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.9039, 'lr': 4.239721044233306e-06, 'epoch': 22.0}\n"," 55% 880/1600 [00:23<00:19, 36.15it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.5048, 'lr': 4.229988788804802e-06, 'epoch': 22.02}\n","{'loss': 2.6411, 'lr': 4.220259520651807e-06, 'epoch': 22.05}\n","{'loss': 2.3716, 'lr': 4.2105332775192355e-06, 'epoch': 22.07}\n","{'loss': 2.8985, 'lr': 4.200810097140261e-06, 'epoch': 22.1}\n","{'loss': 2.5732, 'lr': 4.191090017236177e-06, 'epoch': 22.12}\n","{'loss': 2.8146, 'lr': 4.181373075516248e-06, 'epoch': 22.15}\n","{'loss': 2.826, 'lr': 4.1716593096775636e-06, 'epoch': 22.18}\n","{'loss': 2.6917, 'grad_norm': 12.435490608215332, 'learning_rate': 4.161948757404892e-06, 'epoch': 22.2}\n","{'loss': 3.1345, 'lr': 4.161948757404892e-06, 'epoch': 22.2}\n","{'loss': 2.9248, 'lr': 4.152241456370539e-06, 'epoch': 22.23}\n","{'loss': 3.5572, 'lr': 4.142537444234192e-06, 'epoch': 22.25}\n","{'loss': 2.8451, 'lr': 4.13283675864278e-06, 'epoch': 22.27}\n","{'loss': 2.6316, 'lr': 4.123139437230326e-06, 'epoch': 22.3}\n","{'loss': 2.975, 'lr': 4.113445517617805e-06, 'epoch': 22.32}\n","{'loss': 2.668, 'lr': 4.1037550374129885e-06, 'epoch': 22.35}\n","{'loss': 3.1659, 'lr': 4.094068034210313e-06, 'epoch': 22.38}\n","{'loss': 2.9878, 'grad_norm': 15.48081111907959, 'learning_rate': 4.0843845455907195e-06, 'epoch': 22.4}\n","{'loss': 3.1088, 'lr': 4.0843845455907195e-06, 'epoch': 22.4}\n","{'loss': 2.896, 'lr': 4.074704609121517e-06, 'epoch': 22.43}\n","{'loss': 2.7103, 'lr': 4.0650282623562335e-06, 'epoch': 22.45}\n","{'loss': 3.2212, 'lr': 4.055355542834467e-06, 'epoch': 22.48}\n","{'loss': 2.8758, 'lr': 4.045686488081748e-06, 'epoch': 22.5}\n","{'loss': 2.8058, 'lr': 4.036021135609391e-06, 'epoch': 22.52}\n","{'loss': 2.793, 'lr': 4.026359522914341e-06, 'epoch': 22.55}\n","{'loss': 2.8373, 'lr': 4.016701687479041e-06, 'epoch': 22.57}\n","{'loss': 2.906, 'grad_norm': 13.590105056762695, 'learning_rate': 4.007047666771274e-06, 'epoch': 22.6}\n","{'loss': 2.802, 'lr': 4.007047666771274e-06, 'epoch': 22.6}\n","{'loss': 2.8501, 'lr': 3.997397498244028e-06, 'epoch': 22.62}\n","{'loss': 2.523, 'lr': 3.987751219335344e-06, 'epoch': 22.65}\n","{'loss': 2.4581, 'lr': 3.9781088674681764e-06, 'epoch': 22.68}\n","{'loss': 2.8437, 'lr': 3.968470480050241e-06, 'epoch': 22.7}\n","{'loss': 2.5977, 'lr': 3.958836094473874e-06, 'epoch': 22.73}\n","{'loss': 2.4403, 'lr': 3.9492057481158905e-06, 'epoch': 22.75}\n","{'loss': 2.4913, 'lr': 3.939579478337426e-06, 'epoch': 22.77}\n","{'loss': 2.6258, 'grad_norm': 13.285663604736328, 'learning_rate': 3.929957322483813e-06, 'epoch': 22.8}\n","{'loss': 2.5179, 'lr': 3.929957322483813e-06, 'epoch': 22.8}\n","{'loss': 2.8879, 'lr': 3.9203393178844165e-06, 'epoch': 22.82}\n","{'loss': 2.4736, 'lr': 3.910725501852495e-06, 'epoch': 22.85}\n","{'loss': 2.5906, 'lr': 3.901115911685063e-06, 'epoch': 22.88}\n","{'loss': 2.5941, 'lr': 3.891510584662736e-06, 'epoch': 22.9}\n","{'loss': 2.8306, 'lr': 3.881909558049592e-06, 'epoch': 22.93}\n","{'loss': 2.55, 'lr': 3.87231286909303e-06, 'epoch': 22.95}\n","{'loss': 2.2625, 'lr': 3.8627205550236115e-06, 'epoch': 22.98}\n","{'loss': 2.5884, 'grad_norm': 12.621036529541016, 'learning_rate': 3.853132653054936e-06, 'epoch': 23.0}\n"," 57% 920/1600 [00:33<00:45, 14.97it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.5427, 'lr': 3.853132653054936e-06, 'epoch': 23.0}\n"," 57% 920/1600 [00:34<00:45, 14.97it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.9628, 'lr': 3.843549200383478e-06, 'epoch': 23.02}\n","{'loss': 2.5123, 'lr': 3.833970234188455e-06, 'epoch': 23.05}\n","{'loss': 2.3193, 'lr': 3.8243957916316755e-06, 'epoch': 23.07}\n","{'loss': 2.4531, 'lr': 3.814825909857405e-06, 'epoch': 23.1}\n","{'loss': 3.2372, 'lr': 3.8052606259922097e-06, 'epoch': 23.12}\n","{'loss': 2.8601, 'lr': 3.7956999771448195e-06, 'epoch': 23.15}\n","{'loss': 2.3752, 'lr': 3.7861440004059825e-06, 'epoch': 23.18}\n","{'loss': 2.6578, 'grad_norm': 14.873473167419434, 'learning_rate': 3.7765927328483216e-06, 'epoch': 23.2}\n","{'loss': 2.7512, 'lr': 3.7765927328483216e-06, 'epoch': 23.2}\n","{'loss': 2.749, 'lr': 3.767046211526191e-06, 'epoch': 23.23}\n","{'loss': 2.5964, 'lr': 3.75750447347553e-06, 'epoch': 23.25}\n","{'loss': 2.4098, 'lr': 3.7479675557137253e-06, 'epoch': 23.27}\n","{'loss': 2.8331, 'lr': 3.7384354952394577e-06, 'epoch': 23.3}\n","{'loss': 2.4133, 'lr': 3.7289083290325668e-06, 'epoch': 23.32}\n","{'loss': 2.7546, 'lr': 3.719386094053904e-06, 'epoch': 23.35}\n","{'loss': 2.7579, 'lr': 3.7098688272451893e-06, 'epoch': 23.38}\n","{'loss': 2.6581, 'grad_norm': 13.068785667419434, 'learning_rate': 3.700356565528873e-06, 'epoch': 23.4}\n","{'loss': 3.0375, 'lr': 3.700356565528873e-06, 'epoch': 23.4}\n","{'loss': 3.0576, 'lr': 3.690849345807983e-06, 'epoch': 23.43}\n","{'loss': 2.364, 'lr': 3.681347204965988e-06, 'epoch': 23.45}\n","{'loss': 2.8983, 'lr': 3.671850179866652e-06, 'epoch': 23.48}\n","{'loss': 2.6789, 'lr': 3.662358307353897e-06, 'epoch': 23.5}\n","{'loss': 2.543, 'lr': 3.652871624251648e-06, 'epoch': 23.52}\n","{'loss': 2.8377, 'lr': 3.6433901673637064e-06, 'epoch': 23.55}\n","{'loss': 2.5254, 'lr': 3.6339139734735916e-06, 'epoch': 23.57}\n","{'loss': 2.7428, 'grad_norm': 11.819825172424316, 'learning_rate': 3.624443079344407e-06, 'epoch': 23.6}\n","{'loss': 2.8483, 'lr': 3.624443079344407e-06, 'epoch': 23.6}\n","{'loss': 2.6767, 'lr': 3.6149775217186954e-06, 'epoch': 23.62}\n","{'loss': 2.6765, 'lr': 3.6055173373182978e-06, 'epoch': 23.65}\n","{'loss': 2.5761, 'lr': 3.5960625628442057e-06, 'epoch': 23.68}\n","{'loss': 2.8341, 'lr': 3.5866132349764304e-06, 'epoch': 23.7}\n","{'loss': 2.6715, 'lr': 3.577169390373845e-06, 'epoch': 23.73}\n","{'loss': 2.865, 'lr': 3.5677310656740537e-06, 'epoch': 23.75}\n","{'loss': 2.9018, 'lr': 3.5582982974932467e-06, 'epoch': 23.77}\n","{'loss': 2.7562, 'grad_norm': 15.640892028808594, 'learning_rate': 3.548871122426057e-06, 'epoch': 23.8}\n","{'loss': 2.575, 'lr': 3.548871122426057e-06, 'epoch': 23.8}\n","{'loss': 2.6303, 'lr': 3.539449577045415e-06, 'epoch': 23.82}\n","{'loss': 2.7804, 'lr': 3.530033697902419e-06, 'epoch': 23.85}\n","{'loss': 2.8165, 'lr': 3.5206235215261785e-06, 'epoch': 23.88}\n","{'loss': 2.7684, 'lr': 3.5112190844236805e-06, 'epoch': 23.9}\n","{'loss': 2.5362, 'lr': 3.501820423079646e-06, 'epoch': 23.93}\n","{'loss': 2.7088, 'lr': 3.492427573956388e-06, 'epoch': 23.95}\n","{'loss': 2.9994, 'lr': 3.483040573493676e-06, 'epoch': 23.98}\n","{'loss': 2.7269, 'grad_norm': 16.430362701416016, 'learning_rate': 3.4736594581085837e-06, 'epoch': 24.0}\n"," 60% 960/1600 [00:44<02:22,  4.50it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.7887, 'lr': 3.4736594581085837e-06, 'epoch': 24.0}\n"," 60% 960/1600 [00:45<02:22,  4.50it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.7274, 'lr': 3.464284264195355e-06, 'epoch': 24.02}\n","{'loss': 2.9886, 'lr': 3.4549150281252635e-06, 'epoch': 24.05}\n","{'loss': 2.6157, 'lr': 3.445551786246467e-06, 'epoch': 24.07}\n","{'loss': 2.7474, 'lr': 3.4361945748838664e-06, 'epoch': 24.1}\n","{'loss': 2.6254, 'lr': 3.4268434303389747e-06, 'epoch': 24.12}\n","{'loss': 3.1034, 'lr': 3.417498388889762e-06, 'epoch': 24.15}\n","{'loss': 3.0049, 'lr': 3.408159486790522e-06, 'epoch': 24.18}\n","{'loss': 2.8252, 'grad_norm': 12.596510887145996, 'learning_rate': 3.398826760271734e-06, 'epoch': 24.2}\n","{'loss': 3.0326, 'lr': 3.398826760271734e-06, 'epoch': 24.2}\n","{'loss': 2.5718, 'lr': 3.389500245539914e-06, 'epoch': 24.23}\n","{'loss': 2.5746, 'lr': 3.380179978777482e-06, 'epoch': 24.25}\n","{'loss': 2.8277, 'lr': 3.3708659961426227e-06, 'epoch': 24.27}\n","{'loss': 2.503, 'lr': 3.3615583337691344e-06, 'epoch': 24.3}\n","{'loss': 2.7306, 'lr': 3.3522570277662986e-06, 'epoch': 24.32}\n","{'loss': 2.3447, 'lr': 3.3429621142187403e-06, 'epoch': 24.35}\n","{'loss': 2.6793, 'lr': 3.33367362918628e-06, 'epoch': 24.38}\n","{'loss': 2.658, 'grad_norm': 14.089570045471191, 'learning_rate': 3.3243916087037988e-06, 'epoch': 24.4}\n","{'loss': 3.2687, 'lr': 3.3243916087037988e-06, 'epoch': 24.4}\n","{'loss': 2.416, 'lr': 3.3151160887811047e-06, 'epoch': 24.43}\n"," 61% 977/1600 [00:50<03:16,  3.18it/s]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage/gemma_sft/./ft_gemma/train_7b_save.py\", line 442, in <module>\n","    trainer.train(resume_from_checkpoint=flag_checkpoint)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1932, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2268, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3324, in training_step\n","    self.accelerator.backward(loss, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2151, in backward\n","    loss.backward(**kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 521, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 289, in backward\n","    _engine_run_backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 768, in _engine_run_backward\n","    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"," 61%|██████    | 977/1600 [00:51<00:32, 19.03it/s]\n"]}],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 40 \\\n","    --json_path \"./dataset/family_k1k2.json\" \\\n","    --output_dir \"model7b_M1_family_epoch_20_r_64_moreData_lr_1e-5\" \\\n","    --save_steps 40 \\\n","    --LORA_R 64 \\\n","    --lr 1e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1124444,"status":"ok","timestamp":1724764478377,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"rKDI4xPs6Ciq","outputId":"d07c0aea-26b1-4854-ccf4-e33c5c9ccff7"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.42s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.18s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n"]}],"source":["!./predict_M2k3.sh"]},{"cell_type":"markdown","metadata":{"id":"MCKpjg9WBVIV"},"source":["function kkkk() {\n","    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n","}\n","setInterval(kkkk, 60000);"]},{"cell_type":"markdown","metadata":{"id":"5z7ZbBE3IzFz"},"source":["## 1e-4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":239279,"status":"ok","timestamp":1724766664344,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"J1jE1YEpFq2K","outputId":"06c9a193-3f2d-4bb5-d03b-da128a85a9a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","auto\n","False\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.03it/s]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 80/80 [00:00<00:00, 2200.10 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","  0% 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 40.6529, 'lr': 0.0, 'epoch': 0}\n","  0% 0/800 [00:01<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 40.6529, 'grad_norm': 10.157275199890137, 'learning_rate': 2e-05, 'epoch': 0.03}\n","{'loss': 37.6233, 'lr': 2e-05, 'epoch': 0.03}\n","{'loss': 37.0902, 'lr': 4e-05, 'epoch': 0.05}\n","{'loss': 39.9717, 'lr': 6e-05, 'epoch': 0.07}\n","{'loss': 38.6204, 'lr': 8e-05, 'epoch': 0.1}\n","{'loss': 33.7474, 'lr': 0.0001, 'epoch': 0.12}\n","{'loss': 32.8553, 'lr': 9.999960960438442e-05, 'epoch': 0.15}\n","{'loss': 30.776, 'lr': 9.999843842363401e-05, 'epoch': 0.17}\n","{'loss': 35.812, 'grad_norm': 9.006942749023438, 'learning_rate': 9.999648647603774e-05, 'epoch': 0.2}\n","{'loss': 33.1463, 'lr': 9.999648647603774e-05, 'epoch': 0.2}\n","{'loss': 33.423, 'lr': 9.99937537920769e-05, 'epoch': 0.23}\n","{'loss': 33.8934, 'lr': 9.999024041442456e-05, 'epoch': 0.25}\n","{'loss': 30.8125, 'lr': 9.998594639794501e-05, 'epoch': 0.28}\n","{'loss': 31.0453, 'lr': 9.998087180969289e-05, 'epoch': 0.3}\n","{'loss': 28.4986, 'lr': 9.997501672891207e-05, 'epoch': 0.33}\n","{'loss': 28.0687, 'lr': 9.996838124703447e-05, 'epoch': 0.35}\n","{'loss': 27.6934, 'lr': 9.99609654676786e-05, 'epoch': 0.38}\n","{'loss': 30.8227, 'grad_norm': 18.05828094482422, 'learning_rate': 9.995276950664796e-05, 'epoch': 0.4}\n","{'loss': 27.6615, 'lr': 9.995276950664796e-05, 'epoch': 0.4}\n","{'loss': 26.4448, 'lr': 9.994379349192926e-05, 'epoch': 0.42}\n","{'loss': 27.1647, 'lr': 9.993403756369038e-05, 'epoch': 0.45}\n","{'loss': 26.8942, 'lr': 9.992350187427815e-05, 'epoch': 0.47}\n","{'loss': 24.6746, 'lr': 9.991218658821608e-05, 'epoch': 0.5}\n","{'loss': 24.2788, 'lr': 9.990009188220167e-05, 'epoch': 0.53}\n","{'loss': 25.7011, 'lr': 9.988721794510374e-05, 'epoch': 0.55}\n","{'loss': 22.3312, 'lr': 9.987356497795943e-05, 'epoch': 0.57}\n","{'loss': 25.6439, 'grad_norm': 21.587993621826172, 'learning_rate': 9.985913319397109e-05, 'epoch': 0.6}\n","{'loss': 23.2244, 'lr': 9.985913319397109e-05, 'epoch': 0.6}\n","{'loss': 20.8694, 'lr': 9.984392281850293e-05, 'epoch': 0.62}\n","{'loss': 20.3828, 'lr': 9.982793408907747e-05, 'epoch': 0.65}\n","{'loss': 19.2997, 'lr': 9.981116725537194e-05, 'epoch': 0.68}\n","{'loss': 17.2892, 'lr': 9.979362257921427e-05, 'epoch': 0.7}\n","{'loss': 17.0404, 'lr': 9.977530033457905e-05, 'epoch': 0.72}\n","{'loss': 16.1494, 'lr': 9.97562008075832e-05, 'epoch': 0.75}\n","{'loss': 14.3491, 'lr': 9.973632429648165e-05, 'epoch': 0.78}\n","{'loss': 18.5756, 'grad_norm': 22.448904037475586, 'learning_rate': 9.971567111166246e-05, 'epoch': 0.8}\n","{'loss': 13.62, 'lr': 9.971567111166246e-05, 'epoch': 0.8}\n","{'loss': 14.0385, 'lr': 9.969424157564215e-05, 'epoch': 0.82}\n","{'loss': 11.7283, 'lr': 9.967203602306061e-05, 'epoch': 0.85}\n","{'loss': 10.9371, 'lr': 9.964905480067586e-05, 'epoch': 0.88}\n","{'loss': 9.7186, 'lr': 9.96252982673586e-05, 'epoch': 0.9}\n","{'loss': 8.3768, 'lr': 9.960076679408674e-05, 'epoch': 0.93}\n","{'loss': 7.3525, 'lr': 9.957546076393943e-05, 'epoch': 0.95}\n","{'loss': 7.0772, 'lr': 9.954938057209121e-05, 'epoch': 0.97}\n","{'loss': 10.3561, 'grad_norm': 35.30156707763672, 'learning_rate': 9.952252662580579e-05, 'epoch': 1.0}\n","  5% 40/800 [00:10<02:54,  4.37it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 5.4936, 'lr': 9.952252662580579e-05, 'epoch': 1.0}\n","  5% 40/800 [00:11<02:54,  4.37it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 4.0881, 'lr': 9.949489934442966e-05, 'epoch': 1.02}\n","{'loss': 4.8873, 'lr': 9.946649915938562e-05, 'epoch': 1.05}\n","{'loss': 4.1566, 'lr': 9.943732651416597e-05, 'epoch': 1.07}\n","{'loss': 4.1377, 'lr': 9.940738186432565e-05, 'epoch': 1.1}\n","{'loss': 4.2575, 'lr': 9.937666567747501e-05, 'epoch': 1.12}\n","{'loss': 5.2115, 'lr': 9.934517843327269e-05, 'epoch': 1.15}\n","{'loss': 4.6365, 'lr': 9.931292062341793e-05, 'epoch': 1.18}\n","{'loss': 4.6086, 'grad_norm': 8.471946716308594, 'learning_rate': 9.927989275164305e-05, 'epoch': 1.2}\n","{'loss': 4.6764, 'lr': 9.927989275164305e-05, 'epoch': 1.2}\n","{'loss': 4.1272, 'lr': 9.924609533370551e-05, 'epoch': 1.23}\n","{'loss': 4.0704, 'lr': 9.921152889737984e-05, 'epoch': 1.25}\n","{'loss': 4.2584, 'lr': 9.917619398244949e-05, 'epoch': 1.27}\n","{'loss': 4.1652, 'lr': 9.914009114069824e-05, 'epoch': 1.3}\n","{'loss': 3.3524, 'lr': 9.910322093590177e-05, 'epoch': 1.32}\n","{'loss': 3.7051, 'lr': 9.90655839438187e-05, 'epoch': 1.35}\n","{'loss': 3.8775, 'lr': 9.902718075218176e-05, 'epoch': 1.38}\n","{'loss': 4.0291, 'grad_norm': 4.830753326416016, 'learning_rate': 9.898801196068839e-05, 'epoch': 1.4}\n","{'loss': 3.9004, 'lr': 9.898801196068839e-05, 'epoch': 1.4}\n","{'loss': 3.5713, 'lr': 9.89480781809916e-05, 'epoch': 1.43}\n","{'loss': 3.7848, 'lr': 9.890738003669029e-05, 'epoch': 1.45}\n","{'loss': 4.129, 'lr': 9.886591816331954e-05, 'epoch': 1.48}\n","{'loss': 3.5189, 'lr': 9.882369320834069e-05, 'epoch': 1.5}\n","{'loss': 3.3428, 'lr': 9.878070583113123e-05, 'epoch': 1.52}\n","{'loss': 3.7224, 'lr': 9.87369567029745e-05, 'epoch': 1.55}\n","{'loss': 3.464, 'lr': 9.869244650704923e-05, 'epoch': 1.57}\n","{'loss': 3.6792, 'grad_norm': 5.881464004516602, 'learning_rate': 9.864717593841883e-05, 'epoch': 1.6}\n","{'loss': 3.3711, 'lr': 9.864717593841883e-05, 'epoch': 1.6}\n","{'loss': 3.6466, 'lr': 9.860114570402054e-05, 'epoch': 1.62}\n","{'loss': 3.4969, 'lr': 9.855435652265446e-05, 'epoch': 1.65}\n","{'loss': 3.564, 'lr': 9.85068091249722e-05, 'epoch': 1.68}\n","{'loss': 3.713, 'lr': 9.845850425346563e-05, 'epoch': 1.7}\n","{'loss': 3.4635, 'lr': 9.840944266245511e-05, 'epoch': 1.73}\n","{'loss': 3.3883, 'lr': 9.835962511807786e-05, 'epoch': 1.75}\n","{'loss': 3.753, 'lr': 9.830905239827593e-05, 'epoch': 1.77}\n","{'loss': 3.5495, 'grad_norm': 4.959644317626953, 'learning_rate': 9.825772529278401e-05, 'epoch': 1.8}\n","{'loss': 3.8839, 'lr': 9.825772529278401e-05, 'epoch': 1.8}\n","{'loss': 3.5733, 'lr': 9.820564460311718e-05, 'epoch': 1.82}\n","{'loss': 3.3017, 'lr': 9.815281114255841e-05, 'epoch': 1.85}\n","{'loss': 3.1348, 'lr': 9.809922573614569e-05, 'epoch': 1.88}\n","{'loss': 3.7886, 'lr': 9.804488922065937e-05, 'epoch': 1.9}\n","{'loss': 3.6033, 'lr': 9.798980244460893e-05, 'epoch': 1.93}\n","{'loss': 3.3837, 'lr': 9.79339662682198e-05, 'epoch': 1.95}\n","{'loss': 3.2574, 'lr': 9.787738156341992e-05, 'epoch': 1.98}\n","{'loss': 3.4908, 'grad_norm': 4.174292087554932, 'learning_rate': 9.782004921382612e-05, 'epoch': 2.0}\n"," 10% 80/800 [00:21<03:24,  3.51it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 3.0569, 'lr': 9.782004921382612e-05, 'epoch': 2.0}\n"," 10% 80/800 [00:23<03:24,  3.51it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 3.0623, 'lr': 9.776197011473033e-05, 'epoch': 2.02}\n","{'loss': 3.3947, 'lr': 9.770314517308554e-05, 'epoch': 2.05}\n","{'loss': 2.4808, 'lr': 9.764357530749178e-05, 'epoch': 2.08}\n","{'loss': 2.8597, 'lr': 9.758326144818155e-05, 'epoch': 2.1}\n","{'loss': 3.0847, 'lr': 9.752220453700556e-05, 'epoch': 2.12}\n","{'loss': 3.085, 'lr': 9.74604055274178e-05, 'epoch': 2.15}\n","{'loss': 3.1427, 'lr': 9.739786538446076e-05, 'epoch': 2.17}\n","{'loss': 3.0209, 'grad_norm': 4.438251495361328, 'learning_rate': 9.733458508475037e-05, 'epoch': 2.2}\n","{'loss': 2.5134, 'lr': 9.733458508475037e-05, 'epoch': 2.2}\n","{'loss': 2.2158, 'lr': 9.727056561646066e-05, 'epoch': 2.23}\n","{'loss': 2.8189, 'lr': 9.720580797930845e-05, 'epoch': 2.25}\n","{'loss': 2.2771, 'lr': 9.714031318453764e-05, 'epoch': 2.27}\n","{'loss': 3.0142, 'lr': 9.707408225490344e-05, 'epoch': 2.3}\n","{'loss': 2.8569, 'lr': 9.700711622465643e-05, 'epoch': 2.33}\n","{'loss': 3.157, 'lr': 9.693941613952642e-05, 'epoch': 2.35}\n","{'loss': 2.8171, 'lr': 9.687098305670605e-05, 'epoch': 2.38}\n","{'loss': 2.7088, 'grad_norm': 6.362837791442871, 'learning_rate': 9.680181804483434e-05, 'epoch': 2.4}\n","{'loss': 2.8672, 'lr': 9.680181804483434e-05, 'epoch': 2.4}\n","{'loss': 3.07, 'lr': 9.673192218398e-05, 'epoch': 2.42}\n","{'loss': 3.2367, 'lr': 9.66612965656245e-05, 'epoch': 2.45}\n","{'loss': 3.2566, 'lr': 9.658994229264514e-05, 'epoch': 2.48}\n","{'loss': 2.8738, 'lr': 9.651786047929773e-05, 'epoch': 2.5}\n","{'loss': 2.884, 'lr': 9.644505225119922e-05, 'epoch': 2.52}\n","{'loss': 2.5982, 'lr': 9.637151874531014e-05, 'epoch': 2.55}\n","{'loss': 2.878, 'lr': 9.62972611099168e-05, 'epoch': 2.58}\n","{'loss': 2.9581, 'grad_norm': 5.143988132476807, 'learning_rate': 9.622228050461343e-05, 'epoch': 2.6}\n","{'loss': 3.2197, 'lr': 9.622228050461343e-05, 'epoch': 2.6}\n","{'loss': 2.7147, 'lr': 9.614657810028402e-05, 'epoch': 2.62}\n","{'loss': 2.8942, 'lr': 9.607015507908401e-05, 'epoch': 2.65}\n","{'loss': 2.4278, 'lr': 9.599301263442192e-05, 'epoch': 2.67}\n","{'loss': 3.128, 'lr': 9.591515197094064e-05, 'epoch': 2.7}\n","{'loss': 3.4969, 'lr': 9.583657430449862e-05, 'epoch': 2.73}\n","{'loss': 2.7445, 'lr': 9.575728086215092e-05, 'epoch': 2.75}\n","{'loss': 2.9352, 'lr': 9.567727288213005e-05, 'epoch': 2.77}\n","{'loss': 2.9451, 'grad_norm': 5.11055326461792, 'learning_rate': 9.559655161382657e-05, 'epoch': 2.8}\n","{'loss': 3.0926, 'lr': 9.559655161382657e-05, 'epoch': 2.8}\n","{'loss': 2.7138, 'lr': 9.551511831776965e-05, 'epoch': 2.83}\n","{'loss': 2.7918, 'lr': 9.543297426560739e-05, 'epoch': 2.85}\n","{'loss': 2.9102, 'lr': 9.535012074008687e-05, 'epoch': 2.88}\n","{'loss': 2.6638, 'lr': 9.526655903503423e-05, 'epoch': 2.9}\n","{'loss': 3.1405, 'lr': 9.518229045533438e-05, 'epoch': 2.92}\n","{'loss': 2.4667, 'lr': 9.50973163169107e-05, 'epoch': 2.95}\n","{'loss': 2.6118, 'lr': 9.501163794670444e-05, 'epoch': 2.98}\n","{'loss': 2.7989, 'grad_norm': 5.438312530517578, 'learning_rate': 9.492525668265399e-05, 'epoch': 3.0}\n"," 15% 120/800 [00:32<02:39,  4.25it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 2.1941, 'lr': 9.492525668265399e-05, 'epoch': 3.0}\n"," 15% 120/800 [00:34<02:39,  4.25it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 2.1209, 'lr': 9.483817387367403e-05, 'epoch': 3.02}\n","{'loss': 2.2813, 'lr': 9.475039087963442e-05, 'epoch': 3.05}\n","{'loss': 2.2742, 'lr': 9.4661909071339e-05, 'epoch': 3.08}\n","{'loss': 2.0993, 'lr': 9.45727298305042e-05, 'epoch': 3.1}\n","{'loss': 2.0906, 'lr': 9.448285454973738e-05, 'epoch': 3.12}\n","{'loss': 2.0557, 'lr': 9.439228463251515e-05, 'epoch': 3.15}\n","{'loss': 2.3879, 'lr': 9.430102149316146e-05, 'epoch': 3.17}\n","{'loss': 2.188, 'grad_norm': 6.129668235778809, 'learning_rate': 9.420906655682553e-05, 'epoch': 3.2}\n","{'loss': 2.2211, 'lr': 9.420906655682553e-05, 'epoch': 3.2}\n","{'loss': 2.4301, 'lr': 9.411642125945945e-05, 'epoch': 3.23}\n","{'loss': 2.2087, 'lr': 9.402308704779599e-05, 'epoch': 3.25}\n","{'loss': 2.6231, 'lr': 9.392906537932582e-05, 'epoch': 3.27}\n","{'loss': 2.5683, 'lr': 9.38343577222748e-05, 'epoch': 3.3}\n","{'loss': 2.3781, 'lr': 9.373896555558113e-05, 'epoch': 3.33}\n","{'loss': 1.9731, 'lr': 9.364289036887213e-05, 'epoch': 3.35}\n","{'loss': 1.96, 'lr': 9.354613366244108e-05, 'epoch': 3.38}\n","{'loss': 2.2953, 'grad_norm': 6.054467678070068, 'learning_rate': 9.344869694722372e-05, 'epoch': 3.4}\n","{'loss': 2.3106, 'lr': 9.344869694722372e-05, 'epoch': 3.4}\n","{'loss': 2.3333, 'lr': 9.335058174477471e-05, 'epoch': 3.42}\n","{'loss': 2.2596, 'lr': 9.325178958724386e-05, 'epoch': 3.45}\n","{'loss': 1.9545, 'lr': 9.315232201735217e-05, 'epoch': 3.48}\n","{'loss': 2.3372, 'lr': 9.305218058836778e-05, 'epoch': 3.5}\n","{'loss': 2.5921, 'lr': 9.295136686408166e-05, 'epoch': 3.52}\n","{'loss': 2.1699, 'lr': 9.284988241878326e-05, 'epoch': 3.55}\n","{'loss': 2.0545, 'lr': 9.274772883723587e-05, 'epoch': 3.58}\n","{'loss': 2.2515, 'grad_norm': 6.1219072341918945, 'learning_rate': 9.264490771465191e-05, 'epoch': 3.6}\n","{'loss': 2.3275, 'lr': 9.264490771465191e-05, 'epoch': 3.6}\n","{'loss': 1.967, 'lr': 9.254142065666801e-05, 'epoch': 3.62}\n","{'loss': 2.3771, 'lr': 9.243726927931991e-05, 'epoch': 3.65}\n","{'loss': 2.206, 'lr': 9.233245520901723e-05, 'epoch': 3.67}\n","{'loss': 2.5366, 'lr': 9.222698008251813e-05, 'epoch': 3.7}\n","{'loss': 2.605, 'lr': 9.21208455469037e-05, 'epoch': 3.73}\n","{'loss': 2.218, 'lr': 9.201405325955221e-05, 'epoch': 3.75}\n","{'loss': 2.7097, 'lr': 9.190660488811331e-05, 'epoch': 3.77}\n","{'loss': 2.3684, 'grad_norm': 7.262763023376465, 'learning_rate': 9.179850211048193e-05, 'epoch': 3.8}\n","{'loss': 2.3925, 'lr': 9.179850211048193e-05, 'epoch': 3.8}\n","{'loss': 2.2707, 'lr': 9.168974661477205e-05, 'epoch': 3.83}\n","{'loss': 2.2617, 'lr': 9.158034009929046e-05, 'epoch': 3.85}\n","{'loss': 2.1466, 'lr': 9.14702842725101e-05, 'epoch': 3.88}\n","{'loss': 2.0613, 'lr': 9.135958085304344e-05, 'epoch': 3.9}\n","{'loss': 2.3487, 'lr': 9.12482315696157e-05, 'epoch': 3.92}\n","{'loss': 2.4892, 'lr': 9.113623816103773e-05, 'epoch': 3.95}\n","{'loss': 2.42, 'lr': 9.102360237617899e-05, 'epoch': 3.98}\n","{'loss': 2.2988, 'grad_norm': 6.127704620361328, 'learning_rate': 9.091032597394012e-05, 'epoch': 4.0}\n"," 20% 160/800 [00:44<02:32,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.9747, 'lr': 9.091032597394012e-05, 'epoch': 4.0}\n"," 20% 160/800 [00:45<02:32,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.6959, 'lr': 9.079641072322556e-05, 'epoch': 4.03}\n","{'loss': 1.9598, 'lr': 9.068185840291588e-05, 'epoch': 4.05}\n","{'loss': 2.1543, 'lr': 9.056667080184003e-05, 'epoch': 4.08}\n","{'loss': 1.79, 'lr': 9.045084971874738e-05, 'epoch': 4.1}\n","{'loss': 1.6771, 'lr': 9.033439696227965e-05, 'epoch': 4.12}\n","{'loss': 1.9695, 'lr': 9.021731435094268e-05, 'epoch': 4.15}\n","{'loss': 1.8829, 'lr': 9.009960371307798e-05, 'epoch': 4.17}\n","{'loss': 1.888, 'grad_norm': 7.70947790145874, 'learning_rate': 8.998126688683422e-05, 'epoch': 4.2}\n","{'loss': 2.0313, 'lr': 8.998126688683422e-05, 'epoch': 4.2}\n","{'loss': 1.508, 'lr': 8.986230572013855e-05, 'epoch': 4.22}\n","{'loss': 2.1057, 'lr': 8.974272207066767e-05, 'epoch': 4.25}\n","{'loss': 1.8304, 'lr': 8.962251780581887e-05, 'epoch': 4.28}\n","{'loss': 1.794, 'lr': 8.95016948026809e-05, 'epoch': 4.3}\n","{'loss': 1.7397, 'lr': 8.938025494800454e-05, 'epoch': 4.33}\n","{'loss': 2.1422, 'lr': 8.925820013817329e-05, 'epoch': 4.35}\n","{'loss': 2.0143, 'lr': 8.913553227917367e-05, 'epoch': 4.38}\n","{'loss': 1.8957, 'grad_norm': 6.730741500854492, 'learning_rate': 8.901225328656542e-05, 'epoch': 4.4}\n","{'loss': 1.697, 'lr': 8.901225328656542e-05, 'epoch': 4.4}\n","{'loss': 2.1939, 'lr': 8.88883650854517e-05, 'epoch': 4.42}\n","{'loss': 1.6275, 'lr': 8.876386961044891e-05, 'epoch': 4.45}\n","{'loss': 1.7869, 'lr': 8.863876880565656e-05, 'epoch': 4.47}\n","{'loss': 1.6505, 'lr': 8.851306462462688e-05, 'epoch': 4.5}\n","{'loss': 1.5793, 'lr': 8.83867590303343e-05, 'epoch': 4.53}\n","{'loss': 1.9765, 'lr': 8.825985399514487e-05, 'epoch': 4.55}\n","{'loss': 1.5858, 'lr': 8.813235150078531e-05, 'epoch': 4.58}\n","{'loss': 1.7622, 'grad_norm': 6.497594356536865, 'learning_rate': 8.800425353831226e-05, 'epoch': 4.6}\n","{'loss': 2.0972, 'lr': 8.800425353831226e-05, 'epoch': 4.6}\n","{'loss': 2.0788, 'lr': 8.787556210808101e-05, 'epoch': 4.62}\n","{'loss': 1.9029, 'lr': 8.774627921971436e-05, 'epoch': 4.65}\n","{'loss': 2.2996, 'lr': 8.761640689207123e-05, 'epoch': 4.67}\n","{'loss': 1.6325, 'lr': 8.748594715321512e-05, 'epoch': 4.7}\n","{'loss': 1.8231, 'lr': 8.735490204038243e-05, 'epoch': 4.72}\n","{'loss': 1.865, 'lr': 8.722327359995064e-05, 'epoch': 4.75}\n","{'loss': 1.6198, 'lr': 8.709106388740642e-05, 'epoch': 4.78}\n","{'loss': 1.9148, 'grad_norm': 5.466113567352295, 'learning_rate': 8.695827496731342e-05, 'epoch': 4.8}\n","{'loss': 2.0404, 'lr': 8.695827496731342e-05, 'epoch': 4.8}\n","{'loss': 2.2257, 'lr': 8.682490891328016e-05, 'epoch': 4.83}\n","{'loss': 1.7343, 'lr': 8.669096780792753e-05, 'epoch': 4.85}\n","{'loss': 2.1592, 'lr': 8.655645374285637e-05, 'epoch': 4.88}\n","{'loss': 1.8478, 'lr': 8.64213688186147e-05, 'epoch': 4.9}\n","{'loss': 1.7956, 'lr': 8.628571514466501e-05, 'epoch': 4.92}\n","{'loss': 1.7247, 'lr': 8.61494948393513e-05, 'epoch': 4.95}\n","{'loss': 1.6188, 'lr': 8.601271002986595e-05, 'epoch': 4.97}\n","{'loss': 1.8933, 'grad_norm': 5.839071750640869, 'learning_rate': 8.587536285221656e-05, 'epoch': 5.0}\n"," 25% 200/800 [00:54<02:22,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.4873, 'lr': 8.587536285221656e-05, 'epoch': 5.0}\n"," 25% 200/800 [00:55<02:22,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.6635, 'lr': 8.573745545119257e-05, 'epoch': 5.03}\n","{'loss': 1.5768, 'lr': 8.559898998033178e-05, 'epoch': 5.05}\n","{'loss': 1.498, 'lr': 8.545996860188668e-05, 'epoch': 5.08}\n","{'loss': 1.5061, 'lr': 8.532039348679073e-05, 'epoch': 5.1}\n","{'loss': 1.3465, 'lr': 8.518026681462448e-05, 'epoch': 5.12}\n","{'loss': 1.3071, 'lr': 8.503959077358143e-05, 'epoch': 5.15}\n","{'loss': 1.384, 'lr': 8.4898367560434e-05, 'epoch': 5.17}\n","{'loss': 1.4712, 'grad_norm': 4.617524147033691, 'learning_rate': 8.475659938049912e-05, 'epoch': 5.2}\n","{'loss': 1.3807, 'lr': 8.475659938049912e-05, 'epoch': 5.2}\n","{'loss': 1.6567, 'lr': 8.46142884476038e-05, 'epoch': 5.22}\n","{'loss': 1.591, 'lr': 8.44714369840506e-05, 'epoch': 5.25}\n","{'loss': 2.0321, 'lr': 8.432804722058296e-05, 'epoch': 5.28}\n","{'loss': 1.5058, 'lr': 8.418412139635025e-05, 'epoch': 5.3}\n","{'loss': 1.371, 'lr': 8.403966175887292e-05, 'epoch': 5.33}\n","{'loss': 1.6442, 'lr': 8.389467056400732e-05, 'epoch': 5.35}\n","{'loss': 1.6706, 'lr': 8.374915007591053e-05, 'epoch': 5.38}\n","{'loss': 1.6065, 'grad_norm': 9.349867820739746, 'learning_rate': 8.360310256700497e-05, 'epoch': 5.4}\n","{'loss': 1.8485, 'lr': 8.360310256700497e-05, 'epoch': 5.4}\n","{'loss': 1.8881, 'lr': 8.345653031794292e-05, 'epoch': 5.42}\n","{'loss': 1.7754, 'lr': 8.330943561757092e-05, 'epoch': 5.45}\n","{'loss': 1.5958, 'lr': 8.316182076289401e-05, 'epoch': 5.47}\n","{'loss': 1.4897, 'lr': 8.301368805903988e-05, 'epoch': 5.5}\n","{'loss': 1.4513, 'lr': 8.286503981922283e-05, 'epoch': 5.53}\n","{'loss': 1.4311, 'lr': 8.271587836470775e-05, 'epoch': 5.55}\n","{'loss': 1.4822, 'lr': 8.256620602477372e-05, 'epoch': 5.58}\n","{'loss': 1.6202, 'grad_norm': 5.829284191131592, 'learning_rate': 8.241602513667774e-05, 'epoch': 5.6}\n","{'loss': 1.303, 'lr': 8.241602513667774e-05, 'epoch': 5.6}\n","{'loss': 2.0272, 'lr': 8.226533804561827e-05, 'epoch': 5.62}\n","{'loss': 1.4037, 'lr': 8.211414710469845e-05, 'epoch': 5.65}\n","{'loss': 1.7819, 'lr': 8.19624546748895e-05, 'epoch': 5.67}\n","{'loss': 1.6209, 'lr': 8.181026312499383e-05, 'epoch': 5.7}\n","{'loss': 1.5701, 'lr': 8.165757483160798e-05, 'epoch': 5.72}\n","{'loss': 1.6965, 'lr': 8.150439217908556e-05, 'epoch': 5.75}\n","{'loss': 1.7351, 'lr': 8.13507175595e-05, 'epoch': 5.78}\n","{'loss': 1.6423, 'grad_norm': 11.326977729797363, 'learning_rate': 8.11965533726072e-05, 'epoch': 5.8}\n","{'loss': 1.9813, 'lr': 8.11965533726072e-05, 'epoch': 5.8}\n","{'loss': 1.6102, 'lr': 8.104190202580812e-05, 'epoch': 5.83}\n","{'loss': 1.4513, 'lr': 8.0886765934111e-05, 'epoch': 5.85}\n","{'loss': 1.8725, 'lr': 8.073114752009387e-05, 'epoch': 5.88}\n","{'loss': 1.6405, 'lr': 8.05750492138666e-05, 'epoch': 5.9}\n","{'loss': 1.4994, 'lr': 8.041847345303297e-05, 'epoch': 5.92}\n","{'loss': 1.3284, 'lr': 8.026142268265256e-05, 'epoch': 5.95}\n","{'loss': 1.4816, 'lr': 8.01038993552027e-05, 'epoch': 5.97}\n","{'loss': 1.6082, 'grad_norm': 6.561407566070557, 'learning_rate': 7.994590593054001e-05, 'epoch': 6.0}\n"," 30% 240/800 [01:05<02:12,  4.23it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.281, 'lr': 7.994590593054001e-05, 'epoch': 6.0}\n"," 30% 240/800 [01:06<02:12,  4.23it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.2573, 'lr': 7.978744487586214e-05, 'epoch': 6.03}\n","{'loss': 1.4499, 'lr': 7.962851866566912e-05, 'epoch': 6.05}\n","{'loss': 1.4433, 'lr': 7.946912978172474e-05, 'epoch': 6.08}\n","{'loss': 1.314, 'lr': 7.93092807130179e-05, 'epoch': 6.1}\n","{'loss': 1.213, 'lr': 7.91489739557236e-05, 'epoch': 6.12}\n","{'loss': 1.3775, 'lr': 7.898821201316407e-05, 'epoch': 6.15}\n","{'loss': 1.2874, 'lr': 7.882699739576959e-05, 'epoch': 6.17}\n","{'loss': 1.3279, 'grad_norm': 4.370453834533691, 'learning_rate': 7.866533262103936e-05, 'epoch': 6.2}\n","{'loss': 1.1679, 'lr': 7.866533262103936e-05, 'epoch': 6.2}\n","{'loss': 1.5222, 'lr': 7.850322021350215e-05, 'epoch': 6.22}\n","{'loss': 1.1604, 'lr': 7.83406627046769e-05, 'epoch': 6.25}\n","{'loss': 1.2876, 'lr': 7.817766263303313e-05, 'epoch': 6.28}\n","{'loss': 1.3179, 'lr': 7.801422254395138e-05, 'epoch': 6.3}\n","{'loss': 1.2715, 'lr': 7.785034498968344e-05, 'epoch': 6.33}\n","{'loss': 1.364, 'lr': 7.768603252931243e-05, 'epoch': 6.35}\n","{'loss': 1.2009, 'lr': 7.752128772871292e-05, 'epoch': 6.38}\n","{'loss': 1.2866, 'grad_norm': 5.830416202545166, 'learning_rate': 7.735611316051084e-05, 'epoch': 6.4}\n","{'loss': 1.3255, 'lr': 7.735611316051084e-05, 'epoch': 6.4}\n","{'loss': 1.3763, 'lr': 7.719051140404327e-05, 'epoch': 6.42}\n","{'loss': 1.5708, 'lr': 7.702448504531819e-05, 'epoch': 6.45}\n","{'loss': 1.5099, 'lr': 7.685803667697411e-05, 'epoch': 6.47}\n","{'loss': 1.2513, 'lr': 7.669116889823955e-05, 'epoch': 6.5}\n","{'loss': 1.213, 'lr': 7.652388431489248e-05, 'epoch': 6.53}\n","{'loss': 1.4641, 'lr': 7.635618553921962e-05, 'epoch': 6.55}\n","{'loss': 1.2497, 'lr': 7.618807518997563e-05, 'epoch': 6.58}\n","{'loss': 1.3701, 'grad_norm': 4.712114334106445, 'learning_rate': 7.601955589234227e-05, 'epoch': 6.6}\n","{'loss': 1.4161, 'lr': 7.601955589234227e-05, 'epoch': 6.6}\n","{'loss': 1.2682, 'lr': 7.585063027788731e-05, 'epoch': 6.62}\n","{'loss': 1.2246, 'lr': 7.568130098452351e-05, 'epoch': 6.65}\n","{'loss': 1.399, 'lr': 7.551157065646746e-05, 'epoch': 6.67}\n","{'loss': 1.4256, 'lr': 7.534144194419817e-05, 'epoch': 6.7}\n","{'loss': 1.2796, 'lr': 7.517091750441576e-05, 'epoch': 6.72}\n","{'loss': 1.2895, 'lr': 7.500000000000001e-05, 'epoch': 6.75}\n","{'loss': 1.4471, 'lr': 7.482869209996867e-05, 'epoch': 6.78}\n","{'loss': 1.3437, 'grad_norm': 7.41386604309082, 'learning_rate': 7.465699647943586e-05, 'epoch': 6.8}\n","{'loss': 1.3182, 'lr': 7.465699647943586e-05, 'epoch': 6.8}\n","{'loss': 1.448, 'lr': 7.44849158195703e-05, 'epoch': 6.83}\n","{'loss': 1.2657, 'lr': 7.431245280755336e-05, 'epoch': 6.85}\n","{'loss': 1.3882, 'lr': 7.413961013653726e-05, 'epoch': 6.88}\n","{'loss': 1.2134, 'lr': 7.396639050560275e-05, 'epoch': 6.9}\n","{'loss': 1.374, 'lr': 7.379279661971727e-05, 'epoch': 6.92}\n","{'loss': 1.4906, 'lr': 7.361883118969247e-05, 'epoch': 6.95}\n","{'loss': 1.4078, 'lr': 7.3444496932142e-05, 'epoch': 6.97}\n","{'loss': 1.3632, 'grad_norm': 6.843539714813232, 'learning_rate': 7.326979656943906e-05, 'epoch': 7.0}\n"," 35% 280/800 [01:16<02:08,  4.03it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.2472, 'lr': 7.326979656943906e-05, 'epoch': 7.0}\n"," 35% 280/800 [01:17<02:08,  4.03it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.2089, 'lr': 7.309473282967387e-05, 'epoch': 7.03}\n","{'loss': 1.1847, 'lr': 7.291930844661109e-05, 'epoch': 7.05}\n","{'loss': 1.292, 'lr': 7.274352615964712e-05, 'epoch': 7.08}\n","{'loss': 1.1342, 'lr': 7.256738871376732e-05, 'epoch': 7.1}\n","{'loss': 1.286, 'lr': 7.239089885950316e-05, 'epoch': 7.12}\n","{'loss': 1.1684, 'lr': 7.221405935288925e-05, 'epoch': 7.15}\n","{'loss': 1.1006, 'lr': 7.203687295542032e-05, 'epoch': 7.17}\n","{'loss': 1.2027, 'grad_norm': 3.4356095790863037, 'learning_rate': 7.185934243400806e-05, 'epoch': 7.2}\n","{'loss': 1.2708, 'lr': 7.185934243400806e-05, 'epoch': 7.2}\n","{'loss': 1.1845, 'lr': 7.168147056093797e-05, 'epoch': 7.22}\n","{'loss': 1.2554, 'lr': 7.150326011382604e-05, 'epoch': 7.25}\n","{'loss': 1.2638, 'lr': 7.132471387557532e-05, 'epoch': 7.28}\n","{'loss': 1.1856, 'lr': 7.114583463433259e-05, 'epoch': 7.3}\n","{'loss': 1.344, 'lr': 7.096662518344468e-05, 'epoch': 7.33}\n","{'loss': 1.0623, 'lr': 7.078708832141497e-05, 'epoch': 7.35}\n","{'loss': 1.1769, 'lr': 7.060722685185961e-05, 'epoch': 7.38}\n","{'loss': 1.2179, 'grad_norm': 6.562963962554932, 'learning_rate': 7.042704358346375e-05, 'epoch': 7.4}\n","{'loss': 1.2474, 'lr': 7.042704358346375e-05, 'epoch': 7.4}\n","{'loss': 1.1589, 'lr': 7.024654132993772e-05, 'epoch': 7.42}\n","{'loss': 1.3308, 'lr': 7.006572290997304e-05, 'epoch': 7.45}\n","{'loss': 1.6291, 'lr': 6.988459114719849e-05, 'epoch': 7.47}\n","{'loss': 1.3852, 'lr': 6.970314887013584e-05, 'epoch': 7.5}\n","{'loss': 1.2235, 'lr': 6.952139891215593e-05, 'epoch': 7.53}\n","{'loss': 1.3066, 'lr': 6.93393441114342e-05, 'epoch': 7.55}\n","{'loss': 1.3342, 'lr': 6.915698731090648e-05, 'epoch': 7.58}\n","{'loss': 1.327, 'grad_norm': 9.731990814208984, 'learning_rate': 6.897433135822461e-05, 'epoch': 7.6}\n","{'loss': 1.2351, 'lr': 6.897433135822461e-05, 'epoch': 7.6}\n","{'loss': 1.187, 'lr': 6.879137910571191e-05, 'epoch': 7.62}\n","{'loss': 1.1435, 'lr': 6.860813341031866e-05, 'epoch': 7.65}\n","{'loss': 1.2828, 'lr': 6.842459713357752e-05, 'epoch': 7.67}\n","{'loss': 1.1994, 'lr': 6.824077314155877e-05, 'epoch': 7.7}\n","{'loss': 1.1687, 'lr': 6.805666430482564e-05, 'epoch': 7.72}\n","{'loss': 1.338, 'lr': 6.787227349838947e-05, 'epoch': 7.75}\n","{'loss': 1.141, 'lr': 6.768760360166471e-05, 'epoch': 7.78}\n","{'loss': 1.2119, 'grad_norm': 4.842807769775391, 'learning_rate': 6.750265749842409e-05, 'epoch': 7.8}\n","{'loss': 1.2627, 'lr': 6.750265749842409e-05, 'epoch': 7.8}\n","{'loss': 1.0891, 'lr': 6.731743807675355e-05, 'epoch': 7.83}\n","{'loss': 1.2967, 'lr': 6.713194822900706e-05, 'epoch': 7.85}\n","{'loss': 1.2263, 'lr': 6.694619085176159e-05, 'epoch': 7.88}\n","{'loss': 1.7479, 'lr': 6.676016884577173e-05, 'epoch': 7.9}\n","{'loss': 1.2563, 'lr': 6.657388511592452e-05, 'epoch': 7.92}\n","{'loss': 1.2686, 'lr': 6.638734257119401e-05, 'epoch': 7.95}\n","{'loss': 1.1691, 'lr': 6.620054412459588e-05, 'epoch': 7.97}\n","{'loss': 1.2896, 'grad_norm': 7.579926490783691, 'learning_rate': 6.601349269314188e-05, 'epoch': 8.0}\n"," 40% 320/800 [01:27<02:08,  3.74it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.1508, 'lr': 6.601349269314188e-05, 'epoch': 8.0}\n"," 40% 320/800 [01:28<02:08,  3.74it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.1839, 'lr': 6.582619119779439e-05, 'epoch': 8.03}\n","{'loss': 1.058, 'lr': 6.56386425634207e-05, 'epoch': 8.05}\n","{'loss': 1.2536, 'lr': 6.545084971874738e-05, 'epoch': 8.07}\n","{'loss': 1.0473, 'lr': 6.526281559631458e-05, 'epoch': 8.1}\n","{'loss': 1.0258, 'lr': 6.507454313243015e-05, 'epoch': 8.12}\n","{'loss': 1.0381, 'lr': 6.48860352671239e-05, 'epoch': 8.15}\n","{'loss': 1.1478, 'lr': 6.469729494410158e-05, 'epoch': 8.18}\n","{'loss': 1.1132, 'grad_norm': 3.4523699283599854, 'learning_rate': 6.450832511069897e-05, 'epoch': 8.2}\n","{'loss': 1.0679, 'lr': 6.450832511069897e-05, 'epoch': 8.2}\n","{'loss': 1.1743, 'lr': 6.431912871783586e-05, 'epoch': 8.22}\n","{'loss': 1.1389, 'lr': 6.412970871996995e-05, 'epoch': 8.25}\n","{'loss': 1.2004, 'lr': 6.394006807505067e-05, 'epoch': 8.28}\n","{'loss': 1.0829, 'lr': 6.37502097444731e-05, 'epoch': 8.3}\n","{'loss': 1.0205, 'lr': 6.356013669303162e-05, 'epoch': 8.32}\n","{'loss': 1.0481, 'lr': 6.336985188887366e-05, 'epoch': 8.35}\n","{'loss': 1.2438, 'lr': 6.317935830345338e-05, 'epoch': 8.38}\n","{'loss': 1.1221, 'grad_norm': 10.28886604309082, 'learning_rate': 6.298865891148518e-05, 'epoch': 8.4}\n","{'loss': 1.3303, 'lr': 6.298865891148518e-05, 'epoch': 8.4}\n","{'loss': 1.242, 'lr': 6.279775669089733e-05, 'epoch': 8.43}\n","{'loss': 1.1621, 'lr': 6.260665462278544e-05, 'epoch': 8.45}\n","{'loss': 1.0974, 'lr': 6.241535569136584e-05, 'epoch': 8.47}\n","{'loss': 1.1904, 'lr': 6.222386288392913e-05, 'epoch': 8.5}\n","{'loss': 1.1226, 'lr': 6.203217919079342e-05, 'epoch': 8.53}\n","{'loss': 1.0881, 'lr': 6.184030760525764e-05, 'epoch': 8.55}\n","{'loss': 1.0894, 'lr': 6.164825112355477e-05, 'epoch': 8.57}\n","{'loss': 1.1653, 'grad_norm': 3.2385177612304688, 'learning_rate': 6.145601274480521e-05, 'epoch': 8.6}\n","{'loss': 1.1307, 'lr': 6.145601274480521e-05, 'epoch': 8.6}\n","{'loss': 1.3245, 'lr': 6.126359547096975e-05, 'epoch': 8.62}\n","{'loss': 1.1186, 'lr': 6.107100230680279e-05, 'epoch': 8.65}\n","{'loss': 1.0394, 'lr': 6.0878236259805396e-05, 'epoch': 8.68}\n","{'loss': 1.0408, 'lr': 6.068530034017835e-05, 'epoch': 8.7}\n","{'loss': 1.0861, 'lr': 6.049219756077514e-05, 'epoch': 8.72}\n","{'loss': 1.2524, 'lr': 6.029893093705492e-05, 'epoch': 8.75}\n","{'loss': 1.0427, 'lr': 6.010550348703538e-05, 'epoch': 8.78}\n","{'loss': 1.1294, 'grad_norm': 2.3146097660064697, 'learning_rate': 5.991191823124565e-05, 'epoch': 8.8}\n","{'loss': 1.1835, 'lr': 5.991191823124565e-05, 'epoch': 8.8}\n","{'loss': 1.09, 'lr': 5.971817819267913e-05, 'epoch': 8.82}\n","{'loss': 1.1377, 'lr': 5.952428639674632e-05, 'epoch': 8.85}\n","{'loss': 1.1585, 'lr': 5.9330245871227454e-05, 'epoch': 8.88}\n","{'loss': 1.1417, 'lr': 5.9136059646225375e-05, 'epoch': 8.9}\n","{'loss': 1.1182, 'lr': 5.8941730754118116e-05, 'epoch': 8.93}\n","{'loss': 1.2388, 'lr': 5.874726222951157e-05, 'epoch': 8.95}\n","{'loss': 1.1847, 'lr': 5.855265710919211e-05, 'epoch': 8.97}\n","{'loss': 1.1566, 'grad_norm': 3.6987297534942627, 'learning_rate': 5.835791843207916e-05, 'epoch': 9.0}\n"," 45% 360/800 [01:38<01:54,  3.83it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.1102, 'lr': 5.835791843207916e-05, 'epoch': 9.0}\n"," 45% 360/800 [01:39<01:54,  3.83it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.0358, 'lr': 5.8163049239177784e-05, 'epoch': 9.03}\n","{'loss': 1.0556, 'lr': 5.7968052573531084e-05, 'epoch': 9.05}\n","{'loss': 0.9508, 'lr': 5.7772931480172795e-05, 'epoch': 9.07}\n","{'loss': 0.9559, 'lr': 5.757768900607972e-05, 'epoch': 9.1}\n","{'loss': 1.1151, 'lr': 5.738232820012407e-05, 'epoch': 9.12}\n","{'loss': 0.9905, 'lr': 5.718685211302592e-05, 'epoch': 9.15}\n","{'loss': 1.1618, 'lr': 5.699126379730559e-05, 'epoch': 9.18}\n","{'loss': 1.047, 'grad_norm': 2.5391788482666016, 'learning_rate': 5.6795566307235915e-05, 'epoch': 9.2}\n","{'loss': 1.0607, 'lr': 5.6795566307235915e-05, 'epoch': 9.2}\n","{'loss': 1.0576, 'lr': 5.6599762698794554e-05, 'epoch': 9.22}\n","{'loss': 1.1283, 'lr': 5.640385602961634e-05, 'epoch': 9.25}\n","{'loss': 1.1522, 'lr': 5.620784935894547e-05, 'epoch': 9.28}\n","{'loss': 0.9819, 'lr': 5.601174574758771e-05, 'epoch': 9.3}\n","{'loss': 1.117, 'lr': 5.581554825786267e-05, 'epoch': 9.32}\n","{'loss': 1.1766, 'lr': 5.5619259953555945e-05, 'epoch': 9.35}\n","{'loss': 1.2037, 'lr': 5.5422883899871284e-05, 'epoch': 9.38}\n","{'loss': 1.1097, 'grad_norm': 4.083304405212402, 'learning_rate': 5.522642316338268e-05, 'epoch': 9.4}\n","{'loss': 1.2571, 'lr': 5.522642316338268e-05, 'epoch': 9.4}\n","{'loss': 1.0527, 'lr': 5.5029880811986544e-05, 'epoch': 9.43}\n","{'loss': 1.1202, 'lr': 5.483325991485379e-05, 'epoch': 9.45}\n","{'loss': 1.1094, 'lr': 5.463656354238184e-05, 'epoch': 9.47}\n","{'loss': 0.9535, 'lr': 5.4439794766146746e-05, 'epoch': 9.5}\n","{'loss': 1.0578, 'lr': 5.424295665885523e-05, 'epoch': 9.53}\n","{'loss': 1.1351, 'lr': 5.404605229429664e-05, 'epoch': 9.55}\n","{'loss': 0.9857, 'lr': 5.384908474729501e-05, 'epoch': 9.57}\n","{'loss': 1.0839, 'grad_norm': 2.2124016284942627, 'learning_rate': 5.365205709366099e-05, 'epoch': 9.6}\n","{'loss': 1.1486, 'lr': 5.365205709366099e-05, 'epoch': 9.6}\n","{'loss': 1.0954, 'lr': 5.34549724101439e-05, 'epoch': 9.62}\n","{'loss': 1.0422, 'lr': 5.325783377438357e-05, 'epoch': 9.65}\n","{'loss': 1.1297, 'lr': 5.306064426486237e-05, 'epoch': 9.68}\n","{'loss': 1.0614, 'lr': 5.286340696085709e-05, 'epoch': 9.7}\n","{'loss': 1.0998, 'lr': 5.266612494239088e-05, 'epoch': 9.72}\n","{'loss': 0.9673, 'lr': 5.246880129018516e-05, 'epoch': 9.75}\n","{'loss': 1.1174, 'lr': 5.227143908561145e-05, 'epoch': 9.78}\n","{'loss': 1.0827, 'grad_norm': 3.8493404388427734, 'learning_rate': 5.207404141064334e-05, 'epoch': 9.8}\n","{'loss': 1.1261, 'lr': 5.207404141064334e-05, 'epoch': 9.8}\n","{'loss': 1.2411, 'lr': 5.187661134780829e-05, 'epoch': 9.82}\n","{'loss': 1.5153, 'lr': 5.1679151980139564e-05, 'epoch': 9.85}\n","{'loss': 1.0116, 'lr': 5.148166639112799e-05, 'epoch': 9.88}\n","{'loss': 1.1006, 'lr': 5.128415766467392e-05, 'epoch': 9.9}\n","{'loss': 1.149, 'lr': 5.1086628885038946e-05, 'epoch': 9.93}\n","{'loss': 1.0673, 'lr': 5.0889083136797875e-05, 'epoch': 9.95}\n","{'loss': 1.0588, 'lr': 5.0691523504790474e-05, 'epoch': 9.97}\n","{'loss': 1.1587, 'grad_norm': 6.030564785003662, 'learning_rate': 5.049395307407329e-05, 'epoch': 10.0}\n"," 50% 400/800 [01:48<01:36,  4.16it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.0607, 'lr': 5.049395307407329e-05, 'epoch': 10.0}\n"," 50% 400/800 [01:50<01:36,  4.16it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.9841, 'lr': 5.029637492987153e-05, 'epoch': 10.03}\n","{'loss': 1.0136, 'lr': 5.009879215753085e-05, 'epoch': 10.05}\n","{'loss': 1.0777, 'lr': 4.990120784246917e-05, 'epoch': 10.07}\n","{'loss': 1.0194, 'lr': 4.970362507012848e-05, 'epoch': 10.1}\n","{'loss': 1.0443, 'lr': 4.950604692592672e-05, 'epoch': 10.12}\n","{'loss': 0.9872, 'lr': 4.9308476495209544e-05, 'epoch': 10.15}\n","{'loss': 1.0274, 'lr': 4.911091686320213e-05, 'epoch': 10.18}\n","{'loss': 1.0268, 'grad_norm': 3.9215898513793945, 'learning_rate': 4.891337111496107e-05, 'epoch': 10.2}\n","{'loss': 0.9562, 'lr': 4.891337111496107e-05, 'epoch': 10.2}\n","{'loss': 1.0696, 'lr': 4.87158423353261e-05, 'epoch': 10.22}\n","{'loss': 1.1451, 'lr': 4.851833360887201e-05, 'epoch': 10.25}\n","{'loss': 0.98, 'lr': 4.8320848019860454e-05, 'epoch': 10.28}\n","{'loss': 1.0513, 'lr': 4.8123388652191715e-05, 'epoch': 10.3}\n","{'loss': 1.047, 'lr': 4.7925958589356675e-05, 'epoch': 10.32}\n","{'loss': 1.0178, 'lr': 4.7728560914388566e-05, 'epoch': 10.35}\n","{'loss': 1.2464, 'lr': 4.7531198709814854e-05, 'epoch': 10.38}\n","{'loss': 1.0642, 'grad_norm': 8.692892074584961, 'learning_rate': 4.7333875057609126e-05, 'epoch': 10.4}\n","{'loss': 1.0725, 'lr': 4.7333875057609126e-05, 'epoch': 10.4}\n","{'loss': 0.9539, 'lr': 4.713659303914292e-05, 'epoch': 10.43}\n","{'loss': 1.0235, 'lr': 4.693935573513764e-05, 'epoch': 10.45}\n","{'loss': 1.0308, 'lr': 4.674216622561644e-05, 'epoch': 10.47}\n","{'loss': 1.061, 'lr': 4.654502758985611e-05, 'epoch': 10.5}\n","{'loss': 1.0317, 'lr': 4.6347942906339015e-05, 'epoch': 10.53}\n","{'loss': 1.0099, 'lr': 4.615091525270501e-05, 'epoch': 10.55}\n","{'loss': 1.0249, 'lr': 4.595394770570337e-05, 'epoch': 10.57}\n","{'loss': 1.026, 'grad_norm': 2.866229295730591, 'learning_rate': 4.575704334114478e-05, 'epoch': 10.6}\n","{'loss': 1.0838, 'lr': 4.575704334114478e-05, 'epoch': 10.6}\n","{'loss': 1.0007, 'lr': 4.5560205233853266e-05, 'epoch': 10.62}\n","{'loss': 1.1639, 'lr': 4.5363436457618174e-05, 'epoch': 10.65}\n","{'loss': 1.1582, 'lr': 4.516674008514623e-05, 'epoch': 10.68}\n","{'loss': 1.0854, 'lr': 4.497011918801347e-05, 'epoch': 10.7}\n","{'loss': 1.044, 'lr': 4.477357683661734e-05, 'epoch': 10.72}\n","{'loss': 1.153, 'lr': 4.4577116100128735e-05, 'epoch': 10.75}\n","{'loss': 0.9346, 'lr': 4.4380740046444066e-05, 'epoch': 10.78}\n","{'loss': 1.0779, 'grad_norm': 1.3502070903778076, 'learning_rate': 4.418445174213734e-05, 'epoch': 10.8}\n","{'loss': 1.1022, 'lr': 4.418445174213734e-05, 'epoch': 10.8}\n","{'loss': 1.0673, 'lr': 4.39882542524123e-05, 'epoch': 10.82}\n","{'loss': 1.3466, 'lr': 4.379215064105454e-05, 'epoch': 10.85}\n","{'loss': 1.0296, 'lr': 4.3596143970383664e-05, 'epoch': 10.88}\n","{'loss': 1.0486, 'lr': 4.340023730120545e-05, 'epoch': 10.9}\n","{'loss': 1.0572, 'lr': 4.3204433692764096e-05, 'epoch': 10.93}\n","{'loss': 1.0024, 'lr': 4.3008736202694414e-05, 'epoch': 10.95}\n","{'loss': 1.1524, 'lr': 4.281314788697408e-05, 'epoch': 10.97}\n","{'loss': 1.1008, 'grad_norm': 7.973405361175537, 'learning_rate': 4.2617671799875944e-05, 'epoch': 11.0}\n"," 55% 440/800 [01:59<01:24,  4.28it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.0283, 'lr': 4.2617671799875944e-05, 'epoch': 11.0}\n"," 55% 440/800 [02:00<01:24,  4.28it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.018, 'lr': 4.242231099392029e-05, 'epoch': 11.03}\n","{'loss': 1.0737, 'lr': 4.222706851982721e-05, 'epoch': 11.05}\n","{'loss': 1.0232, 'lr': 4.203194742646893e-05, 'epoch': 11.07}\n","{'loss': 1.0263, 'lr': 4.1836950760822235e-05, 'epoch': 11.1}\n","{'loss': 0.9542, 'lr': 4.1642081567920846e-05, 'epoch': 11.12}\n","{'loss': 0.9911, 'lr': 4.144734289080791e-05, 'epoch': 11.15}\n","{'loss': 0.9886, 'lr': 4.125273777048845e-05, 'epoch': 11.18}\n","{'loss': 1.0129, 'grad_norm': 1.7194576263427734, 'learning_rate': 4.1058269245881896e-05, 'epoch': 11.2}\n","{'loss': 1.1377, 'lr': 4.1058269245881896e-05, 'epoch': 11.2}\n","{'loss': 1.3076, 'lr': 4.086394035377463e-05, 'epoch': 11.22}\n","{'loss': 0.971, 'lr': 4.066975412877255e-05, 'epoch': 11.25}\n","{'loss': 1.0148, 'lr': 4.04757136032537e-05, 'epoch': 11.28}\n","{'loss': 1.0231, 'lr': 4.028182180732088e-05, 'epoch': 11.3}\n","{'loss': 0.9744, 'lr': 4.0088081768754365e-05, 'epoch': 11.32}\n","{'loss': 1.0189, 'lr': 3.9894496512964635e-05, 'epoch': 11.35}\n","{'loss': 1.0108, 'lr': 3.970106906294509e-05, 'epoch': 11.38}\n","{'loss': 1.0573, 'grad_norm': 2.250920057296753, 'learning_rate': 3.950780243922486e-05, 'epoch': 11.4}\n","{'loss': 1.0526, 'lr': 3.950780243922486e-05, 'epoch': 11.4}\n","{'loss': 1.0951, 'lr': 3.9314699659821666e-05, 'epoch': 11.43}\n","{'loss': 1.1246, 'lr': 3.9121763740194616e-05, 'epoch': 11.45}\n","{'loss': 0.9915, 'lr': 3.892899769319722e-05, 'epoch': 11.47}\n","{'loss': 1.0643, 'lr': 3.873640452903026e-05, 'epoch': 11.5}\n","{'loss': 0.9701, 'lr': 3.854398725519479e-05, 'epoch': 11.53}\n","{'loss': 1.0275, 'lr': 3.835174887644523e-05, 'epoch': 11.55}\n","{'loss': 1.0427, 'lr': 3.815969239474238e-05, 'epoch': 11.57}\n","{'loss': 1.046, 'grad_norm': 2.8433170318603516, 'learning_rate': 3.796782080920659e-05, 'epoch': 11.6}\n","{'loss': 1.0553, 'lr': 3.796782080920659e-05, 'epoch': 11.6}\n","{'loss': 0.9841, 'lr': 3.777613711607087e-05, 'epoch': 11.62}\n","{'loss': 1.0012, 'lr': 3.758464430863417e-05, 'epoch': 11.65}\n","{'loss': 1.0617, 'lr': 3.7393345377214586e-05, 'epoch': 11.68}\n","{'loss': 0.8984, 'lr': 3.720224330910268e-05, 'epoch': 11.7}\n","{'loss': 1.0377, 'lr': 3.701134108851483e-05, 'epoch': 11.72}\n","{'loss': 0.9609, 'lr': 3.682064169654663e-05, 'epoch': 11.75}\n","{'loss': 1.0776, 'lr': 3.663014811112634e-05, 'epoch': 11.78}\n","{'loss': 1.0096, 'grad_norm': 2.671644687652588, 'learning_rate': 3.6439863306968395e-05, 'epoch': 11.8}\n","{'loss': 1.0051, 'lr': 3.6439863306968395e-05, 'epoch': 11.8}\n","{'loss': 0.9849, 'lr': 3.6249790255526915e-05, 'epoch': 11.82}\n","{'loss': 1.0412, 'lr': 3.605993192494934e-05, 'epoch': 11.85}\n","{'loss': 1.0271, 'lr': 3.587029128003006e-05, 'epoch': 11.88}\n","{'loss': 1.1683, 'lr': 3.5680871282164144e-05, 'epoch': 11.9}\n","{'loss': 1.3231, 'lr': 3.549167488930103e-05, 'epoch': 11.93}\n","{'loss': 1.177, 'lr': 3.5302705055898425e-05, 'epoch': 11.95}\n","{'loss': 1.1032, 'lr': 3.5113964732876106e-05, 'epoch': 11.97}\n","{'loss': 1.1037, 'grad_norm': 2.473602056503296, 'learning_rate': 3.492545686756986e-05, 'epoch': 12.0}\n"," 60% 480/800 [02:10<01:14,  4.30it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.0544, 'lr': 3.492545686756986e-05, 'epoch': 12.0}\n"," 60% 480/800 [02:11<01:14,  4.30it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.9687, 'lr': 3.473718440368544e-05, 'epoch': 12.03}\n","{'loss': 1.083, 'lr': 3.4549150281252636e-05, 'epoch': 12.05}\n","{'loss': 1.0584, 'lr': 3.4361357436579316e-05, 'epoch': 12.07}\n","{'loss': 0.9383, 'lr': 3.417380880220563e-05, 'epoch': 12.1}\n","{'loss': 1.1037, 'lr': 3.3986507306858125e-05, 'epoch': 12.12}\n","{'loss': 1.0809, 'lr': 3.379945587540414e-05, 'epoch': 12.15}\n","{'loss': 0.9604, 'lr': 3.361265742880599e-05, 'epoch': 12.18}\n","{'loss': 1.031, 'grad_norm': 1.6549638509750366, 'learning_rate': 3.342611488407549e-05, 'epoch': 12.2}\n","{'loss': 0.9324, 'lr': 3.342611488407549e-05, 'epoch': 12.2}\n","{'loss': 1.0067, 'lr': 3.323983115422827e-05, 'epoch': 12.22}\n","{'loss': 0.9007, 'lr': 3.3053809148238426e-05, 'epoch': 12.25}\n","{'loss': 1.1832, 'lr': 3.286805177099293e-05, 'epoch': 12.28}\n","{'loss': 1.2831, 'lr': 3.268256192324647e-05, 'epoch': 12.3}\n","{'loss': 0.9819, 'lr': 3.249734250157592e-05, 'epoch': 12.32}\n","{'loss': 0.9843, 'lr': 3.231239639833531e-05, 'epoch': 12.35}\n","{'loss': 0.9649, 'lr': 3.212772650161056e-05, 'epoch': 12.38}\n","{'loss': 1.0296, 'grad_norm': 2.711782217025757, 'learning_rate': 3.1943335695174365e-05, 'epoch': 12.4}\n","{'loss': 1.0163, 'lr': 3.1943335695174365e-05, 'epoch': 12.4}\n","{'loss': 0.9796, 'lr': 3.175922685844125e-05, 'epoch': 12.43}\n","{'loss': 0.988, 'lr': 3.15754028664225e-05, 'epoch': 12.45}\n","{'loss': 1.0226, 'lr': 3.1391866589681346e-05, 'epoch': 12.47}\n","{'loss': 1.0164, 'lr': 3.12086208942881e-05, 'epoch': 12.5}\n","{'loss': 1.1187, 'lr': 3.10256686417754e-05, 'epoch': 12.53}\n","{'loss': 1.0166, 'lr': 3.084301268909353e-05, 'epoch': 12.55}\n","{'loss': 1.0564, 'lr': 3.0660655888565825e-05, 'epoch': 12.57}\n","{'loss': 1.0268, 'grad_norm': 2.9006645679473877, 'learning_rate': 3.0478601087844096e-05, 'epoch': 12.6}\n","{'loss': 1.0472, 'lr': 3.0478601087844096e-05, 'epoch': 12.6}\n","{'loss': 1.0889, 'lr': 3.0296851129864168e-05, 'epoch': 12.62}\n","{'loss': 1.0246, 'lr': 3.0115408852801535e-05, 'epoch': 12.65}\n","{'loss': 0.906, 'lr': 2.9934277090026964e-05, 'epoch': 12.68}\n","{'loss': 0.9675, 'lr': 2.97534586700623e-05, 'epoch': 12.7}\n","{'loss': 1.0873, 'lr': 2.9572956416536267e-05, 'epoch': 12.72}\n","{'loss': 0.9911, 'lr': 2.9392773148140408e-05, 'epoch': 12.75}\n","{'loss': 1.0717, 'lr': 2.9212911678585043e-05, 'epoch': 12.78}\n","{'loss': 1.023, 'grad_norm': 2.897822380065918, 'learning_rate': 2.9033374816555338e-05, 'epoch': 12.8}\n","{'loss': 1.0517, 'lr': 2.9033374816555338e-05, 'epoch': 12.8}\n","{'loss': 1.186, 'lr': 2.885416536566744e-05, 'epoch': 12.82}\n","{'loss': 0.9783, 'lr': 2.8675286124424693e-05, 'epoch': 12.85}\n","{'loss': 1.208, 'lr': 2.8496739886173995e-05, 'epoch': 12.88}\n","{'loss': 0.978, 'lr': 2.8318529439062035e-05, 'epoch': 12.9}\n","{'loss': 1.0993, 'lr': 2.8140657565991958e-05, 'epoch': 12.93}\n","{'loss': 0.9743, 'lr': 2.7963127044579697e-05, 'epoch': 12.95}\n","{'loss': 0.9563, 'lr': 2.7785940647110763e-05, 'epoch': 12.97}\n","{'loss': 1.054, 'grad_norm': 1.927630066871643, 'learning_rate': 2.7609101140496863e-05, 'epoch': 13.0}\n"," 65% 520/800 [02:21<01:06,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.0259, 'lr': 2.7609101140496863e-05, 'epoch': 13.0}\n"," 65% 520/800 [02:22<01:06,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.0355, 'lr': 2.743261128623269e-05, 'epoch': 13.03}\n","{'loss': 0.9923, 'lr': 2.72564738403529e-05, 'epoch': 13.05}\n","{'loss': 1.1036, 'lr': 2.708069155338892e-05, 'epoch': 13.07}\n","{'loss': 0.9323, 'lr': 2.6905267170326143e-05, 'epoch': 13.1}\n","{'loss': 1.0268, 'lr': 2.6730203430560947e-05, 'epoch': 13.12}\n","{'loss': 1.0365, 'lr': 2.6555503067858013e-05, 'epoch': 13.15}\n","{'loss': 0.9855, 'lr': 2.6381168810307533e-05, 'epoch': 13.18}\n","{'loss': 1.0173, 'grad_norm': 2.859945297241211, 'learning_rate': 2.6207203380282747e-05, 'epoch': 13.2}\n","{'loss': 1.0931, 'lr': 2.6207203380282747e-05, 'epoch': 13.2}\n","{'loss': 0.9027, 'lr': 2.603360949439727e-05, 'epoch': 13.22}\n","{'loss': 0.9575, 'lr': 2.5860389863462765e-05, 'epoch': 13.25}\n","{'loss': 0.944, 'lr': 2.5687547192446647e-05, 'epoch': 13.28}\n","{'loss': 0.956, 'lr': 2.5515084180429716e-05, 'epoch': 13.3}\n","{'loss': 0.9861, 'lr': 2.5343003520564158e-05, 'epoch': 13.32}\n","{'loss': 0.9588, 'lr': 2.5171307900031345e-05, 'epoch': 13.35}\n","{'loss': 1.0809, 'lr': 2.500000000000001e-05, 'epoch': 13.38}\n","{'loss': 0.9849, 'grad_norm': 2.7187633514404297, 'learning_rate': 2.4829082495584242e-05, 'epoch': 13.4}\n","{'loss': 1.0155, 'lr': 2.4829082495584242e-05, 'epoch': 13.4}\n","{'loss': 1.0275, 'lr': 2.465855805580185e-05, 'epoch': 13.43}\n","{'loss': 1.3657, 'lr': 2.448842934353256e-05, 'epoch': 13.45}\n","{'loss': 1.0613, 'lr': 2.4318699015476493e-05, 'epoch': 13.47}\n","{'loss': 1.0033, 'lr': 2.414936972211272e-05, 'epoch': 13.5}\n","{'loss': 0.9987, 'lr': 2.3980444107657747e-05, 'epoch': 13.53}\n","{'loss': 1.0149, 'lr': 2.3811924810024384e-05, 'epoch': 13.55}\n","{'loss': 1.0964, 'lr': 2.3643814460780394e-05, 'epoch': 13.57}\n","{'loss': 1.0729, 'grad_norm': 2.959109306335449, 'learning_rate': 2.347611568510754e-05, 'epoch': 13.6}\n","{'loss': 0.9986, 'lr': 2.347611568510754e-05, 'epoch': 13.6}\n","{'loss': 0.9987, 'lr': 2.3308831101760486e-05, 'epoch': 13.62}\n","{'loss': 0.9515, 'lr': 2.3141963323025916e-05, 'epoch': 13.65}\n","{'loss': 1.0224, 'lr': 2.2975514954681838e-05, 'epoch': 13.68}\n","{'loss': 0.9962, 'lr': 2.2809488595956745e-05, 'epoch': 13.7}\n","{'loss': 1.0692, 'lr': 2.264388683948918e-05, 'epoch': 13.72}\n","{'loss': 0.9851, 'lr': 2.247871227128709e-05, 'epoch': 13.75}\n","{'loss': 1.0238, 'lr': 2.2313967470687593e-05, 'epoch': 13.78}\n","{'loss': 1.0057, 'grad_norm': 1.982679843902588, 'learning_rate': 2.2149655010316573e-05, 'epoch': 13.8}\n","{'loss': 0.9959, 'lr': 2.2149655010316573e-05, 'epoch': 13.8}\n","{'loss': 1.0416, 'lr': 2.1985777456048633e-05, 'epoch': 13.82}\n","{'loss': 0.974, 'lr': 2.1822337366966898e-05, 'epoch': 13.85}\n","{'loss': 0.9745, 'lr': 2.1659337295323118e-05, 'epoch': 13.88}\n","{'loss': 1.0303, 'lr': 2.149677978649786e-05, 'epoch': 13.9}\n","{'loss': 1.0513, 'lr': 2.1334667378960644e-05, 'epoch': 13.93}\n","{'loss': 0.9816, 'lr': 2.1173002604230425e-05, 'epoch': 13.95}\n","{'loss': 1.3354, 'lr': 2.1011787986835934e-05, 'epoch': 13.97}\n","{'loss': 1.0481, 'grad_norm': 7.350011825561523, 'learning_rate': 2.0851026044276406e-05, 'epoch': 14.0}\n"," 70% 560/800 [02:31<01:02,  3.81it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 0.9536, 'lr': 2.0851026044276406e-05, 'epoch': 14.0}\n"," 70% 560/800 [02:33<01:02,  3.81it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.0691, 'lr': 2.0690719286982125e-05, 'epoch': 14.03}\n","{'loss': 1.0019, 'lr': 2.0530870218275273e-05, 'epoch': 14.05}\n","{'loss': 0.9877, 'lr': 2.0371481334330912e-05, 'epoch': 14.07}\n","{'loss': 1.0202, 'lr': 2.0212555124137866e-05, 'epoch': 14.1}\n","{'loss': 1.0122, 'lr': 2.005409406946e-05, 'epoch': 14.12}\n","{'loss': 0.9581, 'lr': 1.9896100644797317e-05, 'epoch': 14.15}\n","{'loss': 0.9263, 'lr': 1.973857731734746e-05, 'epoch': 14.18}\n","{'loss': 0.9911, 'grad_norm': 1.7839906215667725, 'learning_rate': 1.958152654696705e-05, 'epoch': 14.2}\n","{'loss': 1.0372, 'lr': 1.958152654696705e-05, 'epoch': 14.2}\n","{'loss': 0.9977, 'lr': 1.942495078613341e-05, 'epoch': 14.22}\n","{'loss': 0.9226, 'lr': 1.9268852479906147e-05, 'epoch': 14.25}\n","{'loss': 0.9791, 'lr': 1.9113234065889014e-05, 'epoch': 14.28}\n","{'loss': 0.983, 'lr': 1.8958097974191907e-05, 'epoch': 14.3}\n","{'loss': 1.0125, 'lr': 1.8803446627392797e-05, 'epoch': 14.32}\n","{'loss': 0.9858, 'lr': 1.8649282440500015e-05, 'epoch': 14.35}\n","{'loss': 0.9904, 'lr': 1.849560782091445e-05, 'epoch': 14.38}\n","{'loss': 0.9885, 'grad_norm': 3.594400405883789, 'learning_rate': 1.834242516839203e-05, 'epoch': 14.4}\n","{'loss': 1.0006, 'lr': 1.834242516839203e-05, 'epoch': 14.4}\n","{'loss': 0.888, 'lr': 1.8189736875006185e-05, 'epoch': 14.43}\n","{'loss': 0.983, 'lr': 1.8037545325110504e-05, 'epoch': 14.45}\n","{'loss': 0.9495, 'lr': 1.788585289530158e-05, 'epoch': 14.47}\n","{'loss': 1.0104, 'lr': 1.7734661954381754e-05, 'epoch': 14.5}\n","{'loss': 0.9993, 'lr': 1.7583974863322274e-05, 'epoch': 14.53}\n","{'loss': 0.9334, 'lr': 1.7433793975226298e-05, 'epoch': 14.55}\n","{'loss': 0.9403, 'lr': 1.728412163529227e-05, 'epoch': 14.57}\n","{'loss': 0.963, 'grad_norm': 2.3539557456970215, 'learning_rate': 1.713496018077717e-05, 'epoch': 14.6}\n","{'loss': 1.1287, 'lr': 1.713496018077717e-05, 'epoch': 14.6}\n","{'loss': 1.0381, 'lr': 1.6986311940960147e-05, 'epoch': 14.62}\n","{'loss': 1.0148, 'lr': 1.6838179237106016e-05, 'epoch': 14.65}\n","{'loss': 0.9665, 'lr': 1.66905643824291e-05, 'epoch': 14.68}\n","{'loss': 1.0392, 'lr': 1.6543469682057106e-05, 'epoch': 14.7}\n","{'loss': 0.9187, 'lr': 1.6396897432995044e-05, 'epoch': 14.72}\n","{'loss': 1.0527, 'lr': 1.6250849924089484e-05, 'epoch': 14.75}\n","{'loss': 1.0935, 'lr': 1.6105329435992682e-05, 'epoch': 14.78}\n","{'loss': 1.0315, 'grad_norm': 4.750369548797607, 'learning_rate': 1.5960338241127093e-05, 'epoch': 14.8}\n","{'loss': 0.9946, 'lr': 1.5960338241127093e-05, 'epoch': 14.8}\n","{'loss': 0.9967, 'lr': 1.581587860364977e-05, 'epoch': 14.82}\n","{'loss': 1.0061, 'lr': 1.567195277941706e-05, 'epoch': 14.85}\n","{'loss': 0.9638, 'lr': 1.552856301594942e-05, 'epoch': 14.88}\n","{'loss': 0.9391, 'lr': 1.5385711552396227e-05, 'epoch': 14.9}\n","{'loss': 1.0003, 'lr': 1.5243400619500903e-05, 'epoch': 14.93}\n","{'loss': 1.109, 'lr': 1.5101632439565998e-05, 'epoch': 14.95}\n","{'loss': 0.9369, 'lr': 1.4960409226418576e-05, 'epoch': 14.97}\n","{'loss': 0.9933, 'grad_norm': 2.0208306312561035, 'learning_rate': 1.4819733185375534e-05, 'epoch': 15.0}\n"," 75% 600/800 [02:42<00:54,  3.68it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 1.0819, 'lr': 1.4819733185375534e-05, 'epoch': 15.0}\n"," 75% 600/800 [02:44<00:54,  3.68it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.9773, 'lr': 1.4679606513209283e-05, 'epoch': 15.03}\n","{'loss': 0.9967, 'lr': 1.4540031398113335e-05, 'epoch': 15.05}\n","{'loss': 0.9522, 'lr': 1.4401010019668226e-05, 'epoch': 15.07}\n","{'loss': 0.9708, 'lr': 1.4262544548807432e-05, 'epoch': 15.1}\n","{'loss': 0.9789, 'lr': 1.4124637147783432e-05, 'epoch': 15.12}\n","{'loss': 0.9307, 'lr': 1.3987289970134049e-05, 'epoch': 15.15}\n","{'loss': 0.9671, 'lr': 1.3850505160648709e-05, 'epoch': 15.18}\n","{'loss': 0.9819, 'grad_norm': 2.730997085571289, 'learning_rate': 1.371428485533498e-05, 'epoch': 15.2}\n","{'loss': 0.9925, 'lr': 1.371428485533498e-05, 'epoch': 15.2}\n","{'loss': 0.911, 'lr': 1.3578631181385305e-05, 'epoch': 15.22}\n","{'loss': 1.0041, 'lr': 1.3443546257143624e-05, 'epoch': 15.25}\n","{'loss': 0.9949, 'lr': 1.3309032192072463e-05, 'epoch': 15.28}\n","{'loss': 0.9645, 'lr': 1.3175091086719832e-05, 'epoch': 15.3}\n","{'loss': 0.9791, 'lr': 1.304172503268658e-05, 'epoch': 15.32}\n","{'loss': 0.9427, 'lr': 1.29089361125936e-05, 'epoch': 15.35}\n","{'loss': 0.9769, 'lr': 1.277672640004936e-05, 'epoch': 15.38}\n","{'loss': 0.9707, 'grad_norm': 2.6782751083374023, 'learning_rate': 1.2645097959617585e-05, 'epoch': 15.4}\n","{'loss': 0.946, 'lr': 1.2645097959617585e-05, 'epoch': 15.4}\n","{'loss': 0.9023, 'lr': 1.251405284678488e-05, 'epoch': 15.43}\n","{'loss': 0.9055, 'lr': 1.238359310792877e-05, 'epoch': 15.45}\n","{'loss': 0.9072, 'lr': 1.2253720780285639e-05, 'epoch': 15.47}\n","{'loss': 1.0132, 'lr': 1.2124437891918993e-05, 'epoch': 15.5}\n","{'loss': 0.9688, 'lr': 1.1995746461687734e-05, 'epoch': 15.53}\n","{'loss': 0.9868, 'lr': 1.186764849921468e-05, 'epoch': 15.55}\n","{'loss': 1.0319, 'lr': 1.174014600485514e-05, 'epoch': 15.57}\n","{'loss': 0.9577, 'grad_norm': 4.0490241050720215, 'learning_rate': 1.1613240969665685e-05, 'epoch': 15.6}\n","{'loss': 0.9137, 'lr': 1.1613240969665685e-05, 'epoch': 15.6}\n","{'loss': 0.9225, 'lr': 1.1486935375373126e-05, 'epoch': 15.62}\n","{'loss': 1.0491, 'lr': 1.1361231194343436e-05, 'epoch': 15.65}\n","{'loss': 1.1689, 'lr': 1.1236130389551092e-05, 'epoch': 15.68}\n","{'loss': 1.0531, 'lr': 1.1111634914548297e-05, 'epoch': 15.7}\n","{'loss': 1.0278, 'lr': 1.0987746713434576e-05, 'epoch': 15.72}\n","{'loss': 0.9872, 'lr': 1.0864467720826343e-05, 'epoch': 15.75}\n","{'loss': 0.9773, 'lr': 1.0741799861826706e-05, 'epoch': 15.78}\n","{'loss': 1.0124, 'grad_norm': 1.9568688869476318, 'learning_rate': 1.0619745051995472e-05, 'epoch': 15.8}\n","{'loss': 0.9697, 'lr': 1.0619745051995472e-05, 'epoch': 15.8}\n","{'loss': 0.9472, 'lr': 1.0498305197319115e-05, 'epoch': 15.82}\n","{'loss': 1.0028, 'lr': 1.0377482194181132e-05, 'epoch': 15.85}\n","{'loss': 0.952, 'lr': 1.0257277929332332e-05, 'epoch': 15.88}\n","{'loss': 1.0527, 'lr': 1.0137694279861454e-05, 'epoch': 15.9}\n","{'loss': 0.9404, 'lr': 1.0018733113165773e-05, 'epoch': 15.93}\n","{'loss': 0.9717, 'lr': 9.900396286922026e-06, 'epoch': 15.95}\n","{'loss': 0.9391, 'lr': 9.782685649057333e-06, 'epoch': 15.97}\n","{'loss': 0.972, 'grad_norm': 1.8338332176208496, 'learning_rate': 9.66560303772035e-06, 'epoch': 16.0}\n"," 80% 640/800 [02:54<00:38,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 0.9573, 'lr': 9.66560303772035e-06, 'epoch': 16.0}\n"," 80% 640/800 [02:55<00:38,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.958, 'lr': 9.549150281252633e-06, 'epoch': 16.02}\n","{'loss': 0.9453, 'lr': 9.433329198159974e-06, 'epoch': 16.05}\n","{'loss': 0.952, 'lr': 9.31814159708413e-06, 'epoch': 16.07}\n","{'loss': 0.9534, 'lr': 9.203589276774439e-06, 'epoch': 16.1}\n","{'loss': 0.9427, 'lr': 9.08967402605988e-06, 'epoch': 16.12}\n","{'loss': 0.9923, 'lr': 8.976397623821003e-06, 'epoch': 16.15}\n","{'loss': 0.9003, 'lr': 8.86376183896226e-06, 'epoch': 16.18}\n","{'loss': 0.9502, 'grad_norm': 1.7416445016860962, 'learning_rate': 8.751768430384305e-06, 'epoch': 16.2}\n","{'loss': 0.9852, 'lr': 8.751768430384305e-06, 'epoch': 16.2}\n","{'loss': 0.9637, 'lr': 8.640419146956557e-06, 'epoch': 16.23}\n","{'loss': 0.9345, 'lr': 8.529715727489912e-06, 'epoch': 16.25}\n","{'loss': 1.3059, 'lr': 8.419659900709536e-06, 'epoch': 16.27}\n","{'loss': 0.9337, 'lr': 8.310253385227946e-06, 'epoch': 16.3}\n","{'loss': 1.0364, 'lr': 8.201497889518073e-06, 'epoch': 16.32}\n","{'loss': 0.9053, 'lr': 8.093395111886687e-06, 'epoch': 16.35}\n","{'loss': 1.0675, 'lr': 7.985946740447791e-06, 'epoch': 16.38}\n","{'loss': 1.0165, 'grad_norm': 3.166764974594116, 'learning_rate': 7.879154453096304e-06, 'epoch': 16.4}\n","{'loss': 0.8713, 'lr': 7.879154453096304e-06, 'epoch': 16.4}\n","{'loss': 1.06, 'lr': 7.773019917481872e-06, 'epoch': 16.43}\n","{'loss': 1.0737, 'lr': 7.667544790982778e-06, 'epoch': 16.45}\n","{'loss': 1.0948, 'lr': 7.562730720680112e-06, 'epoch': 16.48}\n","{'loss': 0.9048, 'lr': 7.458579343331995e-06, 'epoch': 16.5}\n","{'loss': 0.9589, 'lr': 7.3550922853480915e-06, 'epoch': 16.52}\n","{'loss': 0.9666, 'lr': 7.252271162764129e-06, 'epoch': 16.55}\n","{'loss': 0.9853, 'lr': 7.150117581216748e-06, 'epoch': 16.57}\n","{'loss': 0.9894, 'grad_norm': 2.477670192718506, 'learning_rate': 7.048633135918347e-06, 'epoch': 16.6}\n","{'loss': 0.9623, 'lr': 7.048633135918347e-06, 'epoch': 16.6}\n","{'loss': 0.9556, 'lr': 6.947819411632223e-06, 'epoch': 16.62}\n","{'loss': 0.9604, 'lr': 6.8476779826478265e-06, 'epoch': 16.65}\n","{'loss': 0.9536, 'lr': 6.748210412756134e-06, 'epoch': 16.68}\n","{'loss': 0.9764, 'lr': 6.649418255225298e-06, 'epoch': 16.7}\n","{'loss': 0.9233, 'lr': 6.551303052776292e-06, 'epoch': 16.73}\n","{'loss': 1.0015, 'lr': 6.45386633755894e-06, 'epoch': 16.75}\n","{'loss': 0.9838, 'lr': 6.357109631127889e-06, 'epoch': 16.77}\n","{'loss': 0.9646, 'grad_norm': 2.2507667541503906, 'learning_rate': 6.261034444418879e-06, 'epoch': 16.8}\n","{'loss': 0.9511, 'lr': 6.261034444418879e-06, 'epoch': 16.8}\n","{'loss': 0.942, 'lr': 6.165642277725203e-06, 'epoch': 16.82}\n","{'loss': 1.039, 'lr': 6.07093462067419e-06, 'epoch': 16.85}\n","{'loss': 0.9274, 'lr': 5.976912952204017e-06, 'epoch': 16.88}\n","{'loss': 1.0334, 'lr': 5.883578740540546e-06, 'epoch': 16.9}\n","{'loss': 0.9676, 'lr': 5.79093344317449e-06, 'epoch': 16.93}\n","{'loss': 0.9663, 'lr': 5.698978506838532e-06, 'epoch': 16.95}\n","{'loss': 0.9833, 'lr': 5.607715367484861e-06, 'epoch': 16.98}\n","{'loss': 0.9763, 'grad_norm': 4.06760311126709, 'learning_rate': 5.51714545026264e-06, 'epoch': 17.0}\n"," 85% 680/800 [03:04<00:29,  4.08it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 0.9903, 'lr': 5.51714545026264e-06, 'epoch': 17.0}\n"," 85% 680/800 [03:06<00:29,  4.08it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.9803, 'lr': 5.4272701694958076e-06, 'epoch': 17.02}\n","{'loss': 0.9634, 'lr': 5.338090928660999e-06, 'epoch': 17.05}\n","{'loss': 1.0135, 'lr': 5.249609120365578e-06, 'epoch': 17.07}\n","{'loss': 0.9358, 'lr': 5.16182612632598e-06, 'epoch': 17.1}\n","{'loss': 0.9899, 'lr': 5.074743317346009e-06, 'epoch': 17.12}\n","{'loss': 1.0214, 'lr': 4.988362053295564e-06, 'epoch': 17.15}\n","{'loss': 0.9889, 'lr': 4.902683683089304e-06, 'epoch': 17.18}\n","{'loss': 0.9854, 'grad_norm': 2.921339750289917, 'learning_rate': 4.817709544665628e-06, 'epoch': 17.2}\n","{'loss': 0.9512, 'lr': 4.817709544665628e-06, 'epoch': 17.2}\n","{'loss': 0.9538, 'lr': 4.733440964965791e-06, 'epoch': 17.23}\n","{'loss': 1.0231, 'lr': 4.649879259913137e-06, 'epoch': 17.25}\n","{'loss': 0.9994, 'lr': 4.567025734392622e-06, 'epoch': 17.27}\n","{'loss': 1.0017, 'lr': 4.484881682230341e-06, 'epoch': 17.3}\n","{'loss': 0.9387, 'lr': 4.403448386173437e-06, 'epoch': 17.32}\n","{'loss': 0.8781, 'lr': 4.322727117869951e-06, 'epoch': 17.35}\n","{'loss': 1.2247, 'lr': 4.242719137849077e-06, 'epoch': 17.38}\n","{'loss': 0.9963, 'grad_norm': 7.761528015136719, 'learning_rate': 4.163425695501388e-06, 'epoch': 17.4}\n","{'loss': 0.9576, 'lr': 4.163425695501388e-06, 'epoch': 17.4}\n","{'loss': 0.9468, 'lr': 4.0848480290593625e-06, 'epoch': 17.43}\n","{'loss': 0.8796, 'lr': 4.00698736557808e-06, 'epoch': 17.45}\n","{'loss': 0.988, 'lr': 3.929844920915987e-06, 'epoch': 17.48}\n","{'loss': 0.9588, 'lr': 3.853421899715992e-06, 'epoch': 17.5}\n","{'loss': 0.9404, 'lr': 3.7777194953865667e-06, 'epoch': 17.52}\n","{'loss': 1.0635, 'lr': 3.702738890083207e-06, 'epoch': 17.55}\n","{'loss': 0.963, 'lr': 3.628481254689875e-06, 'epoch': 17.57}\n","{'loss': 0.9622, 'grad_norm': 2.278482437133789, 'learning_rate': 3.5549477488007854e-06, 'epoch': 17.6}\n","{'loss': 0.9905, 'lr': 3.5549477488007854e-06, 'epoch': 17.6}\n","{'loss': 0.9574, 'lr': 3.4821395207022766e-06, 'epoch': 17.62}\n","{'loss': 1.0124, 'lr': 3.4100577073548634e-06, 'epoch': 17.65}\n","{'loss': 0.968, 'lr': 3.3387034343755065e-06, 'epoch': 17.68}\n","{'loss': 1.028, 'lr': 3.2680778160200155e-06, 'epoch': 17.7}\n","{'loss': 0.9806, 'lr': 3.198181955165669e-06, 'epoch': 17.73}\n","{'loss': 0.9518, 'lr': 3.1290169432939553e-06, 'epoch': 17.75}\n","{'loss': 0.9094, 'lr': 3.060583860473587e-06, 'epoch': 17.77}\n","{'loss': 0.9748, 'grad_norm': 2.510957956314087, 'learning_rate': 2.9928837753435746e-06, 'epoch': 17.8}\n","{'loss': 0.8945, 'lr': 2.9928837753435746e-06, 'epoch': 17.8}\n","{'loss': 0.9333, 'lr': 2.9259177450965682e-06, 'epoch': 17.82}\n","{'loss': 1.127, 'lr': 2.8596868154623703e-06, 'epoch': 17.85}\n","{'loss': 0.9326, 'lr': 2.794192020691544e-06, 'epoch': 17.88}\n","{'loss': 0.9914, 'lr': 2.7294343835393368e-06, 'epoch': 17.9}\n","{'loss': 0.9052, 'lr': 2.665414915249631e-06, 'epoch': 17.93}\n","{'loss': 0.9561, 'lr': 2.6021346155392423e-06, 'epoch': 17.95}\n","{'loss': 0.978, 'lr': 2.539594472582213e-06, 'epoch': 17.98}\n","{'loss': 0.9648, 'grad_norm': 2.8016655445098877, 'learning_rate': 2.4777954629944477e-06, 'epoch': 18.0}\n"," 90% 720/800 [03:15<00:18,  4.24it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 0.9515, 'lr': 2.4777954629944477e-06, 'epoch': 18.0}\n"," 90% 720/800 [03:17<00:18,  4.24it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 1.0398, 'lr': 2.416738551818454e-06, 'epoch': 18.02}\n","{'loss': 0.883, 'lr': 2.3564246925082357e-06, 'epoch': 18.05}\n","{'loss': 0.9491, 'lr': 2.2968548269144574e-06, 'epoch': 18.07}\n","{'loss': 1.0509, 'lr': 2.238029885269677e-06, 'epoch': 18.1}\n","{'loss': 1.0078, 'lr': 2.179950786173879e-06, 'epoch': 18.12}\n","{'loss': 0.9793, 'lr': 2.122618436580082e-06, 'epoch': 18.15}\n","{'loss': 0.9295, 'lr': 2.066033731780209e-06, 'epoch': 18.18}\n","{'loss': 0.9739, 'grad_norm': 2.6963987350463867, 'learning_rate': 2.01019755539108e-06, 'epoch': 18.2}\n","{'loss': 0.9102, 'lr': 2.01019755539108e-06, 'epoch': 18.2}\n","{'loss': 1.0027, 'lr': 1.9551107793406355e-06, 'epoch': 18.23}\n","{'loss': 0.9879, 'lr': 1.9007742638543102e-06, 'epoch': 18.25}\n","{'loss': 0.923, 'lr': 1.8471888574415951e-06, 'epoch': 18.27}\n","{'loss': 0.9361, 'lr': 1.794355396882813e-06, 'epoch': 18.3}\n","{'loss': 0.9284, 'lr': 1.7422747072160017e-06, 'epoch': 18.32}\n","{'loss': 0.9342, 'lr': 1.6909476017240912e-06, 'epoch': 18.35}\n","{'loss': 0.9429, 'lr': 1.6403748819221466e-06, 'epoch': 18.38}\n","{'loss': 0.9457, 'grad_norm': 2.693851947784424, 'learning_rate': 1.5905573375449012e-06, 'epoch': 18.4}\n","{'loss': 0.9495, 'lr': 1.5905573375449012e-06, 'epoch': 18.4}\n","{'loss': 1.0118, 'lr': 1.5414957465343882e-06, 'epoch': 18.43}\n","{'loss': 0.9716, 'lr': 1.4931908750278e-06, 'epoch': 18.45}\n","{'loss': 1.0099, 'lr': 1.4456434773455541e-06, 'epoch': 18.48}\n","{'loss': 0.9904, 'lr': 1.3988542959794627e-06, 'epoch': 18.5}\n","{'loss': 0.9728, 'lr': 1.3528240615811816e-06, 'epoch': 18.52}\n","{'loss': 0.9336, 'lr': 1.3075534929507693e-06, 'epoch': 18.55}\n","{'loss': 0.9905, 'lr': 1.2630432970255013e-06, 'epoch': 18.57}\n","{'loss': 0.9788, 'grad_norm': 7.702703952789307, 'learning_rate': 1.2192941688687843e-06, 'epoch': 18.6}\n","{'loss': 0.8893, 'lr': 1.2192941688687843e-06, 'epoch': 18.6}\n","{'loss': 0.8813, 'lr': 1.1763067916593262e-06, 'epoch': 18.62}\n","{'loss': 1.0486, 'lr': 1.1340818366804729e-06, 'epoch': 18.65}\n","{'loss': 0.962, 'lr': 1.0926199633097157e-06, 'epoch': 18.68}\n","{'loss': 0.9683, 'lr': 1.0519218190084056e-06, 'epoch': 18.7}\n","{'loss': 0.9298, 'lr': 1.0119880393116176e-06, 'epoch': 18.73}\n","{'loss': 0.9334, 'lr': 9.728192478182574e-07, 'epoch': 18.75}\n","{'loss': 0.8934, 'lr': 9.344160561812921e-07, 'epoch': 18.77}\n","{'loss': 0.9383, 'grad_norm': 2.5298025608062744, 'learning_rate': 8.967790640982465e-07, 'epoch': 18.8}\n","{'loss': 0.9944, 'lr': 8.967790640982465e-07, 'epoch': 18.8}\n","{'loss': 0.927, 'lr': 8.599088593017723e-07, 'epoch': 18.82}\n","{'loss': 1.0061, 'lr': 8.238060175505269e-07, 'epoch': 18.85}\n","{'loss': 0.9795, 'lr': 7.884711026201585e-07, 'epoch': 18.88}\n","{'loss': 0.9861, 'lr': 7.53904666294497e-07, 'epoch': 18.9}\n","{'loss': 1.0218, 'lr': 7.201072483569549e-07, 'epoch': 18.93}\n","{'loss': 0.9766, 'lr': 6.870793765820782e-07, 'epoch': 18.95}\n","{'loss': 0.9721, 'lr': 6.548215667273206e-07, 'epoch': 18.98}\n","{'loss': 0.9829, 'grad_norm': 2.9979124069213867, 'learning_rate': 6.233343225249933e-07, 'epoch': 19.0}\n"," 95% 760/800 [03:26<00:09,  4.13it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 0.9712, 'lr': 6.233343225249933e-07, 'epoch': 19.0}\n"," 95% 760/800 [03:28<00:09,  4.13it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 0.9396, 'lr': 5.92618135674361e-07, 'epoch': 19.02}\n","{'loss': 0.9759, 'lr': 5.626734858340255e-07, 'epoch': 19.05}\n","{'loss': 0.9493, 'lr': 5.335008406143815e-07, 'epoch': 19.07}\n","{'loss': 0.9393, 'lr': 5.051006555703453e-07, 'epoch': 19.1}\n","{'loss': 0.9519, 'lr': 4.774733741942206e-07, 'epoch': 19.12}\n","{'loss': 1.0149, 'lr': 4.5061942790879386e-07, 'epoch': 19.15}\n","{'loss': 1.0041, 'lr': 4.2453923606057265e-07, 'epoch': 19.18}\n","{'loss': 0.9683, 'grad_norm': 2.8145828247070312, 'learning_rate': 3.992332059132631e-07, 'epoch': 19.2}\n","{'loss': 0.9081, 'lr': 3.992332059132631e-07, 'epoch': 19.2}\n","{'loss': 1.0013, 'lr': 3.747017326413971e-07, 'epoch': 19.23}\n","{'loss': 0.9155, 'lr': 3.5094519932415417e-07, 'epoch': 19.25}\n","{'loss': 1.0674, 'lr': 3.2796397693939385e-07, 'epoch': 19.27}\n","{'loss': 1.1433, 'lr': 3.0575842435785486e-07, 'epoch': 19.3}\n","{'loss': 0.8799, 'lr': 2.843288883375539e-07, 'epoch': 19.32}\n","{'loss': 0.9033, 'lr': 2.6367570351836237e-07, 'epoch': 19.35}\n","{'loss': 0.9055, 'lr': 2.437991924167937e-07, 'epoch': 19.38}\n","{'loss': 0.9655, 'grad_norm': 2.4377787113189697, 'learning_rate': 2.2469966542096322e-07, 'epoch': 19.4}\n","{'loss': 0.9713, 'lr': 2.2469966542096322e-07, 'epoch': 19.4}\n","{'loss': 0.9246, 'lr': 2.0637742078573607e-07, 'epoch': 19.43}\n","{'loss': 0.9451, 'lr': 1.8883274462806467e-07, 'epoch': 19.45}\n","{'loss': 0.8841, 'lr': 1.7206591092253642e-07, 'epoch': 19.48}\n","{'loss': 1.2448, 'lr': 1.560771814970885e-07, 'epoch': 19.5}\n","{'loss': 1.1377, 'lr': 1.4086680602891643e-07, 'epoch': 19.52}\n","{'loss': 0.948, 'lr': 1.264350220405719e-07, 'epoch': 19.55}\n","{'loss': 0.9433, 'lr': 1.1278205489626547e-07, 'epoch': 19.57}\n","{'loss': 0.9999, 'grad_norm': 1.8625210523605347, 'learning_rate': 9.99081177983363e-08, 'epoch': 19.6}\n","{'loss': 0.9046, 'lr': 9.99081177983363e-08, 'epoch': 19.6}\n","{'loss': 0.9666, 'lr': 8.781341178393244e-08, 'epoch': 19.62}\n","{'loss': 0.9986, 'lr': 7.649812572185222e-08, 'epoch': 19.65}\n","{'loss': 0.9371, 'lr': 6.596243630963006e-08, 'epoch': 19.68}\n","{'loss': 1.0839, 'lr': 5.620650807073857e-08, 'epoch': 19.7}\n","{'loss': 0.9616, 'lr': 4.723049335204066e-08, 'epoch': 19.73}\n","{'loss': 0.9479, 'lr': 3.9034532321408076e-08, 'epoch': 19.75}\n","{'loss': 0.998, 'lr': 3.161875296553429e-08, 'epoch': 19.77}\n","{'loss': 0.9748, 'grad_norm': 2.4224002361297607, 'learning_rate': 2.4983271087924974e-08, 'epoch': 19.8}\n","{'loss': 1.0166, 'lr': 2.4983271087924974e-08, 'epoch': 19.8}\n","{'loss': 0.968, 'lr': 1.9128190307105e-08, 'epoch': 19.82}\n","{'loss': 0.9735, 'lr': 1.4053602054991955e-08, 'epoch': 19.85}\n","{'loss': 0.9665, 'lr': 9.75958557545842e-09, 'epoch': 19.88}\n","{'loss': 1.1027, 'lr': 6.246207923116254e-09, 'epoch': 19.9}\n","{'loss': 1.1759, 'lr': 3.513523962256349e-09, 'epoch': 19.93}\n","{'loss': 0.936, 'lr': 1.5615763659881933e-09, 'epoch': 19.95}\n","{'loss': 1.0524, 'lr': 3.903956155848487e-10, 'epoch': 19.98}\n","{'loss': 1.0239, 'grad_norm': 2.8890762329101562, 'learning_rate': 0.0, 'epoch': 20.0}\n","{'train_runtime': 219.428, 'train_samples_per_second': 7.292, 'train_steps_per_second': 3.646, 'train_loss': 2.5863156390190123, 'epoch': 20.0}\n","100% 800/800 [03:39<00:00,  3.65it/s]\n","******model_save_path is model7b_M1_family_epoch_20_r_48_moreData_lr_1e-4/adapter_model.safetensors******\n"]}],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 20 \\\n","    --json_path \"./dataset/family_k1k2.json\" \\\n","    --output_dir \"model7b_M1_family_epoch_20_r_48_moreData_lr_1e-4\" \\\n","    --save_steps 40 \\\n","    --LORA_R 48 \\\n","    --lr 1e-4"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":723723,"status":"ok","timestamp":1724768280575,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"qYe53Qw5GtkL","outputId":"04c8d42f-b099-4147-cd9e-a402ad07256f"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:07<00:00,  1.85s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:55<00:00, 13.83s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:14<00:00,  3.55s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:07<00:00,  1.77s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:08<00:00,  2.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:09<00:00,  2.46s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:14<00:00,  3.69s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:12<00:00,  3.03s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:10<00:00,  2.56s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:10<00:00,  2.56s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:10<00:00,  2.55s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.25s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.26s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.28s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.18s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.25s/it]\n"]}],"source":["!./predict_M1k1.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sjm651NiJn5r"},"outputs":[],"source":["!./predict_M1k2.sh\n","!./predict_M1k3.sh"]},{"cell_type":"markdown","metadata":{"id":"3kAET1DDJ2Rj"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325077,"status":"ok","timestamp":1724762894232,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"ZTBhg-jB0hzh","outputId":"695ee1c3-fe39-4a3b-8918-7d1f4fbf80b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","tokenizer_config.json: 100% 34.2k/34.2k [00:00<00:00, 4.20MB/s]\n","tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 44.8MB/s]\n","special_tokens_map.json: 100% 636/636 [00:00<00:00, 5.18MB/s]\n","tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 256MB/s]\n","auto\n","False\n","config.json: 100% 694/694 [00:00<00:00, 5.75MB/s]\n","model.safetensors.index.json: 100% 20.9k/20.9k [00:00<00:00, 82.8MB/s]\n","Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n","model-00001-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00004.safetensors:   1% 31.5M/5.00G [00:00<00:17, 282MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   1% 62.9M/5.00G [00:00<00:17, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   2% 94.4M/5.00G [00:00<00:17, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   3% 126M/5.00G [00:00<00:17, 278MB/s] \u001b[A\n","model-00001-of-00004.safetensors:   3% 157M/5.00G [00:00<00:17, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   4% 189M/5.00G [00:00<00:17, 276MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   4% 220M/5.00G [00:00<00:17, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   5% 252M/5.00G [00:00<00:17, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   6% 283M/5.00G [00:01<00:16, 282MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   6% 315M/5.00G [00:01<00:16, 283MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   7% 346M/5.00G [00:01<00:16, 283MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   8% 377M/5.00G [00:01<00:16, 276MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   8% 409M/5.00G [00:01<00:16, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   9% 440M/5.00G [00:01<00:16, 284MB/s]\u001b[A\n","model-00001-of-00004.safetensors:   9% 472M/5.00G [00:01<00:15, 287MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  10% 503M/5.00G [00:01<00:15, 290MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  11% 535M/5.00G [00:01<00:15, 286MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  11% 566M/5.00G [00:02<00:15, 289MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  12% 598M/5.00G [00:02<00:15, 289MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  13% 629M/5.00G [00:02<00:15, 288MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  13% 661M/5.00G [00:02<00:15, 287MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  14% 692M/5.00G [00:02<00:15, 285MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  14% 724M/5.00G [00:02<00:14, 286MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  15% 755M/5.00G [00:02<00:15, 272MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  16% 786M/5.00G [00:02<00:15, 273MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  16% 818M/5.00G [00:02<00:15, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  17% 849M/5.00G [00:03<00:15, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  18% 881M/5.00G [00:03<00:15, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  18% 912M/5.00G [00:03<00:15, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  19% 944M/5.00G [00:03<00:15, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  20% 975M/5.00G [00:03<00:14, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  20% 1.01G/5.00G [00:03<00:14, 273MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  21% 1.04G/5.00G [00:03<00:14, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  21% 1.07G/5.00G [00:03<00:14, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  22% 1.10G/5.00G [00:03<00:14, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  23% 1.13G/5.00G [00:04<00:14, 261MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  23% 1.16G/5.00G [00:04<00:14, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  24% 1.20G/5.00G [00:04<00:14, 266MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  25% 1.23G/5.00G [00:04<00:14, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  25% 1.26G/5.00G [00:04<00:14, 266MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  26% 1.29G/5.00G [00:04<00:13, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  26% 1.32G/5.00G [00:04<00:13, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  27% 1.35G/5.00G [00:04<00:13, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  28% 1.38G/5.00G [00:05<00:13, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  28% 1.42G/5.00G [00:05<00:13, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  29% 1.45G/5.00G [00:05<00:13, 266MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  30% 1.48G/5.00G [00:05<00:12, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  30% 1.51G/5.00G [00:05<00:12, 272MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  31% 1.54G/5.00G [00:05<00:12, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  31% 1.57G/5.00G [00:05<00:12, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  32% 1.60G/5.00G [00:05<00:12, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  33% 1.64G/5.00G [00:05<00:12, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  33% 1.67G/5.00G [00:06<00:12, 259MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  34% 1.70G/5.00G [00:06<00:12, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  35% 1.73G/5.00G [00:06<00:12, 265MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  35% 1.76G/5.00G [00:06<00:12, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  36% 1.79G/5.00G [00:06<00:11, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  37% 1.82G/5.00G [00:06<00:11, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  37% 1.86G/5.00G [00:06<00:11, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  38% 1.89G/5.00G [00:06<00:11, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  38% 1.92G/5.00G [00:07<00:11, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  39% 1.95G/5.00G [00:07<00:11, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  40% 1.98G/5.00G [00:07<00:10, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  40% 2.01G/5.00G [00:07<00:10, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  41% 2.04G/5.00G [00:07<00:10, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  42% 2.08G/5.00G [00:07<00:10, 276MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  42% 2.11G/5.00G [00:07<00:10, 263MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  43% 2.14G/5.00G [00:07<00:10, 266MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  43% 2.17G/5.00G [00:07<00:10, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  44% 2.20G/5.00G [00:08<00:10, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  45% 2.23G/5.00G [00:08<00:10, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  45% 2.26G/5.00G [00:08<00:09, 275MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  46% 2.30G/5.00G [00:08<00:09, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  47% 2.33G/5.00G [00:08<00:10, 261MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  47% 2.36G/5.00G [00:08<00:09, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  48% 2.39G/5.00G [00:08<00:09, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  48% 2.42G/5.00G [00:08<00:09, 272MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  49% 2.45G/5.00G [00:08<00:09, 273MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  50% 2.49G/5.00G [00:09<00:08, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  50% 2.52G/5.00G [00:09<00:08, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  51% 2.55G/5.00G [00:09<00:08, 280MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  52% 2.58G/5.00G [00:09<00:08, 280MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  52% 2.61G/5.00G [00:09<00:08, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  53% 2.64G/5.00G [00:09<00:08, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  54% 2.67G/5.00G [00:09<00:08, 280MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  54% 2.71G/5.00G [00:09<00:08, 280MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  55% 2.74G/5.00G [00:10<00:08, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  55% 2.77G/5.00G [00:10<00:08, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  56% 2.80G/5.00G [00:10<00:08, 249MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  57% 2.83G/5.00G [00:10<00:08, 258MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  57% 2.86G/5.00G [00:10<00:08, 263MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  58% 2.89G/5.00G [00:10<00:07, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  59% 2.93G/5.00G [00:10<00:07, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  59% 2.96G/5.00G [00:10<00:07, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  60% 2.99G/5.00G [00:10<00:07, 272MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  60% 3.02G/5.00G [00:11<00:07, 265MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  61% 3.05G/5.00G [00:11<00:07, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  62% 3.08G/5.00G [00:11<00:07, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  62% 3.11G/5.00G [00:11<00:06, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  63% 3.15G/5.00G [00:11<00:06, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  64% 3.18G/5.00G [00:11<00:06, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  64% 3.21G/5.00G [00:11<00:06, 262MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  65% 3.24G/5.00G [00:11<00:06, 257MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  65% 3.27G/5.00G [00:12<00:06, 259MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  66% 3.30G/5.00G [00:12<00:06, 263MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  67% 3.33G/5.00G [00:12<00:06, 260MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  67% 3.37G/5.00G [00:12<00:06, 261MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  68% 3.40G/5.00G [00:12<00:06, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  69% 3.43G/5.00G [00:12<00:09, 173MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  69% 3.46G/5.00G [00:13<00:08, 183MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  70% 3.49G/5.00G [00:13<00:07, 200MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  71% 3.52G/5.00G [00:13<00:06, 213MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  71% 3.55G/5.00G [00:13<00:06, 221MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  72% 3.59G/5.00G [00:13<00:06, 229MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  72% 3.62G/5.00G [00:13<00:05, 241MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  73% 3.65G/5.00G [00:13<00:05, 252MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  74% 3.68G/5.00G [00:13<00:05, 251MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  74% 3.71G/5.00G [00:13<00:04, 260MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  75% 3.74G/5.00G [00:14<00:04, 265MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  76% 3.77G/5.00G [00:14<00:04, 267MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  76% 3.81G/5.00G [00:14<00:04, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  77% 3.84G/5.00G [00:14<00:04, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  77% 3.87G/5.00G [00:14<00:04, 277MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  78% 3.90G/5.00G [00:14<00:03, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  79% 3.93G/5.00G [00:14<00:03, 281MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  79% 3.96G/5.00G [00:14<00:03, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  80% 4.00G/5.00G [00:14<00:03, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  81% 4.03G/5.00G [00:15<00:03, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  81% 4.06G/5.00G [00:15<00:03, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  82% 4.09G/5.00G [00:15<00:03, 281MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  82% 4.12G/5.00G [00:15<00:03, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  83% 4.15G/5.00G [00:15<00:03, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  84% 4.18G/5.00G [00:15<00:02, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  84% 4.22G/5.00G [00:15<00:02, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  85% 4.25G/5.00G [00:15<00:02, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  86% 4.28G/5.00G [00:15<00:02, 279MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  86% 4.31G/5.00G [00:16<00:02, 282MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  87% 4.34G/5.00G [00:16<00:02, 278MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  88% 4.37G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  88% 4.40G/5.00G [00:16<00:02, 275MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  89% 4.44G/5.00G [00:16<00:02, 276MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  89% 4.47G/5.00G [00:16<00:01, 274MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  90% 4.50G/5.00G [00:16<00:01, 273MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  91% 4.53G/5.00G [00:16<00:01, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  91% 4.56G/5.00G [00:17<00:01, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  92% 4.59G/5.00G [00:17<00:01, 268MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  93% 4.62G/5.00G [00:17<00:01, 269MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  93% 4.66G/5.00G [00:17<00:01, 270MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  94% 4.69G/5.00G [00:17<00:01, 271MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  94% 4.72G/5.00G [00:17<00:01, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  95% 4.75G/5.00G [00:17<00:00, 266MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  96% 4.78G/5.00G [00:17<00:00, 265MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  96% 4.81G/5.00G [00:17<00:00, 264MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  97% 4.84G/5.00G [00:18<00:00, 263MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  98% 4.88G/5.00G [00:18<00:00, 255MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  98% 4.91G/5.00G [00:18<00:00, 257MB/s]\u001b[A\n","model-00001-of-00004.safetensors:  99% 4.94G/5.00G [00:18<00:00, 263MB/s]\u001b[A\n","model-00001-of-00004.safetensors: 100% 5.00G/5.00G [00:18<00:00, 267MB/s]\n","Downloading shards:  25% 1/4 [00:18<00:56, 18.78s/it]\n","model-00002-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00004.safetensors:   1% 31.5M/4.98G [00:00<00:19, 248MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   1% 62.9M/4.98G [00:00<00:18, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   2% 94.4M/4.98G [00:00<00:17, 280MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   3% 126M/4.98G [00:00<00:17, 280MB/s] \u001b[A\n","model-00002-of-00004.safetensors:   3% 157M/4.98G [00:00<00:18, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 189M/4.98G [00:00<00:18, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   4% 220M/4.98G [00:00<00:17, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   5% 252M/4.98G [00:00<00:17, 274MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 283M/4.98G [00:01<00:17, 274MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   6% 315M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   7% 346M/4.98G [00:01<00:16, 275MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 377M/4.98G [00:01<00:17, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   8% 409M/4.98G [00:01<00:17, 257MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 440M/4.98G [00:01<00:18, 247MB/s]\u001b[A\n","model-00002-of-00004.safetensors:   9% 472M/4.98G [00:01<00:17, 255MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  10% 503M/4.98G [00:01<00:17, 257MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 535M/4.98G [00:02<00:19, 234MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  11% 566M/4.98G [00:02<00:19, 231MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  12% 598M/4.98G [00:02<00:18, 234MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 629M/4.98G [00:02<00:18, 234MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  13% 661M/4.98G [00:02<00:18, 238MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  14% 692M/4.98G [00:02<00:17, 248MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 724M/4.98G [00:02<00:16, 256MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  15% 755M/4.98G [00:02<00:16, 263MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 786M/4.98G [00:03<00:15, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  16% 818M/4.98G [00:03<00:15, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  17% 849M/4.98G [00:03<00:15, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 881M/4.98G [00:03<00:15, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  18% 912M/4.98G [00:03<00:15, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  19% 944M/4.98G [00:03<00:15, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 975M/4.98G [00:03<00:14, 271MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  20% 1.01G/4.98G [00:03<00:14, 266MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.04G/4.98G [00:03<00:14, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  21% 1.07G/4.98G [00:04<00:14, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  22% 1.10G/4.98G [00:04<00:22, 176MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.13G/4.98G [00:04<00:19, 194MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  23% 1.16G/4.98G [00:04<00:28, 132MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  24% 1.20G/4.98G [00:05<00:24, 152MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.23G/4.98G [00:05<00:22, 169MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  25% 1.26G/4.98G [00:05<00:19, 190MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  26% 1.29G/4.98G [00:05<00:17, 207MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.32G/4.98G [00:05<00:16, 223MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  27% 1.35G/4.98G [00:05<00:15, 231MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.38G/4.98G [00:05<00:15, 239MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  28% 1.42G/4.98G [00:05<00:14, 251MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  29% 1.45G/4.98G [00:06<00:13, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.48G/4.98G [00:06<00:13, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  30% 1.51G/4.98G [00:06<00:13, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  31% 1.54G/4.98G [00:06<00:13, 257MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.57G/4.98G [00:06<00:12, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  32% 1.60G/4.98G [00:06<00:12, 271MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.64G/4.98G [00:06<00:12, 273MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  33% 1.67G/4.98G [00:06<00:12, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  34% 1.70G/4.98G [00:07<00:12, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.73G/4.98G [00:07<00:12, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  35% 1.76G/4.98G [00:07<00:11, 271MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  36% 1.79G/4.98G [00:07<00:11, 273MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.82G/4.98G [00:07<00:11, 273MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  37% 1.86G/4.98G [00:07<00:11, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  38% 1.89G/4.98G [00:07<00:11, 271MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.92G/4.98G [00:07<00:11, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  39% 1.95G/4.98G [00:07<00:11, 273MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 1.98G/4.98G [00:08<00:11, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  40% 2.01G/4.98G [00:08<00:11, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  41% 2.04G/4.98G [00:08<00:11, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.08G/4.98G [00:08<00:10, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  42% 2.11G/4.98G [00:08<00:10, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  43% 2.14G/4.98G [00:08<00:10, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.17G/4.98G [00:08<00:10, 266MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  44% 2.20G/4.98G [00:08<00:10, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.23G/4.98G [00:08<00:10, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  45% 2.26G/4.98G [00:09<00:10, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  46% 2.30G/4.98G [00:09<00:10, 254MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.33G/4.98G [00:09<00:10, 256MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  47% 2.36G/4.98G [00:09<00:10, 250MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  48% 2.39G/4.98G [00:09<00:10, 250MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.42G/4.98G [00:09<00:10, 254MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  49% 2.45G/4.98G [00:09<00:10, 251MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  50% 2.49G/4.98G [00:09<00:09, 256MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.52G/4.98G [00:10<00:09, 254MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  51% 2.55G/4.98G [00:10<00:09, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.58G/4.98G [00:10<00:09, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  52% 2.61G/4.98G [00:10<00:09, 263MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  53% 2.64G/4.98G [00:10<00:08, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.67G/4.98G [00:10<00:08, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  54% 2.71G/4.98G [00:10<00:08, 260MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  55% 2.74G/4.98G [00:10<00:08, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.77G/4.98G [00:11<00:08, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  56% 2.80G/4.98G [00:11<00:08, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.83G/4.98G [00:11<00:08, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  57% 2.86G/4.98G [00:11<00:08, 262MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  58% 2.89G/4.98G [00:11<00:07, 263MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.93G/4.98G [00:11<00:07, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  59% 2.96G/4.98G [00:11<00:07, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  60% 2.99G/4.98G [00:11<00:07, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.02G/4.98G [00:12<00:07, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  61% 3.05G/4.98G [00:12<00:07, 266MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.08G/4.98G [00:12<00:07, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  62% 3.11G/4.98G [00:12<00:07, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  63% 3.15G/4.98G [00:12<00:06, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.18G/4.98G [00:12<00:06, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  64% 3.21G/4.98G [00:12<00:06, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  65% 3.24G/4.98G [00:12<00:06, 262MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.27G/4.98G [00:12<00:06, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  66% 3.30G/4.98G [00:13<00:06, 256MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  67% 3.33G/4.98G [00:13<00:06, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.37G/4.98G [00:13<00:06, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  68% 3.40G/4.98G [00:13<00:06, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.43G/4.98G [00:13<00:06, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  69% 3.46G/4.98G [00:13<00:05, 263MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  70% 3.49G/4.98G [00:13<00:05, 266MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.52G/4.98G [00:13<00:05, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  71% 3.55G/4.98G [00:14<00:05, 260MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  72% 3.59G/4.98G [00:14<00:05, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.62G/4.98G [00:14<00:05, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  73% 3.65G/4.98G [00:14<00:04, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.68G/4.98G [00:14<00:04, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  74% 3.71G/4.98G [00:14<00:04, 274MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  75% 3.74G/4.98G [00:14<00:04, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.77G/4.98G [00:14<00:04, 273MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  76% 3.81G/4.98G [00:14<00:04, 277MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  77% 3.84G/4.98G [00:15<00:04, 276MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.87G/4.98G [00:15<00:04, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  78% 3.90G/4.98G [00:15<00:06, 174MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  79% 3.93G/4.98G [00:15<00:05, 196MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 3.96G/4.98G [00:15<00:04, 215MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  80% 4.00G/4.98G [00:15<00:04, 224MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.03G/4.98G [00:16<00:04, 232MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  81% 4.06G/4.98G [00:16<00:03, 246MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  82% 4.09G/4.98G [00:16<00:03, 256MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.12G/4.98G [00:16<00:03, 262MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  83% 4.15G/4.98G [00:16<00:03, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  84% 4.18G/4.98G [00:16<00:03, 260MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.22G/4.98G [00:16<00:02, 262MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  85% 4.25G/4.98G [00:16<00:02, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.28G/4.98G [00:16<00:02, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  86% 4.31G/4.98G [00:17<00:02, 253MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  87% 4.34G/4.98G [00:17<00:02, 259MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.37G/4.98G [00:17<00:02, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  88% 4.40G/4.98G [00:17<00:02, 266MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  89% 4.44G/4.98G [00:17<00:02, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.47G/4.98G [00:17<00:01, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  90% 4.50G/4.98G [00:17<00:01, 270MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  91% 4.53G/4.98G [00:17<00:01, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.56G/4.98G [00:18<00:01, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  92% 4.59G/4.98G [00:18<00:01, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.62G/4.98G [00:18<00:01, 269MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  93% 4.66G/4.98G [00:18<00:01, 272MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  94% 4.69G/4.98G [00:18<00:01, 265MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.72G/4.98G [00:18<00:00, 268MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  95% 4.75G/4.98G [00:18<00:00, 264MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  96% 4.78G/4.98G [00:18<00:00, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.81G/4.98G [00:18<00:00, 257MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  97% 4.84G/4.98G [00:19<00:00, 258MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.88G/4.98G [00:19<00:00, 262MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  98% 4.91G/4.98G [00:19<00:00, 267MB/s]\u001b[A\n","model-00002-of-00004.safetensors:  99% 4.94G/4.98G [00:19<00:00, 261MB/s]\u001b[A\n","model-00002-of-00004.safetensors: 100% 4.98G/4.98G [00:19<00:00, 254MB/s]\n","Downloading shards:  50% 2/4 [00:38<00:38, 19.36s/it]\n","model-00003-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n","model-00003-of-00004.safetensors:   1% 31.5M/4.98G [00:00<00:17, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   1% 62.9M/4.98G [00:00<00:17, 275MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   2% 94.4M/4.98G [00:00<00:17, 275MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   3% 126M/4.98G [00:00<00:18, 269MB/s] \u001b[A\n","model-00003-of-00004.safetensors:   3% 157M/4.98G [00:00<00:18, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   4% 189M/4.98G [00:00<00:18, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   4% 220M/4.98G [00:00<00:17, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   5% 252M/4.98G [00:00<00:18, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   6% 283M/4.98G [00:01<00:17, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   6% 315M/4.98G [00:01<00:17, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   7% 346M/4.98G [00:01<00:17, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   8% 377M/4.98G [00:01<00:17, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   8% 409M/4.98G [00:01<00:16, 277MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   9% 440M/4.98G [00:01<00:16, 279MB/s]\u001b[A\n","model-00003-of-00004.safetensors:   9% 472M/4.98G [00:01<00:16, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  10% 503M/4.98G [00:01<00:16, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  11% 535M/4.98G [00:01<00:16, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  11% 566M/4.98G [00:02<00:16, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  12% 598M/4.98G [00:02<00:15, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  13% 629M/4.98G [00:02<00:15, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  13% 661M/4.98G [00:02<00:15, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  14% 692M/4.98G [00:02<00:15, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  15% 724M/4.98G [00:02<00:15, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  15% 755M/4.98G [00:02<00:15, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  16% 786M/4.98G [00:02<00:15, 270MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  16% 818M/4.98G [00:03<00:15, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  17% 849M/4.98G [00:03<00:15, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  18% 881M/4.98G [00:03<00:15, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  18% 912M/4.98G [00:03<00:15, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  19% 944M/4.98G [00:03<00:15, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  20% 975M/4.98G [00:03<00:14, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  20% 1.01G/4.98G [00:03<00:14, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  21% 1.04G/4.98G [00:03<00:14, 277MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  21% 1.07G/4.98G [00:03<00:14, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  22% 1.10G/4.98G [00:04<00:14, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  23% 1.13G/4.98G [00:04<00:14, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  23% 1.16G/4.98G [00:04<00:14, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  24% 1.20G/4.98G [00:04<00:13, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  25% 1.23G/4.98G [00:04<00:13, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  25% 1.26G/4.98G [00:04<00:13, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  26% 1.29G/4.98G [00:04<00:13, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  27% 1.32G/4.98G [00:04<00:13, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  27% 1.35G/4.98G [00:05<00:13, 275MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  28% 1.38G/4.98G [00:05<00:13, 275MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  28% 1.42G/4.98G [00:05<00:12, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  29% 1.45G/4.98G [00:05<00:13, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  30% 1.48G/4.98G [00:05<00:13, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  30% 1.51G/4.98G [00:05<00:13, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  31% 1.54G/4.98G [00:05<00:12, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  32% 1.57G/4.98G [00:05<00:12, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  32% 1.60G/4.98G [00:05<00:13, 255MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  33% 1.64G/4.98G [00:06<00:19, 168MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  33% 1.67G/4.98G [00:06<00:17, 186MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  34% 1.70G/4.98G [00:06<00:15, 206MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  35% 1.73G/4.98G [00:06<00:14, 223MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  35% 1.76G/4.98G [00:06<00:13, 240MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  36% 1.79G/4.98G [00:06<00:12, 250MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  37% 1.82G/4.98G [00:07<00:12, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  37% 1.86G/4.98G [00:07<00:11, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  38% 1.89G/4.98G [00:07<00:11, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  39% 1.92G/4.98G [00:07<00:11, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  39% 1.95G/4.98G [00:07<00:11, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  40% 1.98G/4.98G [00:07<00:10, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  40% 2.01G/4.98G [00:07<00:11, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  41% 2.04G/4.98G [00:07<00:11, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  42% 2.08G/4.98G [00:07<00:11, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  42% 2.11G/4.98G [00:08<00:10, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  43% 2.14G/4.98G [00:08<00:10, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  44% 2.17G/4.98G [00:08<00:10, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  44% 2.20G/4.98G [00:08<00:10, 270MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  45% 2.23G/4.98G [00:08<00:10, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  45% 2.26G/4.98G [00:08<00:09, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  46% 2.30G/4.98G [00:08<00:10, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  47% 2.33G/4.98G [00:08<00:10, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  47% 2.36G/4.98G [00:09<00:09, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  48% 2.39G/4.98G [00:09<00:09, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.42G/4.98G [00:09<00:09, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  49% 2.45G/4.98G [00:09<00:09, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  50% 2.49G/4.98G [00:09<00:09, 259MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.52G/4.98G [00:09<00:09, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  51% 2.55G/4.98G [00:09<00:09, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.58G/4.98G [00:09<00:08, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  52% 2.61G/4.98G [00:09<00:08, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  53% 2.64G/4.98G [00:10<00:08, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  54% 2.67G/4.98G [00:10<00:08, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  54% 2.71G/4.98G [00:10<00:08, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  55% 2.74G/4.98G [00:10<00:08, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  56% 2.77G/4.98G [00:10<00:08, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  56% 2.80G/4.98G [00:10<00:08, 258MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  57% 2.83G/4.98G [00:10<00:08, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  57% 2.86G/4.98G [00:10<00:08, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  58% 2.89G/4.98G [00:11<00:07, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  59% 2.93G/4.98G [00:11<00:07, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  59% 2.96G/4.98G [00:11<00:07, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  60% 2.99G/4.98G [00:11<00:07, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  61% 3.02G/4.98G [00:11<00:07, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  61% 3.05G/4.98G [00:11<00:07, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  62% 3.08G/4.98G [00:11<00:07, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  62% 3.11G/4.98G [00:11<00:07, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  63% 3.15G/4.98G [00:11<00:06, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  64% 3.18G/4.98G [00:12<00:06, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  64% 3.21G/4.98G [00:12<00:06, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  65% 3.24G/4.98G [00:12<00:06, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  66% 3.27G/4.98G [00:12<00:06, 271MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  66% 3.30G/4.98G [00:12<00:06, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  67% 3.33G/4.98G [00:12<00:06, 256MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.37G/4.98G [00:12<00:06, 258MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  68% 3.40G/4.98G [00:12<00:06, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  69% 3.43G/4.98G [00:13<00:05, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  69% 3.46G/4.98G [00:13<00:05, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  70% 3.49G/4.98G [00:13<00:05, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  71% 3.52G/4.98G [00:13<00:05, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  71% 3.55G/4.98G [00:13<00:05, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  72% 3.59G/4.98G [00:13<00:05, 276MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  73% 3.62G/4.98G [00:13<00:05, 273MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  73% 3.65G/4.98G [00:13<00:05, 254MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.68G/4.98G [00:14<00:04, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  74% 3.71G/4.98G [00:14<00:04, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  75% 3.74G/4.98G [00:14<00:04, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  76% 3.77G/4.98G [00:14<00:04, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  76% 3.81G/4.98G [00:14<00:04, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  77% 3.84G/4.98G [00:14<00:04, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  78% 3.87G/4.98G [00:14<00:04, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  78% 3.90G/4.98G [00:14<00:04, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  79% 3.93G/4.98G [00:14<00:03, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  80% 3.96G/4.98G [00:15<00:03, 257MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  80% 4.00G/4.98G [00:15<00:03, 259MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  81% 4.03G/4.98G [00:15<00:03, 264MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  81% 4.06G/4.98G [00:15<00:03, 260MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  82% 4.09G/4.98G [00:15<00:03, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  83% 4.12G/4.98G [00:15<00:03, 269MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  83% 4.15G/4.98G [00:15<00:03, 239MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  84% 4.18G/4.98G [00:15<00:03, 254MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  85% 4.22G/4.98G [00:16<00:03, 249MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  85% 4.25G/4.98G [00:16<00:02, 253MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  86% 4.28G/4.98G [00:16<00:02, 261MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  86% 4.31G/4.98G [00:16<00:02, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  87% 4.34G/4.98G [00:16<00:02, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  88% 4.37G/4.98G [00:16<00:02, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  88% 4.40G/4.98G [00:16<00:02, 268MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  89% 4.44G/4.98G [00:16<00:02, 272MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  90% 4.47G/4.98G [00:16<00:01, 275MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  90% 4.50G/4.98G [00:17<00:01, 278MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  91% 4.53G/4.98G [00:17<00:01, 280MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  92% 4.56G/4.98G [00:17<00:01, 283MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  92% 4.59G/4.98G [00:17<00:01, 280MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  93% 4.62G/4.98G [00:17<00:01, 274MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  93% 4.66G/4.98G [00:17<00:01, 265MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  94% 4.69G/4.98G [00:17<00:01, 266MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  95% 4.72G/4.98G [00:17<00:00, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  95% 4.75G/4.98G [00:18<00:00, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  96% 4.78G/4.98G [00:18<00:00, 258MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.81G/4.98G [00:18<00:00, 262MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  97% 4.84G/4.98G [00:18<00:00, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  98% 4.88G/4.98G [00:18<00:00, 267MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  98% 4.91G/4.98G [00:18<00:00, 270MB/s]\u001b[A\n","model-00003-of-00004.safetensors:  99% 4.94G/4.98G [00:18<00:00, 263MB/s]\u001b[A\n","model-00003-of-00004.safetensors: 100% 4.98G/4.98G [00:18<00:00, 263MB/s]\n","Downloading shards:  75% 3/4 [00:57<00:19, 19.21s/it]\n","model-00004-of-00004.safetensors:   0% 0.00/2.11G [00:00<?, ?B/s]\u001b[A\n","model-00004-of-00004.safetensors:   1% 31.5M/2.11G [00:00<00:07, 272MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   3% 62.9M/2.11G [00:00<00:07, 275MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   4% 94.4M/2.11G [00:00<00:07, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   6% 126M/2.11G [00:00<00:07, 273MB/s] \u001b[A\n","model-00004-of-00004.safetensors:   7% 157M/2.11G [00:00<00:07, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:   9% 189M/2.11G [00:00<00:07, 275MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  10% 220M/2.11G [00:00<00:06, 277MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  12% 252M/2.11G [00:00<00:06, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  13% 283M/2.11G [00:01<00:06, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  15% 315M/2.11G [00:01<00:07, 253MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  16% 346M/2.11G [00:01<00:06, 260MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  18% 377M/2.11G [00:01<00:06, 268MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  19% 409M/2.11G [00:01<00:06, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  21% 440M/2.11G [00:01<00:09, 173MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  22% 472M/2.11G [00:01<00:08, 194MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  24% 503M/2.11G [00:02<00:07, 213MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  25% 535M/2.11G [00:02<00:06, 230MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  27% 566M/2.11G [00:02<00:06, 241MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  28% 598M/2.11G [00:02<00:06, 248MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  30% 629M/2.11G [00:02<00:05, 259MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  31% 661M/2.11G [00:02<00:05, 259MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  33% 692M/2.11G [00:02<00:05, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  34% 724M/2.11G [00:02<00:05, 271MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  36% 755M/2.11G [00:03<00:05, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  37% 786M/2.11G [00:03<00:04, 274MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  39% 818M/2.11G [00:03<00:04, 274MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  40% 849M/2.11G [00:03<00:04, 278MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  42% 881M/2.11G [00:03<00:04, 264MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  43% 912M/2.11G [00:03<00:04, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  45% 944M/2.11G [00:03<00:04, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  46% 975M/2.11G [00:03<00:04, 275MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  48% 1.01G/2.11G [00:03<00:04, 275MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  49% 1.04G/2.11G [00:04<00:03, 277MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  51% 1.07G/2.11G [00:04<00:03, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  52% 1.10G/2.11G [00:04<00:03, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  54% 1.13G/2.11G [00:04<00:03, 273MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  55% 1.16G/2.11G [00:04<00:03, 273MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  57% 1.20G/2.11G [00:04<00:03, 272MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  58% 1.23G/2.11G [00:04<00:03, 276MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  60% 1.26G/2.11G [00:04<00:03, 279MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  61% 1.29G/2.11G [00:04<00:02, 278MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  62% 1.32G/2.11G [00:05<00:03, 261MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  64% 1.35G/2.11G [00:05<00:02, 264MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  65% 1.38G/2.11G [00:05<00:02, 260MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  67% 1.42G/2.11G [00:05<00:02, 263MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  68% 1.45G/2.11G [00:05<00:02, 264MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  70% 1.48G/2.11G [00:05<00:02, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  71% 1.51G/2.11G [00:05<00:02, 271MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  73% 1.54G/2.11G [00:05<00:02, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  74% 1.57G/2.11G [00:06<00:02, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  76% 1.60G/2.11G [00:06<00:01, 268MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  77% 1.64G/2.11G [00:06<00:01, 271MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  79% 1.67G/2.11G [00:06<00:01, 269MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  80% 1.70G/2.11G [00:06<00:01, 264MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  82% 1.73G/2.11G [00:06<00:01, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  83% 1.76G/2.11G [00:06<00:01, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  85% 1.79G/2.11G [00:06<00:01, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  86% 1.82G/2.11G [00:06<00:01, 261MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  88% 1.86G/2.11G [00:07<00:00, 265MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  89% 1.89G/2.11G [00:07<00:00, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  91% 1.92G/2.11G [00:07<00:00, 268MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  92% 1.95G/2.11G [00:07<00:00, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  94% 1.98G/2.11G [00:07<00:00, 265MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  95% 2.01G/2.11G [00:07<00:00, 267MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  97% 2.04G/2.11G [00:07<00:00, 270MB/s]\u001b[A\n","model-00004-of-00004.safetensors:  98% 2.08G/2.11G [00:07<00:00, 272MB/s]\u001b[A\n","model-00004-of-00004.safetensors: 100% 2.11G/2.11G [00:08<00:00, 263MB/s]\n","Downloading shards: 100% 4/4 [01:05<00:00, 16.43s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:03<00:00,  1.27it/s]\n","generation_config.json: 100% 137/137 [00:00<00:00, 987kB/s]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","Generating train split: 80 examples [00:00, 4115.39 examples/s]\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 80/80 [00:00<00:00, 2217.73 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","  0% 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 39.8267, 'lr': 0.0, 'epoch': 0}\n","  0% 0/800 [00:01<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 39.8267, 'grad_norm': 8.36042594909668, 'learning_rate': 2e-07, 'epoch': 0.03}\n","{'loss': 37.9414, 'lr': 2e-07, 'epoch': 0.03}\n","{'loss': 37.6461, 'lr': 4e-07, 'epoch': 0.05}\n","{'loss': 37.8296, 'lr': 6e-07, 'epoch': 0.07}\n","{'loss': 38.8848, 'lr': 8e-07, 'epoch': 0.1}\n","{'loss': 37.2718, 'lr': 1e-06, 'epoch': 0.12}\n","{'loss': 37.0521, 'lr': 9.99996096043844e-07, 'epoch': 0.15}\n","{'loss': 40.2852, 'lr': 9.9998438423634e-07, 'epoch': 0.17}\n","{'loss': 38.1302, 'grad_norm': 8.051902770996094, 'learning_rate': 9.999648647603774e-07, 'epoch': 0.2}\n","{'loss': 40.5749, 'lr': 9.999648647603774e-07, 'epoch': 0.2}\n","{'loss': 41.3845, 'lr': 9.99937537920769e-07, 'epoch': 0.23}\n","{'loss': 38.169, 'lr': 9.999024041442455e-07, 'epoch': 0.25}\n","{'loss': 36.7288, 'lr': 9.9985946397945e-07, 'epoch': 0.28}\n","{'loss': 39.0949, 'lr': 9.99808718096929e-07, 'epoch': 0.3}\n","{'loss': 36.1635, 'lr': 9.997501672891206e-07, 'epoch': 0.33}\n","{'loss': 36.277, 'lr': 9.996838124703446e-07, 'epoch': 0.35}\n","{'loss': 38.3768, 'lr': 9.996096546767859e-07, 'epoch': 0.38}\n","{'loss': 38.3462, 'grad_norm': 8.658787727355957, 'learning_rate': 9.995276950664795e-07, 'epoch': 0.4}\n","{'loss': 38.5154, 'lr': 9.995276950664795e-07, 'epoch': 0.4}\n","{'loss': 41.0441, 'lr': 9.994379349192926e-07, 'epoch': 0.42}\n","{'loss': 37.211, 'lr': 9.993403756369037e-07, 'epoch': 0.45}\n","{'loss': 39.4367, 'lr': 9.992350187427815e-07, 'epoch': 0.47}\n","{'loss': 38.1542, 'lr': 9.991218658821608e-07, 'epoch': 0.5}\n","{'loss': 37.1114, 'lr': 9.990009188220165e-07, 'epoch': 0.53}\n","{'loss': 37.1914, 'lr': 9.988721794510373e-07, 'epoch': 0.55}\n","{'loss': 40.0649, 'lr': 9.987356497795942e-07, 'epoch': 0.57}\n","{'loss': 38.5911, 'grad_norm': 8.278231620788574, 'learning_rate': 9.985913319397107e-07, 'epoch': 0.6}\n","{'loss': 37.2212, 'lr': 9.985913319397107e-07, 'epoch': 0.6}\n","{'loss': 39.6145, 'lr': 9.98439228185029e-07, 'epoch': 0.62}\n","{'loss': 40.2718, 'lr': 9.982793408907746e-07, 'epoch': 0.65}\n","{'loss': 39.43, 'lr': 9.981116725537192e-07, 'epoch': 0.68}\n","{'loss': 37.3025, 'lr': 9.979362257921427e-07, 'epoch': 0.7}\n","{'loss': 39.6635, 'lr': 9.977530033457903e-07, 'epoch': 0.72}\n","{'loss': 38.3905, 'lr': 9.97562008075832e-07, 'epoch': 0.75}\n","{'loss': 39.1416, 'lr': 9.973632429648163e-07, 'epoch': 0.78}\n","{'loss': 38.8794, 'grad_norm': 8.253862380981445, 'learning_rate': 9.971567111166245e-07, 'epoch': 0.8}\n","{'loss': 37.6529, 'lr': 9.971567111166245e-07, 'epoch': 0.8}\n","{'loss': 38.9835, 'lr': 9.969424157564215e-07, 'epoch': 0.82}\n","{'loss': 40.2549, 'lr': 9.96720360230606e-07, 'epoch': 0.85}\n","{'loss': 38.1453, 'lr': 9.964905480067584e-07, 'epoch': 0.88}\n","{'loss': 39.1683, 'lr': 9.96252982673586e-07, 'epoch': 0.9}\n","{'loss': 35.1591, 'lr': 9.960076679408673e-07, 'epoch': 0.93}\n","{'loss': 38.1144, 'lr': 9.957546076393943e-07, 'epoch': 0.95}\n","{'loss': 39.155, 'lr': 9.95493805720912e-07, 'epoch': 0.97}\n","{'loss': 38.3292, 'grad_norm': 8.372920036315918, 'learning_rate': 9.952252662580579e-07, 'epoch': 1.0}\n","  5% 40/800 [00:11<02:55,  4.34it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 37.192, 'lr': 9.952252662580579e-07, 'epoch': 1.0}\n","  5% 40/800 [00:12<02:55,  4.34it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 35.8968, 'lr': 9.949489934442965e-07, 'epoch': 1.02}\n","{'loss': 40.2839, 'lr': 9.94664991593856e-07, 'epoch': 1.05}\n","{'loss': 40.3181, 'lr': 9.943732651416596e-07, 'epoch': 1.07}\n","{'loss': 38.6641, 'lr': 9.940738186432563e-07, 'epoch': 1.1}\n","{'loss': 39.1033, 'lr': 9.9376665677475e-07, 'epoch': 1.12}\n","{'loss': 39.108, 'lr': 9.934517843327267e-07, 'epoch': 1.15}\n","{'loss': 40.2285, 'lr': 9.931292062341791e-07, 'epoch': 1.18}\n","{'loss': 38.8493, 'grad_norm': 8.724413871765137, 'learning_rate': 9.927989275164303e-07, 'epoch': 1.2}\n","{'loss': 38.8488, 'lr': 9.927989275164303e-07, 'epoch': 1.2}\n","{'loss': 37.108, 'lr': 9.92460953337055e-07, 'epoch': 1.23}\n","{'loss': 38.5996, 'lr': 9.921152889737984e-07, 'epoch': 1.25}\n","{'loss': 36.4889, 'lr': 9.917619398244948e-07, 'epoch': 1.27}\n","{'loss': 39.4444, 'lr': 9.914009114069821e-07, 'epoch': 1.3}\n","{'loss': 37.5953, 'lr': 9.910322093590176e-07, 'epoch': 1.32}\n","{'loss': 36.7487, 'lr': 9.90655839438187e-07, 'epoch': 1.35}\n","{'loss': 37.9374, 'lr': 9.902718075218176e-07, 'epoch': 1.38}\n","{'loss': 37.8464, 'grad_norm': 8.957743644714355, 'learning_rate': 9.898801196068837e-07, 'epoch': 1.4}\n","{'loss': 38.2697, 'lr': 9.898801196068837e-07, 'epoch': 1.4}\n","{'loss': 35.6826, 'lr': 9.894807818099158e-07, 'epoch': 1.43}\n","{'loss': 38.8389, 'lr': 9.890738003669027e-07, 'epoch': 1.45}\n","{'loss': 36.2809, 'lr': 9.886591816331953e-07, 'epoch': 1.48}\n","{'loss': 36.4945, 'lr': 9.882369320834068e-07, 'epoch': 1.5}\n","{'loss': 36.0137, 'lr': 9.878070583113122e-07, 'epoch': 1.52}\n","{'loss': 38.0461, 'lr': 9.87369567029745e-07, 'epoch': 1.55}\n","{'loss': 38.2143, 'lr': 9.869244650704921e-07, 'epoch': 1.57}\n","{'loss': 37.2301, 'grad_norm': 8.633685111999512, 'learning_rate': 9.864717593841883e-07, 'epoch': 1.6}\n","{'loss': 37.7892, 'lr': 9.864717593841883e-07, 'epoch': 1.6}\n","{'loss': 40.2079, 'lr': 9.860114570402054e-07, 'epoch': 1.62}\n","{'loss': 39.8701, 'lr': 9.855435652265446e-07, 'epoch': 1.65}\n","{'loss': 39.913, 'lr': 9.85068091249722e-07, 'epoch': 1.68}\n","{'loss': 37.3874, 'lr': 9.845850425346561e-07, 'epoch': 1.7}\n","{'loss': 37.0727, 'lr': 9.84094426624551e-07, 'epoch': 1.73}\n","{'loss': 40.486, 'lr': 9.835962511807785e-07, 'epoch': 1.75}\n","{'loss': 37.2982, 'lr': 9.830905239827592e-07, 'epoch': 1.77}\n","{'loss': 38.7531, 'grad_norm': 8.582254409790039, 'learning_rate': 9.8257725292784e-07, 'epoch': 1.8}\n","{'loss': 40.1783, 'lr': 9.8257725292784e-07, 'epoch': 1.8}\n","{'loss': 36.9597, 'lr': 9.820564460311717e-07, 'epoch': 1.82}\n","{'loss': 40.6705, 'lr': 9.81528111425584e-07, 'epoch': 1.85}\n","{'loss': 35.7021, 'lr': 9.809922573614569e-07, 'epoch': 1.88}\n","{'loss': 40.0842, 'lr': 9.804488922065937e-07, 'epoch': 1.9}\n","{'loss': 39.3569, 'lr': 9.798980244460892e-07, 'epoch': 1.93}\n","{'loss': 40.108, 'lr': 9.79339662682198e-07, 'epoch': 1.95}\n","{'loss': 42.1002, 'lr': 9.787738156341992e-07, 'epoch': 1.98}\n","{'loss': 39.395, 'grad_norm': 9.440959930419922, 'learning_rate': 9.78200492138261e-07, 'epoch': 2.0}\n"," 10% 80/800 [00:21<03:02,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 40.05, 'lr': 9.78200492138261e-07, 'epoch': 2.0}\n"," 10% 80/800 [00:23<03:02,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 38.0562, 'lr': 9.776197011473032e-07, 'epoch': 2.02}\n","{'loss': 36.2774, 'lr': 9.770314517308554e-07, 'epoch': 2.05}\n","{'loss': 37.8479, 'lr': 9.764357530749176e-07, 'epoch': 2.08}\n","{'loss': 35.6823, 'lr': 9.758326144818154e-07, 'epoch': 2.1}\n","{'loss': 41.3338, 'lr': 9.752220453700554e-07, 'epoch': 2.12}\n","{'loss': 37.7674, 'lr': 9.74604055274178e-07, 'epoch': 2.15}\n","{'loss': 37.0442, 'lr': 9.739786538446075e-07, 'epoch': 2.17}\n","{'loss': 38.0074, 'grad_norm': 7.718472480773926, 'learning_rate': 9.733458508475036e-07, 'epoch': 2.2}\n","{'loss': 40.2193, 'lr': 9.733458508475036e-07, 'epoch': 2.2}\n","{'loss': 36.1015, 'lr': 9.727056561646065e-07, 'epoch': 2.23}\n","{'loss': 35.8197, 'lr': 9.720580797930844e-07, 'epoch': 2.25}\n","{'loss': 36.3763, 'lr': 9.714031318453763e-07, 'epoch': 2.27}\n","{'loss': 39.3029, 'lr': 9.707408225490342e-07, 'epoch': 2.3}\n","{'loss': 37.8026, 'lr': 9.700711622465643e-07, 'epoch': 2.33}\n","{'loss': 38.6411, 'lr': 9.69394161395264e-07, 'epoch': 2.35}\n","{'loss': 38.4567, 'lr': 9.687098305670604e-07, 'epoch': 2.38}\n","{'loss': 37.84, 'grad_norm': 8.916491508483887, 'learning_rate': 9.680181804483434e-07, 'epoch': 2.4}\n","{'loss': 38.4337, 'lr': 9.680181804483434e-07, 'epoch': 2.4}\n","{'loss': 37.4148, 'lr': 9.673192218398e-07, 'epoch': 2.42}\n","{'loss': 39.0724, 'lr': 9.66612965656245e-07, 'epoch': 2.45}\n","{'loss': 37.7461, 'lr': 9.658994229264513e-07, 'epoch': 2.48}\n","{'loss': 40.2842, 'lr': 9.651786047929772e-07, 'epoch': 2.5}\n","{'loss': 36.5102, 'lr': 9.644505225119922e-07, 'epoch': 2.52}\n","{'loss': 39.5213, 'lr': 9.637151874531013e-07, 'epoch': 2.55}\n","{'loss': 38.5496, 'lr': 9.629726110991679e-07, 'epoch': 2.58}\n","{'loss': 38.4415, 'grad_norm': 9.199149131774902, 'learning_rate': 9.622228050461342e-07, 'epoch': 2.6}\n","{'loss': 39.7156, 'lr': 9.622228050461342e-07, 'epoch': 2.6}\n","{'loss': 39.0076, 'lr': 9.6146578100284e-07, 'epoch': 2.62}\n","{'loss': 40.4098, 'lr': 9.6070155079084e-07, 'epoch': 2.65}\n","{'loss': 36.9965, 'lr': 9.599301263442193e-07, 'epoch': 2.67}\n","{'loss': 38.991, 'lr': 9.591515197094063e-07, 'epoch': 2.7}\n","{'loss': 36.2394, 'lr': 9.58365743044986e-07, 'epoch': 2.73}\n","{'loss': 36.2525, 'lr': 9.575728086215091e-07, 'epoch': 2.75}\n","{'loss': 36.6919, 'lr': 9.567727288213004e-07, 'epoch': 2.77}\n","{'loss': 38.038, 'grad_norm': 8.973401069641113, 'learning_rate': 9.559655161382655e-07, 'epoch': 2.8}\n","{'loss': 37.3069, 'lr': 9.559655161382655e-07, 'epoch': 2.8}\n","{'loss': 38.8489, 'lr': 9.551511831776965e-07, 'epoch': 2.83}\n","{'loss': 39.5334, 'lr': 9.543297426560738e-07, 'epoch': 2.85}\n","{'loss': 37.4349, 'lr': 9.535012074008686e-07, 'epoch': 2.88}\n","{'loss': 35.3663, 'lr': 9.526655903503422e-07, 'epoch': 2.9}\n","{'loss': 36.1224, 'lr': 9.518229045533437e-07, 'epoch': 2.92}\n","{'loss': 36.5868, 'lr': 9.509731631691069e-07, 'epoch': 2.95}\n","{'loss': 38.7732, 'lr': 9.501163794670444e-07, 'epoch': 2.98}\n","{'loss': 37.4966, 'grad_norm': 9.650946617126465, 'learning_rate': 9.492525668265399e-07, 'epoch': 3.0}\n"," 15% 120/800 [00:33<03:06,  3.64it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 36.871, 'lr': 9.492525668265399e-07, 'epoch': 3.0}\n"," 15% 120/800 [00:35<03:06,  3.64it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 40.1593, 'lr': 9.483817387367402e-07, 'epoch': 3.02}\n","{'loss': 38.8853, 'lr': 9.475039087963441e-07, 'epoch': 3.05}\n","{'loss': 37.3609, 'lr': 9.4661909071339e-07, 'epoch': 3.08}\n","{'loss': 39.1467, 'lr': 9.457272983050419e-07, 'epoch': 3.1}\n","{'loss': 38.7752, 'lr': 9.448285454973737e-07, 'epoch': 3.12}\n","{'loss': 36.7278, 'lr': 9.439228463251514e-07, 'epoch': 3.15}\n","{'loss': 35.46, 'lr': 9.430102149316146e-07, 'epoch': 3.17}\n","{'loss': 37.9233, 'grad_norm': 8.729132652282715, 'learning_rate': 9.420906655682551e-07, 'epoch': 3.2}\n","{'loss': 35.9098, 'lr': 9.420906655682551e-07, 'epoch': 3.2}\n","{'loss': 37.7104, 'lr': 9.411642125945944e-07, 'epoch': 3.23}\n","{'loss': 37.8923, 'lr': 9.402308704779598e-07, 'epoch': 3.25}\n","{'loss': 34.4457, 'lr': 9.392906537932581e-07, 'epoch': 3.27}\n","{'loss': 38.2246, 'lr': 9.383435772227479e-07, 'epoch': 3.3}\n","{'loss': 37.7695, 'lr': 9.373896555558111e-07, 'epoch': 3.33}\n","{'loss': 39.7861, 'lr': 9.364289036887212e-07, 'epoch': 3.35}\n","{'loss': 39.167, 'lr': 9.354613366244106e-07, 'epoch': 3.38}\n","{'loss': 37.6132, 'grad_norm': 8.937637329101562, 'learning_rate': 9.34486969472237e-07, 'epoch': 3.4}\n","{'loss': 38.4816, 'lr': 9.34486969472237e-07, 'epoch': 3.4}\n","{'loss': 38.6251, 'lr': 9.335058174477471e-07, 'epoch': 3.42}\n","{'loss': 34.7189, 'lr': 9.325178958724386e-07, 'epoch': 3.45}\n","{'loss': 40.0513, 'lr': 9.315232201735217e-07, 'epoch': 3.48}\n","{'loss': 36.6325, 'lr': 9.305218058836776e-07, 'epoch': 3.5}\n","{'loss': 37.766, 'lr': 9.295136686408165e-07, 'epoch': 3.52}\n","{'loss': 35.5024, 'lr': 9.284988241878325e-07, 'epoch': 3.55}\n","{'loss': 36.4631, 'lr': 9.274772883723586e-07, 'epoch': 3.58}\n","{'loss': 37.2801, 'grad_norm': 9.15047550201416, 'learning_rate': 9.26449077146519e-07, 'epoch': 3.6}\n","{'loss': 36.9036, 'lr': 9.26449077146519e-07, 'epoch': 3.6}\n","{'loss': 35.9249, 'lr': 9.2541420656668e-07, 'epoch': 3.62}\n","{'loss': 36.8419, 'lr': 9.24372692793199e-07, 'epoch': 3.65}\n","{'loss': 36.9172, 'lr': 9.233245520901722e-07, 'epoch': 3.67}\n","{'loss': 37.407, 'lr': 9.222698008251813e-07, 'epoch': 3.7}\n","{'loss': 37.6858, 'lr': 9.212084554690369e-07, 'epoch': 3.73}\n","{'loss': 38.2152, 'lr': 9.20140532595522e-07, 'epoch': 3.75}\n","{'loss': 37.6645, 'lr': 9.19066048881133e-07, 'epoch': 3.77}\n","{'loss': 37.195, 'grad_norm': 9.55728816986084, 'learning_rate': 9.179850211048192e-07, 'epoch': 3.8}\n","{'loss': 36.3972, 'lr': 9.179850211048192e-07, 'epoch': 3.8}\n","{'loss': 36.5808, 'lr': 9.168974661477204e-07, 'epoch': 3.83}\n","{'loss': 38.003, 'lr': 9.158034009929045e-07, 'epoch': 3.85}\n","{'loss': 35.9082, 'lr': 9.147028427251009e-07, 'epoch': 3.88}\n","{'loss': 36.1301, 'lr': 9.135958085304343e-07, 'epoch': 3.9}\n","{'loss': 34.8659, 'lr': 9.12482315696157e-07, 'epoch': 3.92}\n","{'loss': 37.1866, 'lr': 9.113623816103772e-07, 'epoch': 3.95}\n","{'loss': 39.0885, 'lr': 9.102360237617899e-07, 'epoch': 3.98}\n","{'loss': 36.77, 'grad_norm': 10.019708633422852, 'learning_rate': 9.091032597394012e-07, 'epoch': 4.0}\n"," 20% 160/800 [00:45<02:33,  4.18it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 37.2079, 'lr': 9.091032597394012e-07, 'epoch': 4.0}\n"," 20% 160/800 [00:46<02:33,  4.18it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 37.4864, 'lr': 9.079641072322554e-07, 'epoch': 4.03}\n","{'loss': 37.1055, 'lr': 9.068185840291587e-07, 'epoch': 4.05}\n","{'loss': 37.2026, 'lr': 9.056667080184002e-07, 'epoch': 4.08}\n","{'loss': 35.6454, 'lr': 9.045084971874737e-07, 'epoch': 4.1}\n","{'loss': 38.9961, 'lr': 9.033439696227965e-07, 'epoch': 4.12}\n","{'loss': 35.2445, 'lr': 9.021731435094267e-07, 'epoch': 4.15}\n","{'loss': 37.1782, 'lr': 9.009960371307796e-07, 'epoch': 4.17}\n","{'loss': 37.0083, 'grad_norm': 9.66624641418457, 'learning_rate': 8.998126688683421e-07, 'epoch': 4.2}\n","{'loss': 35.2316, 'lr': 8.998126688683421e-07, 'epoch': 4.2}\n","{'loss': 36.1526, 'lr': 8.986230572013854e-07, 'epoch': 4.22}\n","{'loss': 38.5001, 'lr': 8.974272207066767e-07, 'epoch': 4.25}\n","{'loss': 38.541, 'lr': 8.962251780581887e-07, 'epoch': 4.28}\n","{'loss': 35.9607, 'lr': 8.950169480268088e-07, 'epoch': 4.3}\n","{'loss': 39.0182, 'lr': 8.938025494800453e-07, 'epoch': 4.33}\n","{'loss': 37.7977, 'lr': 8.925820013817329e-07, 'epoch': 4.35}\n","{'loss': 38.7762, 'lr': 8.913553227917365e-07, 'epoch': 4.38}\n","{'loss': 37.4973, 'grad_norm': 9.539331436157227, 'learning_rate': 8.901225328656542e-07, 'epoch': 4.4}\n","{'loss': 37.1406, 'lr': 8.901225328656542e-07, 'epoch': 4.4}\n","{'loss': 34.8368, 'lr': 8.88883650854517e-07, 'epoch': 4.42}\n","{'loss': 36.302, 'lr': 8.876386961044891e-07, 'epoch': 4.45}\n","{'loss': 37.3641, 'lr': 8.863876880565655e-07, 'epoch': 4.47}\n","{'loss': 36.1449, 'lr': 8.851306462462688e-07, 'epoch': 4.5}\n","{'loss': 34.9479, 'lr': 8.83867590303343e-07, 'epoch': 4.53}\n","{'loss': 37.0874, 'lr': 8.825985399514486e-07, 'epoch': 4.55}\n","{'loss': 37.38, 'lr': 8.81323515007853e-07, 'epoch': 4.58}\n","{'loss': 36.4005, 'grad_norm': 9.55331802368164, 'learning_rate': 8.800425353831226e-07, 'epoch': 4.6}\n","{'loss': 36.6529, 'lr': 8.800425353831226e-07, 'epoch': 4.6}\n","{'loss': 34.1971, 'lr': 8.7875562108081e-07, 'epoch': 4.62}\n","{'loss': 38.5314, 'lr': 8.774627921971436e-07, 'epoch': 4.65}\n","{'loss': 35.6735, 'lr': 8.761640689207123e-07, 'epoch': 4.67}\n","{'loss': 38.2606, 'lr': 8.748594715321511e-07, 'epoch': 4.7}\n","{'loss': 40.3624, 'lr': 8.735490204038242e-07, 'epoch': 4.72}\n","{'loss': 37.2019, 'lr': 8.722327359995063e-07, 'epoch': 4.75}\n","{'loss': 34.3269, 'lr': 8.70910638874064e-07, 'epoch': 4.78}\n","{'loss': 36.9008, 'grad_norm': 8.792088508605957, 'learning_rate': 8.695827496731341e-07, 'epoch': 4.8}\n","{'loss': 35.2047, 'lr': 8.695827496731341e-07, 'epoch': 4.8}\n","{'loss': 36.3301, 'lr': 8.682490891328016e-07, 'epoch': 4.83}\n","{'loss': 37.0799, 'lr': 8.669096780792753e-07, 'epoch': 4.85}\n","{'loss': 35.8519, 'lr': 8.655645374285636e-07, 'epoch': 4.88}\n","{'loss': 37.8709, 'lr': 8.64213688186147e-07, 'epoch': 4.9}\n","{'loss': 37.0533, 'lr': 8.6285715144665e-07, 'epoch': 4.92}\n","{'loss': 37.4353, 'lr': 8.61494948393513e-07, 'epoch': 4.95}\n","{'loss': 35.5863, 'lr': 8.601271002986595e-07, 'epoch': 4.97}\n","{'loss': 36.5516, 'grad_norm': 9.462902069091797, 'learning_rate': 8.587536285221655e-07, 'epoch': 5.0}\n"," 25% 200/800 [00:55<02:20,  4.27it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 35.771, 'lr': 8.587536285221655e-07, 'epoch': 5.0}\n"," 25% 200/800 [00:57<02:20,  4.27it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 35.8282, 'lr': 8.573745545119256e-07, 'epoch': 5.03}\n","{'loss': 38.3334, 'lr': 8.559898998033177e-07, 'epoch': 5.05}\n","{'loss': 37.6952, 'lr': 8.545996860188667e-07, 'epoch': 5.08}\n","{'loss': 36.4243, 'lr': 8.532039348679072e-07, 'epoch': 5.1}\n","{'loss': 35.9825, 'lr': 8.518026681462447e-07, 'epoch': 5.12}\n","{'loss': 38.6888, 'lr': 8.503959077358143e-07, 'epoch': 5.15}\n","{'loss': 34.7954, 'lr': 8.4898367560434e-07, 'epoch': 5.17}\n","{'loss': 36.6898, 'grad_norm': 8.149863243103027, 'learning_rate': 8.47565993804991e-07, 'epoch': 5.2}\n","{'loss': 37.4867, 'lr': 8.47565993804991e-07, 'epoch': 5.2}\n","{'loss': 37.1497, 'lr': 8.461428844760379e-07, 'epoch': 5.22}\n","{'loss': 35.5551, 'lr': 8.447143698405059e-07, 'epoch': 5.25}\n","{'loss': 34.408, 'lr': 8.432804722058296e-07, 'epoch': 5.28}\n","{'loss': 34.8127, 'lr': 8.418412139635024e-07, 'epoch': 5.3}\n","{'loss': 34.3884, 'lr': 8.403966175887291e-07, 'epoch': 5.33}\n","{'loss': 37.391, 'lr': 8.389467056400732e-07, 'epoch': 5.35}\n","{'loss': 35.4977, 'lr': 8.374915007591052e-07, 'epoch': 5.38}\n","{'loss': 35.8362, 'grad_norm': 9.877924919128418, 'learning_rate': 8.360310256700496e-07, 'epoch': 5.4}\n","{'loss': 35.9785, 'lr': 8.360310256700496e-07, 'epoch': 5.4}\n","{'loss': 36.4871, 'lr': 8.34565303179429e-07, 'epoch': 5.42}\n","{'loss': 36.9099, 'lr': 8.330943561757091e-07, 'epoch': 5.45}\n","{'loss': 37.2472, 'lr': 8.3161820762894e-07, 'epoch': 5.47}\n","{'loss': 36.8762, 'lr': 8.301368805903986e-07, 'epoch': 5.5}\n","{'loss': 36.4304, 'lr': 8.286503981922283e-07, 'epoch': 5.53}\n","{'loss': 37.1269, 'lr': 8.271587836470774e-07, 'epoch': 5.55}\n","{'loss': 36.1951, 'lr': 8.256620602477371e-07, 'epoch': 5.58}\n","{'loss': 36.6564, 'grad_norm': 8.519927978515625, 'learning_rate': 8.241602513667773e-07, 'epoch': 5.6}\n","{'loss': 36.6204, 'lr': 8.241602513667773e-07, 'epoch': 5.6}\n","{'loss': 35.2208, 'lr': 8.226533804561826e-07, 'epoch': 5.62}\n","{'loss': 37.2522, 'lr': 8.211414710469844e-07, 'epoch': 5.65}\n","{'loss': 36.4957, 'lr': 8.196245467488949e-07, 'epoch': 5.67}\n","{'loss': 34.9811, 'lr': 8.181026312499383e-07, 'epoch': 5.7}\n","{'loss': 38.1503, 'lr': 8.165757483160798e-07, 'epoch': 5.72}\n","{'loss': 36.9992, 'lr': 8.150439217908556e-07, 'epoch': 5.75}\n","{'loss': 39.3605, 'lr': 8.135071755949999e-07, 'epoch': 5.78}\n","{'loss': 36.885, 'grad_norm': 11.039724349975586, 'learning_rate': 8.11965533726072e-07, 'epoch': 5.8}\n","{'loss': 35.0547, 'lr': 8.11965533726072e-07, 'epoch': 5.8}\n","{'loss': 37.3255, 'lr': 8.104190202580811e-07, 'epoch': 5.83}\n","{'loss': 33.2796, 'lr': 8.088676593411099e-07, 'epoch': 5.85}\n","{'loss': 36.1724, 'lr': 8.073114752009387e-07, 'epoch': 5.88}\n","{'loss': 37.1497, 'lr': 8.057504921386659e-07, 'epoch': 5.9}\n","{'loss': 38.3322, 'lr': 8.041847345303295e-07, 'epoch': 5.92}\n","{'loss': 39.7385, 'lr': 8.026142268265254e-07, 'epoch': 5.95}\n","{'loss': 31.9586, 'lr': 8.010389935520269e-07, 'epoch': 5.97}\n","{'loss': 36.1264, 'grad_norm': 7.927724838256836, 'learning_rate': 7.994590593054e-07, 'epoch': 6.0}\n"," 30% 240/800 [01:07<02:15,  4.14it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 35.7716, 'lr': 7.994590593054e-07, 'epoch': 6.0}\n"," 30% 240/800 [01:09<02:15,  4.14it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 35.7059, 'lr': 7.978744487586213e-07, 'epoch': 6.03}\n","{'loss': 37.4662, 'lr': 7.96285186656691e-07, 'epoch': 6.05}\n","{'loss': 35.8662, 'lr': 7.946912978172473e-07, 'epoch': 6.08}\n","{'loss': 37.5447, 'lr': 7.93092807130179e-07, 'epoch': 6.1}\n","{'loss': 34.3557, 'lr': 7.91489739557236e-07, 'epoch': 6.12}\n","{'loss': 34.1198, 'lr': 7.898821201316407e-07, 'epoch': 6.15}\n","{'loss': 37.5314, 'lr': 7.882699739576959e-07, 'epoch': 6.17}\n","{'loss': 36.0452, 'grad_norm': 10.92962646484375, 'learning_rate': 7.866533262103936e-07, 'epoch': 6.2}\n","{'loss': 33.6751, 'lr': 7.866533262103936e-07, 'epoch': 6.2}\n","{'loss': 37.6403, 'lr': 7.850322021350215e-07, 'epoch': 6.22}\n","{'loss': 37.0831, 'lr': 7.834066270467689e-07, 'epoch': 6.25}\n","{'loss': 34.7073, 'lr': 7.817766263303312e-07, 'epoch': 6.28}\n","{'loss': 35.0681, 'lr': 7.801422254395138e-07, 'epoch': 6.3}\n","{'loss': 35.7436, 'lr': 7.785034498968342e-07, 'epoch': 6.33}\n","{'loss': 35.4321, 'lr': 7.768603252931242e-07, 'epoch': 6.35}\n","{'loss': 33.7278, 'lr': 7.752128772871292e-07, 'epoch': 6.38}\n","{'loss': 35.3847, 'grad_norm': 9.38039779663086, 'learning_rate': 7.735611316051083e-07, 'epoch': 6.4}\n","{'loss': 37.4753, 'lr': 7.735611316051083e-07, 'epoch': 6.4}\n","{'loss': 35.5774, 'lr': 7.719051140404326e-07, 'epoch': 6.42}\n","{'loss': 35.5345, 'lr': 7.702448504531818e-07, 'epoch': 6.45}\n","{'loss': 33.9997, 'lr': 7.685803667697411e-07, 'epoch': 6.47}\n","{'loss': 35.1226, 'lr': 7.669116889823954e-07, 'epoch': 6.5}\n","{'loss': 36.8823, 'lr': 7.652388431489248e-07, 'epoch': 6.53}\n","{'loss': 36.0989, 'lr': 7.63561855392196e-07, 'epoch': 6.55}\n","{'loss': 38.1463, 'lr': 7.618807518997563e-07, 'epoch': 6.58}\n","{'loss': 36.1046, 'grad_norm': 11.21712589263916, 'learning_rate': 7.601955589234225e-07, 'epoch': 6.6}\n","{'loss': 32.6941, 'lr': 7.601955589234225e-07, 'epoch': 6.6}\n","{'loss': 34.1845, 'lr': 7.58506302778873e-07, 'epoch': 6.62}\n","{'loss': 35.3617, 'lr': 7.568130098452351e-07, 'epoch': 6.65}\n","{'loss': 39.0669, 'lr': 7.551157065646746e-07, 'epoch': 6.67}\n","{'loss': 36.5347, 'lr': 7.534144194419815e-07, 'epoch': 6.7}\n","{'loss': 34.3344, 'lr': 7.517091750441575e-07, 'epoch': 6.72}\n","{'loss': 37.9843, 'lr': 7.5e-07, 'epoch': 6.75}\n","{'loss': 35.4759, 'lr': 7.482869209996866e-07, 'epoch': 6.78}\n","{'loss': 35.7046, 'grad_norm': 10.476127624511719, 'learning_rate': 7.465699647943585e-07, 'epoch': 6.8}\n","{'loss': 36.9191, 'lr': 7.465699647943585e-07, 'epoch': 6.8}\n","{'loss': 35.5954, 'lr': 7.448491581957029e-07, 'epoch': 6.83}\n","{'loss': 34.4725, 'lr': 7.431245280755336e-07, 'epoch': 6.85}\n","{'loss': 35.8605, 'lr': 7.413961013653725e-07, 'epoch': 6.88}\n","{'loss': 35.1684, 'lr': 7.396639050560274e-07, 'epoch': 6.9}\n","{'loss': 37.2052, 'lr': 7.379279661971727e-07, 'epoch': 6.92}\n","{'loss': 34.6084, 'lr': 7.361883118969247e-07, 'epoch': 6.95}\n","{'loss': 34.8552, 'lr': 7.3444496932142e-07, 'epoch': 6.97}\n","{'loss': 35.5856, 'grad_norm': 10.172587394714355, 'learning_rate': 7.326979656943905e-07, 'epoch': 7.0}\n"," 35% 280/800 [01:18<02:15,  3.84it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 36.2766, 'lr': 7.326979656943905e-07, 'epoch': 7.0}\n"," 35% 280/800 [01:20<02:15,  3.84it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.2662, 'lr': 7.309473282967386e-07, 'epoch': 7.03}\n","{'loss': 37.2408, 'lr': 7.291930844661108e-07, 'epoch': 7.05}\n","{'loss': 35.7065, 'lr': 7.274352615964712e-07, 'epoch': 7.08}\n","{'loss': 33.7555, 'lr': 7.256738871376732e-07, 'epoch': 7.1}\n","{'loss': 35.7545, 'lr': 7.239089885950316e-07, 'epoch': 7.12}\n","{'loss': 35.1393, 'lr': 7.221405935288925e-07, 'epoch': 7.15}\n","{'loss': 36.0303, 'lr': 7.203687295542032e-07, 'epoch': 7.17}\n","{'loss': 35.5212, 'grad_norm': 9.998859405517578, 'learning_rate': 7.185934243400805e-07, 'epoch': 7.2}\n","{'loss': 36.8695, 'lr': 7.185934243400805e-07, 'epoch': 7.2}\n","{'loss': 36.5437, 'lr': 7.168147056093796e-07, 'epoch': 7.22}\n","{'loss': 35.422, 'lr': 7.150326011382603e-07, 'epoch': 7.25}\n","{'loss': 35.9781, 'lr': 7.132471387557532e-07, 'epoch': 7.28}\n","{'loss': 33.911, 'lr': 7.114583463433258e-07, 'epoch': 7.3}\n","{'loss': 35.5073, 'lr': 7.096662518344468e-07, 'epoch': 7.33}\n","{'loss': 38.4174, 'lr': 7.078708832141497e-07, 'epoch': 7.35}\n","{'loss': 34.7878, 'lr': 7.06072268518596e-07, 'epoch': 7.38}\n","{'loss': 35.9296, 'grad_norm': 9.35323429107666, 'learning_rate': 7.042704358346374e-07, 'epoch': 7.4}\n","{'loss': 36.1797, 'lr': 7.042704358346374e-07, 'epoch': 7.4}\n","{'loss': 36.2415, 'lr': 7.024654132993772e-07, 'epoch': 7.42}\n","{'loss': 34.8044, 'lr': 7.006572290997304e-07, 'epoch': 7.45}\n","{'loss': 36.4994, 'lr': 6.988459114719848e-07, 'epoch': 7.47}\n","{'loss': 35.4921, 'lr': 6.970314887013585e-07, 'epoch': 7.5}\n","{'loss': 36.036, 'lr': 6.952139891215592e-07, 'epoch': 7.53}\n","{'loss': 36.8347, 'lr': 6.933934411143419e-07, 'epoch': 7.55}\n","{'loss': 36.3597, 'lr': 6.915698731090648e-07, 'epoch': 7.58}\n","{'loss': 36.0559, 'grad_norm': 10.205085754394531, 'learning_rate': 6.89743313582246e-07, 'epoch': 7.6}\n","{'loss': 37.6027, 'lr': 6.89743313582246e-07, 'epoch': 7.6}\n","{'loss': 34.5092, 'lr': 6.87913791057119e-07, 'epoch': 7.62}\n","{'loss': 33.8076, 'lr': 6.860813341031865e-07, 'epoch': 7.65}\n","{'loss': 34.0172, 'lr': 6.842459713357751e-07, 'epoch': 7.67}\n","{'loss': 35.0107, 'lr': 6.824077314155876e-07, 'epoch': 7.7}\n","{'loss': 32.3776, 'lr': 6.805666430482564e-07, 'epoch': 7.72}\n","{'loss': 33.6565, 'lr': 6.787227349838946e-07, 'epoch': 7.75}\n","{'loss': 34.407, 'lr': 6.76876036016647e-07, 'epoch': 7.78}\n","{'loss': 34.4236, 'grad_norm': 10.075678825378418, 'learning_rate': 6.750265749842409e-07, 'epoch': 7.8}\n","{'loss': 34.6562, 'lr': 6.750265749842409e-07, 'epoch': 7.8}\n","{'loss': 35.1545, 'lr': 6.731743807675354e-07, 'epoch': 7.83}\n","{'loss': 34.7494, 'lr': 6.713194822900706e-07, 'epoch': 7.85}\n","{'loss': 33.9597, 'lr': 6.694619085176159e-07, 'epoch': 7.88}\n","{'loss': 35.7839, 'lr': 6.676016884577173e-07, 'epoch': 7.9}\n","{'loss': 34.3542, 'lr': 6.657388511592452e-07, 'epoch': 7.92}\n","{'loss': 35.1877, 'lr': 6.6387342571194e-07, 'epoch': 7.95}\n","{'loss': 32.7425, 'lr': 6.620054412459587e-07, 'epoch': 7.97}\n","{'loss': 34.5735, 'grad_norm': 8.83890438079834, 'learning_rate': 6.601349269314187e-07, 'epoch': 8.0}\n"," 40% 320/800 [01:29<02:03,  3.89it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 34.253, 'lr': 6.601349269314187e-07, 'epoch': 8.0}\n"," 40% 320/800 [01:31<02:03,  3.89it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.4121, 'lr': 6.582619119779438e-07, 'epoch': 8.03}\n","{'loss': 34.3771, 'lr': 6.563864256342069e-07, 'epoch': 8.05}\n","{'loss': 35.0946, 'lr': 6.545084971874736e-07, 'epoch': 8.07}\n","{'loss': 35.244, 'lr': 6.526281559631457e-07, 'epoch': 8.1}\n","{'loss': 34.5577, 'lr': 6.507454313243015e-07, 'epoch': 8.12}\n","{'loss': 33.6348, 'lr': 6.48860352671239e-07, 'epoch': 8.15}\n","{'loss': 36.6076, 'lr': 6.469729494410158e-07, 'epoch': 8.18}\n","{'loss': 34.7726, 'grad_norm': 11.82612419128418, 'learning_rate': 6.450832511069897e-07, 'epoch': 8.2}\n","{'loss': 35.6811, 'lr': 6.450832511069897e-07, 'epoch': 8.2}\n","{'loss': 32.8394, 'lr': 6.431912871783586e-07, 'epoch': 8.22}\n","{'loss': 33.3165, 'lr': 6.412970871996995e-07, 'epoch': 8.25}\n","{'loss': 33.0158, 'lr': 6.394006807505067e-07, 'epoch': 8.28}\n","{'loss': 35.81, 'lr': 6.37502097444731e-07, 'epoch': 8.3}\n","{'loss': 35.0604, 'lr': 6.356013669303161e-07, 'epoch': 8.32}\n","{'loss': 35.3639, 'lr': 6.336985188887366e-07, 'epoch': 8.35}\n","{'loss': 34.7368, 'lr': 6.317935830345338e-07, 'epoch': 8.38}\n","{'loss': 34.478, 'grad_norm': 11.124263763427734, 'learning_rate': 6.298865891148517e-07, 'epoch': 8.4}\n","{'loss': 34.4399, 'lr': 6.298865891148517e-07, 'epoch': 8.4}\n","{'loss': 35.0574, 'lr': 6.279775669089733e-07, 'epoch': 8.43}\n","{'loss': 36.4236, 'lr': 6.260665462278543e-07, 'epoch': 8.45}\n","{'loss': 34.8596, 'lr': 6.241535569136583e-07, 'epoch': 8.47}\n","{'loss': 34.6961, 'lr': 6.222386288392914e-07, 'epoch': 8.5}\n","{'loss': 34.6455, 'lr': 6.203217919079342e-07, 'epoch': 8.53}\n","{'loss': 38.0057, 'lr': 6.184030760525762e-07, 'epoch': 8.55}\n","{'loss': 33.1132, 'lr': 6.164825112355477e-07, 'epoch': 8.57}\n","{'loss': 35.1551, 'grad_norm': 9.793229103088379, 'learning_rate': 6.145601274480521e-07, 'epoch': 8.6}\n","{'loss': 34.3816, 'lr': 6.145601274480521e-07, 'epoch': 8.6}\n","{'loss': 37.9004, 'lr': 6.126359547096974e-07, 'epoch': 8.62}\n","{'loss': 37.8424, 'lr': 6.107100230680278e-07, 'epoch': 8.65}\n","{'loss': 33.0923, 'lr': 6.087823625980539e-07, 'epoch': 8.68}\n","{'loss': 35.1563, 'lr': 6.068530034017835e-07, 'epoch': 8.7}\n","{'loss': 34.9009, 'lr': 6.049219756077513e-07, 'epoch': 8.72}\n","{'loss': 35.118, 'lr': 6.029893093705491e-07, 'epoch': 8.75}\n","{'loss': 36.1275, 'lr': 6.010550348703537e-07, 'epoch': 8.78}\n","{'loss': 35.5649, 'grad_norm': 11.178509712219238, 'learning_rate': 5.991191823124564e-07, 'epoch': 8.8}\n","{'loss': 33.0442, 'lr': 5.991191823124564e-07, 'epoch': 8.8}\n","{'loss': 33.5693, 'lr': 5.971817819267912e-07, 'epoch': 8.82}\n","{'loss': 35.7842, 'lr': 5.952428639674631e-07, 'epoch': 8.85}\n","{'loss': 34.1407, 'lr': 5.933024587122745e-07, 'epoch': 8.88}\n","{'loss': 36.6714, 'lr': 5.913605964622537e-07, 'epoch': 8.9}\n","{'loss': 33.0509, 'lr': 5.894173075411811e-07, 'epoch': 8.93}\n","{'loss': 34.5206, 'lr': 5.874726222951156e-07, 'epoch': 8.95}\n","{'loss': 36.2686, 'lr': 5.85526571091921e-07, 'epoch': 8.97}\n","{'loss': 34.6312, 'grad_norm': 10.76843547821045, 'learning_rate': 5.835791843207916e-07, 'epoch': 9.0}\n"," 45% 360/800 [01:40<01:49,  4.04it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 34.1071, 'lr': 5.835791843207916e-07, 'epoch': 9.0}\n"," 45% 360/800 [01:42<01:49,  4.04it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.5664, 'lr': 5.816304923917778e-07, 'epoch': 9.03}\n","{'loss': 33.4112, 'lr': 5.796805257353108e-07, 'epoch': 9.05}\n","{'loss': 32.5827, 'lr': 5.777293148017279e-07, 'epoch': 9.07}\n","{'loss': 36.6812, 'lr': 5.757768900607971e-07, 'epoch': 9.1}\n","{'loss': 34.6278, 'lr': 5.738232820012407e-07, 'epoch': 9.12}\n","{'loss': 33.3253, 'lr': 5.718685211302592e-07, 'epoch': 9.15}\n","{'loss': 34.4249, 'lr': 5.699126379730559e-07, 'epoch': 9.18}\n","{'loss': 34.2158, 'grad_norm': 9.744242668151855, 'learning_rate': 5.679556630723591e-07, 'epoch': 9.2}\n","{'loss': 35.9386, 'lr': 5.679556630723591e-07, 'epoch': 9.2}\n","{'loss': 34.8559, 'lr': 5.659976269879455e-07, 'epoch': 9.22}\n","{'loss': 34.3195, 'lr': 5.640385602961634e-07, 'epoch': 9.25}\n","{'loss': 33.963, 'lr': 5.620784935894547e-07, 'epoch': 9.28}\n","{'loss': 35.7461, 'lr': 5.60117457475877e-07, 'epoch': 9.3}\n","{'loss': 33.4018, 'lr': 5.581554825786266e-07, 'epoch': 9.32}\n","{'loss': 34.7997, 'lr': 5.561925995355595e-07, 'epoch': 9.35}\n","{'loss': 34.0494, 'lr': 5.542288389987128e-07, 'epoch': 9.38}\n","{'loss': 34.6342, 'grad_norm': 10.484657287597656, 'learning_rate': 5.522642316338268e-07, 'epoch': 9.4}\n","{'loss': 34.5588, 'lr': 5.522642316338268e-07, 'epoch': 9.4}\n","{'loss': 34.8133, 'lr': 5.502988081198654e-07, 'epoch': 9.43}\n","{'loss': 33.9529, 'lr': 5.483325991485378e-07, 'epoch': 9.45}\n","{'loss': 35.7383, 'lr': 5.463656354238183e-07, 'epoch': 9.47}\n","{'loss': 35.9264, 'lr': 5.443979476614674e-07, 'epoch': 9.5}\n","{'loss': 34.8571, 'lr': 5.424295665885522e-07, 'epoch': 9.53}\n","{'loss': 33.982, 'lr': 5.404605229429663e-07, 'epoch': 9.55}\n","{'loss': 33.7656, 'lr': 5.3849084747295e-07, 'epoch': 9.57}\n","{'loss': 34.6993, 'grad_norm': 10.563658714294434, 'learning_rate': 5.365205709366099e-07, 'epoch': 9.6}\n","{'loss': 36.3171, 'lr': 5.365205709366099e-07, 'epoch': 9.6}\n","{'loss': 35.6337, 'lr': 5.34549724101439e-07, 'epoch': 9.62}\n","{'loss': 35.005, 'lr': 5.325783377438356e-07, 'epoch': 9.65}\n","{'loss': 32.4755, 'lr': 5.306064426486237e-07, 'epoch': 9.68}\n","{'loss': 37.0414, 'lr': 5.286340696085709e-07, 'epoch': 9.7}\n","{'loss': 36.0692, 'lr': 5.266612494239088e-07, 'epoch': 9.72}\n","{'loss': 35.9469, 'lr': 5.246880129018515e-07, 'epoch': 9.75}\n","{'loss': 33.869, 'lr': 5.227143908561145e-07, 'epoch': 9.78}\n","{'loss': 35.2947, 'grad_norm': 11.2066011428833, 'learning_rate': 5.207404141064333e-07, 'epoch': 9.8}\n","{'loss': 35.5106, 'lr': 5.207404141064333e-07, 'epoch': 9.8}\n","{'loss': 36.7317, 'lr': 5.187661134780829e-07, 'epoch': 9.82}\n","{'loss': 32.9794, 'lr': 5.167915198013956e-07, 'epoch': 9.85}\n","{'loss': 32.9323, 'lr': 5.148166639112799e-07, 'epoch': 9.88}\n","{'loss': 33.8297, 'lr': 5.128415766467391e-07, 'epoch': 9.9}\n","{'loss': 34.5688, 'lr': 5.108662888503894e-07, 'epoch': 9.93}\n","{'loss': 35.8208, 'lr': 5.088908313679787e-07, 'epoch': 9.95}\n","{'loss': 35.2706, 'lr': 5.069152350479047e-07, 'epoch': 9.97}\n","{'loss': 34.7055, 'grad_norm': 11.863041877746582, 'learning_rate': 5.049395307407328e-07, 'epoch': 10.0}\n"," 50% 400/800 [01:51<01:35,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 35.1549, 'lr': 5.049395307407328e-07, 'epoch': 10.0}\n"," 50% 400/800 [01:53<01:35,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 31.6317, 'lr': 5.029637492987152e-07, 'epoch': 10.03}\n","{'loss': 35.2772, 'lr': 5.009879215753084e-07, 'epoch': 10.05}\n","{'loss': 33.9403, 'lr': 4.990120784246916e-07, 'epoch': 10.07}\n","{'loss': 34.0, 'lr': 4.970362507012848e-07, 'epoch': 10.1}\n","{'loss': 35.4778, 'lr': 4.950604692592673e-07, 'epoch': 10.12}\n","{'loss': 34.6477, 'lr': 4.930847649520954e-07, 'epoch': 10.15}\n","{'loss': 33.0697, 'lr': 4.911091686320213e-07, 'epoch': 10.18}\n","{'loss': 34.1499, 'grad_norm': 10.141073226928711, 'learning_rate': 4.891337111496107e-07, 'epoch': 10.2}\n","{'loss': 32.9059, 'lr': 4.891337111496107e-07, 'epoch': 10.2}\n","{'loss': 35.6812, 'lr': 4.871584233532609e-07, 'epoch': 10.22}\n","{'loss': 34.231, 'lr': 4.851833360887201e-07, 'epoch': 10.25}\n","{'loss': 33.9572, 'lr': 4.832084801986045e-07, 'epoch': 10.28}\n","{'loss': 35.523, 'lr': 4.812338865219171e-07, 'epoch': 10.3}\n","{'loss': 36.01, 'lr': 4.792595858935667e-07, 'epoch': 10.32}\n","{'loss': 32.3159, 'lr': 4.772856091438856e-07, 'epoch': 10.35}\n","{'loss': 34.2176, 'lr': 4.753119870981485e-07, 'epoch': 10.38}\n","{'loss': 34.3552, 'grad_norm': 10.098332405090332, 'learning_rate': 4.733387505760912e-07, 'epoch': 10.4}\n","{'loss': 34.6578, 'lr': 4.733387505760912e-07, 'epoch': 10.4}\n","{'loss': 34.5571, 'lr': 4.7136593039142914e-07, 'epoch': 10.43}\n","{'loss': 35.5879, 'lr': 4.6939355735137635e-07, 'epoch': 10.45}\n","{'loss': 34.6174, 'lr': 4.674216622561644e-07, 'epoch': 10.47}\n","{'loss': 33.3009, 'lr': 4.6545027589856105e-07, 'epoch': 10.5}\n","{'loss': 36.1938, 'lr': 4.634794290633901e-07, 'epoch': 10.53}\n","{'loss': 34.7547, 'lr': 4.6150915252705e-07, 'epoch': 10.55}\n","{'loss': 34.0737, 'lr': 4.5953947705703365e-07, 'epoch': 10.57}\n","{'loss': 34.7179, 'grad_norm': 10.999678611755371, 'learning_rate': 4.575704334114478e-07, 'epoch': 10.6}\n","{'loss': 34.6909, 'lr': 4.575704334114478e-07, 'epoch': 10.6}\n","{'loss': 34.1429, 'lr': 4.556020523385326e-07, 'epoch': 10.62}\n","{'loss': 33.566, 'lr': 4.536343645761817e-07, 'epoch': 10.65}\n","{'loss': 34.5484, 'lr': 4.5166740085146225e-07, 'epoch': 10.68}\n","{'loss': 34.6122, 'lr': 4.497011918801346e-07, 'epoch': 10.7}\n","{'loss': 34.4848, 'lr': 4.477357683661733e-07, 'epoch': 10.72}\n","{'loss': 34.0822, 'lr': 4.457711610012873e-07, 'epoch': 10.75}\n","{'loss': 34.6771, 'lr': 4.438074004644406e-07, 'epoch': 10.78}\n","{'loss': 34.3506, 'grad_norm': 10.61461067199707, 'learning_rate': 4.4184451742137334e-07, 'epoch': 10.8}\n","{'loss': 35.7482, 'lr': 4.4184451742137334e-07, 'epoch': 10.8}\n","{'loss': 33.8574, 'lr': 4.3988254252412297e-07, 'epoch': 10.82}\n","{'loss': 30.9475, 'lr': 4.379215064105454e-07, 'epoch': 10.85}\n","{'loss': 32.8709, 'lr': 4.359614397038366e-07, 'epoch': 10.88}\n","{'loss': 35.0586, 'lr': 4.3400237301205447e-07, 'epoch': 10.9}\n","{'loss': 37.1593, 'lr': 4.320443369276409e-07, 'epoch': 10.93}\n","{'loss': 33.1568, 'lr': 4.300873620269441e-07, 'epoch': 10.95}\n","{'loss': 34.3253, 'lr': 4.2813147886974076e-07, 'epoch': 10.97}\n","{'loss': 34.1405, 'grad_norm': 11.033607482910156, 'learning_rate': 4.2617671799875944e-07, 'epoch': 11.0}\n"," 55% 440/800 [02:02<01:25,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 33.5252, 'lr': 4.2617671799875944e-07, 'epoch': 11.0}\n"," 55% 440/800 [02:04<01:25,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.2286, 'lr': 4.2422310993920286e-07, 'epoch': 11.03}\n","{'loss': 36.1511, 'lr': 4.22270685198272e-07, 'epoch': 11.05}\n","{'loss': 32.4251, 'lr': 4.203194742646892e-07, 'epoch': 11.07}\n","{'loss': 35.7408, 'lr': 4.183695076082223e-07, 'epoch': 11.1}\n","{'loss': 34.4788, 'lr': 4.164208156792084e-07, 'epoch': 11.12}\n","{'loss': 35.3804, 'lr': 4.14473428908079e-07, 'epoch': 11.15}\n","{'loss': 32.994, 'lr': 4.125273777048844e-07, 'epoch': 11.18}\n","{'loss': 34.3655, 'grad_norm': 11.054380416870117, 'learning_rate': 4.1058269245881894e-07, 'epoch': 11.2}\n","{'loss': 35.2577, 'lr': 4.1058269245881894e-07, 'epoch': 11.2}\n","{'loss': 35.7843, 'lr': 4.0863940353774627e-07, 'epoch': 11.22}\n","{'loss': 33.3652, 'lr': 4.066975412877255e-07, 'epoch': 11.25}\n","{'loss': 35.0207, 'lr': 4.0475713603253694e-07, 'epoch': 11.28}\n","{'loss': 35.0308, 'lr': 4.0281821807320874e-07, 'epoch': 11.3}\n","{'loss': 35.3919, 'lr': 4.008808176875436e-07, 'epoch': 11.32}\n","{'loss': 34.3466, 'lr': 3.9894496512964634e-07, 'epoch': 11.35}\n","{'loss': 34.9012, 'lr': 3.970106906294509e-07, 'epoch': 11.38}\n","{'loss': 34.8873, 'grad_norm': 10.328295707702637, 'learning_rate': 3.9507802439224856e-07, 'epoch': 11.4}\n","{'loss': 36.034, 'lr': 3.9507802439224856e-07, 'epoch': 11.4}\n","{'loss': 35.2991, 'lr': 3.931469965982166e-07, 'epoch': 11.43}\n","{'loss': 32.5919, 'lr': 3.9121763740194613e-07, 'epoch': 11.45}\n","{'loss': 34.0284, 'lr': 3.8928997693197217e-07, 'epoch': 11.47}\n","{'loss': 34.3398, 'lr': 3.8736404529030255e-07, 'epoch': 11.5}\n","{'loss': 31.4743, 'lr': 3.854398725519479e-07, 'epoch': 11.53}\n","{'loss': 35.4498, 'lr': 3.835174887644523e-07, 'epoch': 11.55}\n","{'loss': 32.8053, 'lr': 3.815969239474238e-07, 'epoch': 11.57}\n","{'loss': 34.0028, 'grad_norm': 9.151572227478027, 'learning_rate': 3.7967820809206584e-07, 'epoch': 11.6}\n","{'loss': 34.7467, 'lr': 3.7967820809206584e-07, 'epoch': 11.6}\n","{'loss': 34.1144, 'lr': 3.777613711607087e-07, 'epoch': 11.62}\n","{'loss': 33.0592, 'lr': 3.7584644308634165e-07, 'epoch': 11.65}\n","{'loss': 33.3287, 'lr': 3.739334537721458e-07, 'epoch': 11.68}\n","{'loss': 34.3303, 'lr': 3.7202243309102677e-07, 'epoch': 11.7}\n","{'loss': 35.5303, 'lr': 3.7011341088514827e-07, 'epoch': 11.72}\n","{'loss': 34.6456, 'lr': 3.6820641696546627e-07, 'epoch': 11.75}\n","{'loss': 33.2214, 'lr': 3.663014811112634e-07, 'epoch': 11.78}\n","{'loss': 34.1221, 'grad_norm': 9.893167495727539, 'learning_rate': 3.6439863306968394e-07, 'epoch': 11.8}\n","{'loss': 32.7788, 'lr': 3.6439863306968394e-07, 'epoch': 11.8}\n","{'loss': 33.047, 'lr': 3.6249790255526915e-07, 'epoch': 11.82}\n","{'loss': 32.887, 'lr': 3.605993192494934e-07, 'epoch': 11.85}\n","{'loss': 35.1872, 'lr': 3.587029128003006e-07, 'epoch': 11.88}\n","{'loss': 33.4408, 'lr': 3.568087128216414e-07, 'epoch': 11.9}\n","{'loss': 32.6306, 'lr': 3.549167488930103e-07, 'epoch': 11.93}\n","{'loss': 35.6771, 'lr': 3.530270505589842e-07, 'epoch': 11.95}\n","{'loss': 33.5874, 'lr': 3.5113964732876106e-07, 'epoch': 11.97}\n","{'loss': 33.6545, 'grad_norm': 11.308751106262207, 'learning_rate': 3.492545686756986e-07, 'epoch': 12.0}\n"," 60% 480/800 [02:13<01:23,  3.83it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 34.476, 'lr': 3.492545686756986e-07, 'epoch': 12.0}\n"," 60% 480/800 [02:15<01:23,  3.83it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 35.8723, 'lr': 3.473718440368544e-07, 'epoch': 12.03}\n","{'loss': 34.05, 'lr': 3.454915028125263e-07, 'epoch': 12.05}\n","{'loss': 33.9308, 'lr': 3.436135743657931e-07, 'epoch': 12.07}\n","{'loss': 35.0777, 'lr': 3.417380880220563e-07, 'epoch': 12.1}\n","{'loss': 35.6396, 'lr': 3.3986507306858125e-07, 'epoch': 12.12}\n","{'loss': 33.5685, 'lr': 3.379945587540413e-07, 'epoch': 12.15}\n","{'loss': 32.4794, 'lr': 3.361265742880599e-07, 'epoch': 12.18}\n","{'loss': 34.3868, 'grad_norm': 8.891949653625488, 'learning_rate': 3.3426114884075486e-07, 'epoch': 12.2}\n","{'loss': 34.4876, 'lr': 3.3426114884075486e-07, 'epoch': 12.2}\n","{'loss': 32.9414, 'lr': 3.323983115422827e-07, 'epoch': 12.22}\n","{'loss': 36.0525, 'lr': 3.305380914823842e-07, 'epoch': 12.25}\n","{'loss': 33.1206, 'lr': 3.286805177099293e-07, 'epoch': 12.28}\n","{'loss': 34.6786, 'lr': 3.2682561923246466e-07, 'epoch': 12.3}\n","{'loss': 35.3893, 'lr': 3.249734250157592e-07, 'epoch': 12.32}\n","{'loss': 34.9916, 'lr': 3.231239639833531e-07, 'epoch': 12.35}\n","{'loss': 33.7402, 'lr': 3.2127726501610554e-07, 'epoch': 12.38}\n","{'loss': 34.4252, 'grad_norm': 11.264946937561035, 'learning_rate': 3.1943335695174363e-07, 'epoch': 12.4}\n","{'loss': 33.2845, 'lr': 3.1943335695174363e-07, 'epoch': 12.4}\n","{'loss': 33.9308, 'lr': 3.1759226858441244e-07, 'epoch': 12.43}\n","{'loss': 32.356, 'lr': 3.15754028664225e-07, 'epoch': 12.45}\n","{'loss': 33.8738, 'lr': 3.139186658968134e-07, 'epoch': 12.47}\n","{'loss': 35.7667, 'lr': 3.12086208942881e-07, 'epoch': 12.5}\n","{'loss': 33.3778, 'lr': 3.1025668641775397e-07, 'epoch': 12.53}\n","{'loss': 32.348, 'lr': 3.084301268909353e-07, 'epoch': 12.55}\n","{'loss': 33.1202, 'lr': 3.066065588856582e-07, 'epoch': 12.57}\n","{'loss': 33.5072, 'grad_norm': 9.415574073791504, 'learning_rate': 3.047860108784409e-07, 'epoch': 12.6}\n","{'loss': 32.047, 'lr': 3.047860108784409e-07, 'epoch': 12.6}\n","{'loss': 32.9179, 'lr': 3.0296851129864165e-07, 'epoch': 12.62}\n","{'loss': 32.452, 'lr': 3.011540885280153e-07, 'epoch': 12.65}\n","{'loss': 33.428, 'lr': 2.993427709002696e-07, 'epoch': 12.68}\n","{'loss': 32.7345, 'lr': 2.97534586700623e-07, 'epoch': 12.7}\n","{'loss': 35.1059, 'lr': 2.9572956416536266e-07, 'epoch': 12.72}\n","{'loss': 32.1301, 'lr': 2.9392773148140404e-07, 'epoch': 12.75}\n","{'loss': 35.0724, 'lr': 2.921291167858504e-07, 'epoch': 12.78}\n","{'loss': 33.236, 'grad_norm': 11.680055618286133, 'learning_rate': 2.9033374816555335e-07, 'epoch': 12.8}\n","{'loss': 36.0531, 'lr': 2.9033374816555335e-07, 'epoch': 12.8}\n","{'loss': 34.4297, 'lr': 2.885416536566744e-07, 'epoch': 12.82}\n","{'loss': 34.3005, 'lr': 2.867528612442469e-07, 'epoch': 12.85}\n","{'loss': 33.1485, 'lr': 2.849673988617399e-07, 'epoch': 12.88}\n","{'loss': 33.1163, 'lr': 2.831852943906203e-07, 'epoch': 12.9}\n","{'loss': 35.4993, 'lr': 2.8140657565991955e-07, 'epoch': 12.93}\n","{'loss': 32.043, 'lr': 2.796312704457969e-07, 'epoch': 12.95}\n","{'loss': 35.102, 'lr': 2.778594064711076e-07, 'epoch': 12.97}\n","{'loss': 34.2115, 'grad_norm': 11.787992477416992, 'learning_rate': 2.760910114049686e-07, 'epoch': 13.0}\n"," 65% 520/800 [02:25<01:12,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 35.6021, 'lr': 2.760910114049686e-07, 'epoch': 13.0}\n"," 65% 520/800 [02:28<01:12,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 32.6821, 'lr': 2.743261128623269e-07, 'epoch': 13.03}\n","{'loss': 35.5595, 'lr': 2.72564738403529e-07, 'epoch': 13.05}\n","{'loss': 32.7884, 'lr': 2.7080691553388916e-07, 'epoch': 13.07}\n","{'loss': 32.1904, 'lr': 2.690526717032614e-07, 'epoch': 13.1}\n","{'loss': 33.8052, 'lr': 2.673020343056094e-07, 'epoch': 13.12}\n","{'loss': 35.4581, 'lr': 2.655550306785801e-07, 'epoch': 13.15}\n","{'loss': 32.5921, 'lr': 2.638116881030753e-07, 'epoch': 13.18}\n","{'loss': 33.8348, 'grad_norm': 10.812131881713867, 'learning_rate': 2.6207203380282744e-07, 'epoch': 13.2}\n","{'loss': 33.9747, 'lr': 2.6207203380282744e-07, 'epoch': 13.2}\n","{'loss': 34.2621, 'lr': 2.6033609494397267e-07, 'epoch': 13.22}\n","{'loss': 35.108, 'lr': 2.5860389863462763e-07, 'epoch': 13.25}\n","{'loss': 32.7161, 'lr': 2.5687547192446647e-07, 'epoch': 13.28}\n","{'loss': 34.4509, 'lr': 2.5515084180429716e-07, 'epoch': 13.3}\n","{'loss': 34.2258, 'lr': 2.5343003520564153e-07, 'epoch': 13.32}\n","{'loss': 34.6582, 'lr': 2.517130790003134e-07, 'epoch': 13.35}\n","{'loss': 34.3718, 'lr': 2.500000000000001e-07, 'epoch': 13.38}\n","{'loss': 34.221, 'grad_norm': 11.955896377563477, 'learning_rate': 2.482908249558424e-07, 'epoch': 13.4}\n","{'loss': 30.9376, 'lr': 2.482908249558424e-07, 'epoch': 13.4}\n","{'loss': 33.1015, 'lr': 2.4658558055801847e-07, 'epoch': 13.43}\n","{'loss': 33.8529, 'lr': 2.4488429343532556e-07, 'epoch': 13.45}\n","{'loss': 33.2297, 'lr': 2.431869901547649e-07, 'epoch': 13.47}\n","{'loss': 32.5636, 'lr': 2.4149369722112715e-07, 'epoch': 13.5}\n","{'loss': 31.9192, 'lr': 2.398044410765774e-07, 'epoch': 13.53}\n","{'loss': 35.4476, 'lr': 2.381192481002438e-07, 'epoch': 13.55}\n","{'loss': 34.9899, 'lr': 2.3643814460780393e-07, 'epoch': 13.57}\n","{'loss': 33.2552, 'grad_norm': 12.168783187866211, 'learning_rate': 2.347611568510754e-07, 'epoch': 13.6}\n","{'loss': 34.3514, 'lr': 2.347611568510754e-07, 'epoch': 13.6}\n","{'loss': 32.0968, 'lr': 2.3308831101760483e-07, 'epoch': 13.62}\n","{'loss': 34.9659, 'lr': 2.3141963323025914e-07, 'epoch': 13.65}\n","{'loss': 34.7004, 'lr': 2.2975514954681834e-07, 'epoch': 13.68}\n","{'loss': 33.2719, 'lr': 2.2809488595956745e-07, 'epoch': 13.7}\n","{'loss': 34.1707, 'lr': 2.264388683948918e-07, 'epoch': 13.72}\n","{'loss': 33.0043, 'lr': 2.2478712271287087e-07, 'epoch': 13.75}\n","{'loss': 32.7007, 'lr': 2.231396747068759e-07, 'epoch': 13.78}\n","{'loss': 33.6578, 'grad_norm': 9.816309928894043, 'learning_rate': 2.214965501031657e-07, 'epoch': 13.8}\n","{'loss': 33.9459, 'lr': 2.214965501031657e-07, 'epoch': 13.8}\n","{'loss': 34.0748, 'lr': 2.1985777456048632e-07, 'epoch': 13.82}\n","{'loss': 32.531, 'lr': 2.1822337366966897e-07, 'epoch': 13.85}\n","{'loss': 35.2998, 'lr': 2.1659337295323114e-07, 'epoch': 13.88}\n","{'loss': 32.3455, 'lr': 2.1496779786497859e-07, 'epoch': 13.9}\n","{'loss': 34.2566, 'lr': 2.133466737896064e-07, 'epoch': 13.93}\n","{'loss': 34.1166, 'lr': 2.1173002604230423e-07, 'epoch': 13.95}\n","{'loss': 36.3571, 'lr': 2.1011787986835932e-07, 'epoch': 13.97}\n","{'loss': 34.1159, 'grad_norm': 12.519892692565918, 'learning_rate': 2.0851026044276405e-07, 'epoch': 14.0}\n"," 70% 560/800 [02:37<01:09,  3.45it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 33.2022, 'lr': 2.0851026044276405e-07, 'epoch': 14.0}\n"," 70% 560/800 [02:39<01:09,  3.45it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.1127, 'lr': 2.0690719286982123e-07, 'epoch': 14.03}\n","{'loss': 31.5399, 'lr': 2.053087021827527e-07, 'epoch': 14.05}\n","{'loss': 34.322, 'lr': 2.0371481334330908e-07, 'epoch': 14.07}\n","{'loss': 33.3116, 'lr': 2.0212555124137864e-07, 'epoch': 14.1}\n","{'loss': 33.8745, 'lr': 2.005409406946e-07, 'epoch': 14.12}\n","{'loss': 33.6053, 'lr': 1.9896100644797315e-07, 'epoch': 14.15}\n","{'loss': 31.9146, 'lr': 1.973857731734746e-07, 'epoch': 14.18}\n","{'loss': 33.2353, 'grad_norm': 10.320533752441406, 'learning_rate': 1.9581526546967047e-07, 'epoch': 14.2}\n","{'loss': 33.0738, 'lr': 1.9581526546967047e-07, 'epoch': 14.2}\n","{'loss': 34.6965, 'lr': 1.942495078613341e-07, 'epoch': 14.22}\n","{'loss': 33.4751, 'lr': 1.9268852479906145e-07, 'epoch': 14.25}\n","{'loss': 31.9253, 'lr': 1.911323406588901e-07, 'epoch': 14.28}\n","{'loss': 33.2629, 'lr': 1.8958097974191906e-07, 'epoch': 14.3}\n","{'loss': 32.4947, 'lr': 1.8803446627392794e-07, 'epoch': 14.32}\n","{'loss': 35.8959, 'lr': 1.8649282440500015e-07, 'epoch': 14.35}\n","{'loss': 31.7102, 'lr': 1.849560782091445e-07, 'epoch': 14.38}\n","{'loss': 33.3168, 'grad_norm': 10.442946434020996, 'learning_rate': 1.834242516839203e-07, 'epoch': 14.4}\n","{'loss': 35.473, 'lr': 1.834242516839203e-07, 'epoch': 14.4}\n","{'loss': 32.2891, 'lr': 1.8189736875006185e-07, 'epoch': 14.43}\n","{'loss': 35.162, 'lr': 1.8037545325110504e-07, 'epoch': 14.45}\n","{'loss': 33.007, 'lr': 1.7885852895301578e-07, 'epoch': 14.47}\n","{'loss': 33.2942, 'lr': 1.7734661954381752e-07, 'epoch': 14.5}\n","{'loss': 35.743, 'lr': 1.758397486332227e-07, 'epoch': 14.53}\n","{'loss': 34.3016, 'lr': 1.7433793975226296e-07, 'epoch': 14.55}\n","{'loss': 31.5524, 'lr': 1.728412163529227e-07, 'epoch': 14.57}\n","{'loss': 33.8528, 'grad_norm': 10.854354858398438, 'learning_rate': 1.713496018077717e-07, 'epoch': 14.6}\n","{'loss': 32.8755, 'lr': 1.713496018077717e-07, 'epoch': 14.6}\n","{'loss': 35.1896, 'lr': 1.6986311940960147e-07, 'epoch': 14.62}\n","{'loss': 34.7427, 'lr': 1.6838179237106014e-07, 'epoch': 14.65}\n","{'loss': 32.6061, 'lr': 1.6690564382429101e-07, 'epoch': 14.68}\n","{'loss': 32.7505, 'lr': 1.6543469682057104e-07, 'epoch': 14.7}\n","{'loss': 36.6106, 'lr': 1.6396897432995045e-07, 'epoch': 14.72}\n","{'loss': 34.3055, 'lr': 1.6250849924089482e-07, 'epoch': 14.75}\n","{'loss': 32.8708, 'lr': 1.610532943599268e-07, 'epoch': 14.78}\n","{'loss': 33.9939, 'grad_norm': 9.887027740478516, 'learning_rate': 1.596033824112709e-07, 'epoch': 14.8}\n","{'loss': 34.4167, 'lr': 1.596033824112709e-07, 'epoch': 14.8}\n","{'loss': 35.0408, 'lr': 1.581587860364977e-07, 'epoch': 14.82}\n","{'loss': 35.0369, 'lr': 1.5671952779417057e-07, 'epoch': 14.85}\n","{'loss': 34.2461, 'lr': 1.552856301594942e-07, 'epoch': 14.88}\n","{'loss': 32.3389, 'lr': 1.5385711552396224e-07, 'epoch': 14.9}\n","{'loss': 32.8023, 'lr': 1.52434006195009e-07, 'epoch': 14.93}\n","{'loss': 33.3856, 'lr': 1.5101632439565999e-07, 'epoch': 14.95}\n","{'loss': 35.3291, 'lr': 1.4960409226418575e-07, 'epoch': 14.97}\n","{'loss': 34.0746, 'grad_norm': 11.24792194366455, 'learning_rate': 1.4819733185375531e-07, 'epoch': 15.0}\n"," 75% 600/800 [02:49<00:47,  4.25it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 33.5647, 'lr': 1.4819733185375531e-07, 'epoch': 15.0}\n"," 75% 600/800 [02:51<00:47,  4.25it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.1264, 'lr': 1.4679606513209281e-07, 'epoch': 15.03}\n","{'loss': 34.6373, 'lr': 1.4540031398113334e-07, 'epoch': 15.05}\n","{'loss': 34.1111, 'lr': 1.4401010019668224e-07, 'epoch': 15.07}\n","{'loss': 33.9368, 'lr': 1.426254454880743e-07, 'epoch': 15.1}\n","{'loss': 33.4584, 'lr': 1.412463714778343e-07, 'epoch': 15.12}\n","{'loss': 35.3934, 'lr': 1.3987289970134048e-07, 'epoch': 15.15}\n","{'loss': 32.3344, 'lr': 1.3850505160648706e-07, 'epoch': 15.18}\n","{'loss': 33.9453, 'grad_norm': 10.402097702026367, 'learning_rate': 1.371428485533498e-07, 'epoch': 15.2}\n","{'loss': 33.6148, 'lr': 1.371428485533498e-07, 'epoch': 15.2}\n","{'loss': 35.1115, 'lr': 1.3578631181385304e-07, 'epoch': 15.22}\n","{'loss': 32.2865, 'lr': 1.3443546257143623e-07, 'epoch': 15.25}\n","{'loss': 33.4761, 'lr': 1.330903219207246e-07, 'epoch': 15.28}\n","{'loss': 33.1389, 'lr': 1.317509108671983e-07, 'epoch': 15.3}\n","{'loss': 34.9551, 'lr': 1.3041725032686578e-07, 'epoch': 15.32}\n","{'loss': 33.0838, 'lr': 1.29089361125936e-07, 'epoch': 15.35}\n","{'loss': 33.3192, 'lr': 1.277672640004936e-07, 'epoch': 15.38}\n","{'loss': 33.6232, 'grad_norm': 10.60783576965332, 'learning_rate': 1.2645097959617586e-07, 'epoch': 15.4}\n","{'loss': 34.7037, 'lr': 1.2645097959617586e-07, 'epoch': 15.4}\n","{'loss': 36.4063, 'lr': 1.2514052846784878e-07, 'epoch': 15.43}\n","{'loss': 31.5565, 'lr': 1.238359310792877e-07, 'epoch': 15.45}\n","{'loss': 35.5725, 'lr': 1.2253720780285636e-07, 'epoch': 15.47}\n","{'loss': 34.1282, 'lr': 1.2124437891918993e-07, 'epoch': 15.5}\n","{'loss': 32.7045, 'lr': 1.1995746461687732e-07, 'epoch': 15.53}\n","{'loss': 34.419, 'lr': 1.1867648499214677e-07, 'epoch': 15.55}\n","{'loss': 30.6754, 'lr': 1.174014600485514e-07, 'epoch': 15.57}\n","{'loss': 33.7708, 'grad_norm': 9.5859956741333, 'learning_rate': 1.1613240969665683e-07, 'epoch': 15.6}\n","{'loss': 32.999, 'lr': 1.1613240969665683e-07, 'epoch': 15.6}\n","{'loss': 34.7008, 'lr': 1.1486935375373124e-07, 'epoch': 15.62}\n","{'loss': 33.8747, 'lr': 1.1361231194343435e-07, 'epoch': 15.65}\n","{'loss': 31.4709, 'lr': 1.123613038955109e-07, 'epoch': 15.68}\n","{'loss': 35.8614, 'lr': 1.1111634914548296e-07, 'epoch': 15.7}\n","{'loss': 33.3001, 'lr': 1.0987746713434576e-07, 'epoch': 15.72}\n","{'loss': 32.407, 'lr': 1.0864467720826343e-07, 'epoch': 15.75}\n","{'loss': 33.1516, 'lr': 1.0741799861826705e-07, 'epoch': 15.78}\n","{'loss': 33.4707, 'grad_norm': 11.488251686096191, 'learning_rate': 1.0619745051995471e-07, 'epoch': 15.8}\n","{'loss': 33.0813, 'lr': 1.0619745051995471e-07, 'epoch': 15.8}\n","{'loss': 32.8545, 'lr': 1.0498305197319113e-07, 'epoch': 15.82}\n","{'loss': 34.3661, 'lr': 1.037748219418113e-07, 'epoch': 15.85}\n","{'loss': 32.8371, 'lr': 1.0257277929332331e-07, 'epoch': 15.88}\n","{'loss': 32.0824, 'lr': 1.0137694279861453e-07, 'epoch': 15.9}\n","{'loss': 32.8122, 'lr': 1.0018733113165772e-07, 'epoch': 15.93}\n","{'loss': 34.6622, 'lr': 9.900396286922025e-08, 'epoch': 15.95}\n","{'loss': 32.8707, 'lr': 9.782685649057332e-08, 'epoch': 15.97}\n","{'loss': 33.1958, 'grad_norm': 11.211450576782227, 'learning_rate': 9.66560303772035e-08, 'epoch': 16.0}\n"," 80% 640/800 [03:01<00:39,  4.06it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 32.9841, 'lr': 9.66560303772035e-08, 'epoch': 16.0}\n"," 80% 640/800 [03:02<00:39,  4.06it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 33.4841, 'lr': 9.549150281252632e-08, 'epoch': 16.02}\n","{'loss': 34.174, 'lr': 9.433329198159973e-08, 'epoch': 16.05}\n","{'loss': 35.1879, 'lr': 9.318141597084128e-08, 'epoch': 16.07}\n","{'loss': 34.1762, 'lr': 9.203589276774437e-08, 'epoch': 16.1}\n","{'loss': 33.5627, 'lr': 9.08967402605988e-08, 'epoch': 16.12}\n","{'loss': 35.1744, 'lr': 8.976397623821003e-08, 'epoch': 16.15}\n","{'loss': 32.5138, 'lr': 8.863761838962258e-08, 'epoch': 16.18}\n","{'loss': 33.9071, 'grad_norm': 11.488085746765137, 'learning_rate': 8.751768430384305e-08, 'epoch': 16.2}\n","{'loss': 32.0129, 'lr': 8.751768430384305e-08, 'epoch': 16.2}\n","{'loss': 33.1703, 'lr': 8.640419146956557e-08, 'epoch': 16.23}\n","{'loss': 32.5117, 'lr': 8.529715727489912e-08, 'epoch': 16.25}\n","{'loss': 33.0826, 'lr': 8.419659900709535e-08, 'epoch': 16.27}\n","{'loss': 34.4462, 'lr': 8.310253385227944e-08, 'epoch': 16.3}\n","{'loss': 34.0221, 'lr': 8.201497889518072e-08, 'epoch': 16.32}\n","{'loss': 31.5957, 'lr': 8.093395111886686e-08, 'epoch': 16.35}\n","{'loss': 32.688, 'lr': 7.985946740447791e-08, 'epoch': 16.38}\n","{'loss': 32.9412, 'grad_norm': 11.089111328125, 'learning_rate': 7.879154453096304e-08, 'epoch': 16.4}\n","{'loss': 34.1808, 'lr': 7.879154453096304e-08, 'epoch': 16.4}\n","{'loss': 33.28, 'lr': 7.773019917481871e-08, 'epoch': 16.43}\n","{'loss': 35.8434, 'lr': 7.667544790982777e-08, 'epoch': 16.45}\n","{'loss': 33.3343, 'lr': 7.56273072068011e-08, 'epoch': 16.48}\n","{'loss': 33.3849, 'lr': 7.458579343331995e-08, 'epoch': 16.5}\n","{'loss': 34.8827, 'lr': 7.355092285348091e-08, 'epoch': 16.52}\n","{'loss': 32.4127, 'lr': 7.252271162764128e-08, 'epoch': 16.55}\n","{'loss': 33.4756, 'lr': 7.150117581216747e-08, 'epoch': 16.57}\n","{'loss': 33.8493, 'grad_norm': 11.478360176086426, 'learning_rate': 7.048633135918347e-08, 'epoch': 16.6}\n","{'loss': 31.7372, 'lr': 7.048633135918347e-08, 'epoch': 16.6}\n","{'loss': 31.8344, 'lr': 6.947819411632222e-08, 'epoch': 16.62}\n","{'loss': 35.4489, 'lr': 6.847677982647825e-08, 'epoch': 16.65}\n","{'loss': 35.7127, 'lr': 6.748210412756134e-08, 'epoch': 16.68}\n","{'loss': 34.99, 'lr': 6.649418255225297e-08, 'epoch': 16.7}\n","{'loss': 34.3031, 'lr': 6.551303052776291e-08, 'epoch': 16.73}\n","{'loss': 33.5884, 'lr': 6.453866337558939e-08, 'epoch': 16.75}\n","{'loss': 33.085, 'lr': 6.357109631127889e-08, 'epoch': 16.77}\n","{'loss': 33.8375, 'grad_norm': 10.854402542114258, 'learning_rate': 6.261034444418878e-08, 'epoch': 16.8}\n","{'loss': 33.286, 'lr': 6.261034444418878e-08, 'epoch': 16.8}\n","{'loss': 32.8577, 'lr': 6.165642277725203e-08, 'epoch': 16.82}\n","{'loss': 35.8503, 'lr': 6.07093462067419e-08, 'epoch': 16.85}\n","{'loss': 33.7491, 'lr': 5.976912952204016e-08, 'epoch': 16.88}\n","{'loss': 34.0079, 'lr': 5.883578740540546e-08, 'epoch': 16.9}\n","{'loss': 33.5048, 'lr': 5.7909334431744896e-08, 'epoch': 16.93}\n","{'loss': 34.2268, 'lr': 5.698978506838531e-08, 'epoch': 16.95}\n","{'loss': 32.3318, 'lr': 5.607715367484861e-08, 'epoch': 16.98}\n","{'loss': 33.7268, 'grad_norm': 10.912671089172363, 'learning_rate': 5.517145450262639e-08, 'epoch': 17.0}\n"," 85% 680/800 [03:11<00:29,  4.10it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 31.3608, 'lr': 5.517145450262639e-08, 'epoch': 17.0}\n"," 85% 680/800 [03:13<00:29,  4.10it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 35.1917, 'lr': 5.427270169495807e-08, 'epoch': 17.02}\n","{'loss': 32.6844, 'lr': 5.338090928660999e-08, 'epoch': 17.05}\n","{'loss': 33.3734, 'lr': 5.249609120365578e-08, 'epoch': 17.07}\n","{'loss': 34.17, 'lr': 5.16182612632598e-08, 'epoch': 17.1}\n","{'loss': 34.5816, 'lr': 5.0747433173460086e-08, 'epoch': 17.12}\n","{'loss': 32.2502, 'lr': 4.988362053295564e-08, 'epoch': 17.15}\n","{'loss': 31.26, 'lr': 4.902683683089304e-08, 'epoch': 17.18}\n","{'loss': 33.109, 'grad_norm': 11.031139373779297, 'learning_rate': 4.817709544665627e-08, 'epoch': 17.2}\n","{'loss': 33.0712, 'lr': 4.817709544665627e-08, 'epoch': 17.2}\n","{'loss': 35.0022, 'lr': 4.7334409649657906e-08, 'epoch': 17.23}\n","{'loss': 30.6811, 'lr': 4.649879259913136e-08, 'epoch': 17.25}\n","{'loss': 32.0898, 'lr': 4.567025734392621e-08, 'epoch': 17.27}\n","{'loss': 35.4073, 'lr': 4.4848816822303404e-08, 'epoch': 17.3}\n","{'loss': 33.4868, 'lr': 4.403448386173436e-08, 'epoch': 17.32}\n","{'loss': 30.6964, 'lr': 4.322727117869951e-08, 'epoch': 17.35}\n","{'loss': 35.8981, 'lr': 4.242719137849077e-08, 'epoch': 17.38}\n","{'loss': 33.2916, 'grad_norm': 11.65601634979248, 'learning_rate': 4.163425695501388e-08, 'epoch': 17.4}\n","{'loss': 33.3813, 'lr': 4.163425695501388e-08, 'epoch': 17.4}\n","{'loss': 36.1214, 'lr': 4.084848029059362e-08, 'epoch': 17.43}\n","{'loss': 34.903, 'lr': 4.006987365578079e-08, 'epoch': 17.45}\n","{'loss': 34.9787, 'lr': 3.929844920915987e-08, 'epoch': 17.48}\n","{'loss': 33.983, 'lr': 3.853421899715992e-08, 'epoch': 17.5}\n","{'loss': 33.9457, 'lr': 3.777719495386566e-08, 'epoch': 17.52}\n","{'loss': 33.4549, 'lr': 3.702738890083207e-08, 'epoch': 17.55}\n","{'loss': 33.5011, 'lr': 3.6284812546898744e-08, 'epoch': 17.57}\n","{'loss': 34.2836, 'grad_norm': 10.990205764770508, 'learning_rate': 3.554947748800785e-08, 'epoch': 17.6}\n","{'loss': 33.2908, 'lr': 3.554947748800785e-08, 'epoch': 17.6}\n","{'loss': 34.2613, 'lr': 3.482139520702276e-08, 'epoch': 17.62}\n","{'loss': 33.5882, 'lr': 3.410057707354863e-08, 'epoch': 17.65}\n","{'loss': 32.7536, 'lr': 3.338703434375506e-08, 'epoch': 17.68}\n","{'loss': 34.932, 'lr': 3.268077816020015e-08, 'epoch': 17.7}\n","{'loss': 33.6321, 'lr': 3.198181955165668e-08, 'epoch': 17.73}\n","{'loss': 32.543, 'lr': 3.129016943293955e-08, 'epoch': 17.75}\n","{'loss': 33.2341, 'lr': 3.060583860473587e-08, 'epoch': 17.77}\n","{'loss': 33.5294, 'grad_norm': 11.278878211975098, 'learning_rate': 2.992883775343574e-08, 'epoch': 17.8}\n","{'loss': 34.786, 'lr': 2.992883775343574e-08, 'epoch': 17.8}\n","{'loss': 32.9144, 'lr': 2.925917745096568e-08, 'epoch': 17.82}\n","{'loss': 33.7983, 'lr': 2.85968681546237e-08, 'epoch': 17.85}\n","{'loss': 34.0626, 'lr': 2.7941920206915436e-08, 'epoch': 17.88}\n","{'loss': 33.7145, 'lr': 2.7294343835393364e-08, 'epoch': 17.9}\n","{'loss': 32.1108, 'lr': 2.6654149152496307e-08, 'epoch': 17.93}\n","{'loss': 33.4697, 'lr': 2.6021346155392422e-08, 'epoch': 17.95}\n","{'loss': 35.6052, 'lr': 2.539594472582213e-08, 'epoch': 17.98}\n","{'loss': 33.8077, 'grad_norm': 12.146208763122559, 'learning_rate': 2.4777954629944475e-08, 'epoch': 18.0}\n"," 90% 720/800 [03:23<00:20,  3.99it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 33.368, 'lr': 2.4777954629944475e-08, 'epoch': 18.0}\n"," 90% 720/800 [03:25<00:20,  3.99it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 34.3826, 'lr': 2.4167385518184536e-08, 'epoch': 18.02}\n","{'loss': 32.2701, 'lr': 2.3564246925082352e-08, 'epoch': 18.05}\n","{'loss': 33.1425, 'lr': 2.296854826914457e-08, 'epoch': 18.07}\n","{'loss': 33.9904, 'lr': 2.2380298852696767e-08, 'epoch': 18.1}\n","{'loss': 33.5539, 'lr': 2.1799507861738788e-08, 'epoch': 18.12}\n","{'loss': 32.7471, 'lr': 2.1226184365800813e-08, 'epoch': 18.15}\n","{'loss': 36.6688, 'lr': 2.066033731780209e-08, 'epoch': 18.18}\n","{'loss': 33.7654, 'grad_norm': 12.989852905273438, 'learning_rate': 2.01019755539108e-08, 'epoch': 18.2}\n","{'loss': 33.5068, 'lr': 2.01019755539108e-08, 'epoch': 18.2}\n","{'loss': 31.4859, 'lr': 1.955110779340635e-08, 'epoch': 18.23}\n","{'loss': 33.2769, 'lr': 1.90077426385431e-08, 'epoch': 18.25}\n","{'loss': 34.7739, 'lr': 1.847188857441595e-08, 'epoch': 18.27}\n","{'loss': 33.876, 'lr': 1.794355396882813e-08, 'epoch': 18.3}\n","{'loss': 35.0124, 'lr': 1.7422747072160016e-08, 'epoch': 18.32}\n","{'loss': 33.2334, 'lr': 1.6909476017240908e-08, 'epoch': 18.35}\n","{'loss': 32.2021, 'lr': 1.6403748819221462e-08, 'epoch': 18.38}\n","{'loss': 33.4209, 'grad_norm': 11.1782865524292, 'learning_rate': 1.590557337544901e-08, 'epoch': 18.4}\n","{'loss': 35.5555, 'lr': 1.590557337544901e-08, 'epoch': 18.4}\n","{'loss': 33.9723, 'lr': 1.541495746534388e-08, 'epoch': 18.43}\n","{'loss': 32.5493, 'lr': 1.4931908750278e-08, 'epoch': 18.45}\n","{'loss': 31.2805, 'lr': 1.445643477345554e-08, 'epoch': 18.48}\n","{'loss': 32.2337, 'lr': 1.3988542959794625e-08, 'epoch': 18.5}\n","{'loss': 35.2527, 'lr': 1.3528240615811815e-08, 'epoch': 18.52}\n","{'loss': 34.2958, 'lr': 1.3075534929507693e-08, 'epoch': 18.55}\n","{'loss': 33.3776, 'lr': 1.2630432970255012e-08, 'epoch': 18.57}\n","{'loss': 33.5647, 'grad_norm': 10.41858196258545, 'learning_rate': 1.2192941688687842e-08, 'epoch': 18.6}\n","{'loss': 32.3307, 'lr': 1.2192941688687842e-08, 'epoch': 18.6}\n","{'loss': 33.5839, 'lr': 1.176306791659326e-08, 'epoch': 18.62}\n","{'loss': 32.9689, 'lr': 1.1340818366804728e-08, 'epoch': 18.65}\n","{'loss': 35.7034, 'lr': 1.0926199633097154e-08, 'epoch': 18.68}\n","{'loss': 34.9663, 'lr': 1.0519218190084056e-08, 'epoch': 18.7}\n","{'loss': 35.8774, 'lr': 1.0119880393116175e-08, 'epoch': 18.73}\n","{'loss': 33.1759, 'lr': 9.728192478182573e-09, 'epoch': 18.75}\n","{'loss': 33.6071, 'lr': 9.34416056181292e-09, 'epoch': 18.77}\n","{'loss': 34.0267, 'grad_norm': 11.341251373291016, 'learning_rate': 8.967790640982464e-09, 'epoch': 18.8}\n","{'loss': 32.3358, 'lr': 8.967790640982464e-09, 'epoch': 18.8}\n","{'loss': 31.6619, 'lr': 8.599088593017722e-09, 'epoch': 18.82}\n","{'loss': 34.2247, 'lr': 8.238060175505268e-09, 'epoch': 18.85}\n","{'loss': 34.6616, 'lr': 7.884711026201584e-09, 'epoch': 18.88}\n","{'loss': 33.7462, 'lr': 7.53904666294497e-09, 'epoch': 18.9}\n","{'loss': 32.8777, 'lr': 7.201072483569548e-09, 'epoch': 18.93}\n","{'loss': 32.8876, 'lr': 6.870793765820782e-09, 'epoch': 18.95}\n","{'loss': 32.6086, 'lr': 6.548215667273205e-09, 'epoch': 18.98}\n","{'loss': 33.1255, 'grad_norm': 10.583483695983887, 'learning_rate': 6.233343225249932e-09, 'epoch': 19.0}\n"," 95% 760/800 [03:34<00:09,  4.06it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","{'loss': 35.8539, 'lr': 6.233343225249932e-09, 'epoch': 19.0}\n"," 95% 760/800 [03:36<00:09,  4.06it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n","  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","{'loss': 33.2763, 'lr': 5.926181356743609e-09, 'epoch': 19.02}\n","{'loss': 30.7615, 'lr': 5.626734858340254e-09, 'epoch': 19.05}\n","{'loss': 32.5357, 'lr': 5.335008406143815e-09, 'epoch': 19.07}\n","{'loss': 34.6172, 'lr': 5.051006555703452e-09, 'epoch': 19.1}\n","{'loss': 34.6963, 'lr': 4.7747337419422054e-09, 'epoch': 19.12}\n","{'loss': 33.3578, 'lr': 4.5061942790879384e-09, 'epoch': 19.15}\n","{'loss': 35.3635, 'lr': 4.245392360605726e-09, 'epoch': 19.18}\n","{'loss': 33.8078, 'grad_norm': 12.105005264282227, 'learning_rate': 3.99233205913263e-09, 'epoch': 19.2}\n","{'loss': 35.5063, 'lr': 3.99233205913263e-09, 'epoch': 19.2}\n","{'loss': 34.6321, 'lr': 3.747017326413971e-09, 'epoch': 19.23}\n","{'loss': 34.481, 'lr': 3.509451993241541e-09, 'epoch': 19.25}\n","{'loss': 32.887, 'lr': 3.279639769393938e-09, 'epoch': 19.27}\n","{'loss': 33.8408, 'lr': 3.0575842435785483e-09, 'epoch': 19.3}\n","{'loss': 33.8989, 'lr': 2.8432888833755388e-09, 'epoch': 19.32}\n","{'loss': 32.4539, 'lr': 2.6367570351836234e-09, 'epoch': 19.35}\n","{'loss': 34.14, 'lr': 2.437991924167937e-09, 'epoch': 19.38}\n","{'loss': 33.98, 'grad_norm': 10.369142532348633, 'learning_rate': 2.246996654209632e-09, 'epoch': 19.4}\n","{'loss': 35.8098, 'lr': 2.246996654209632e-09, 'epoch': 19.4}\n","{'loss': 33.5597, 'lr': 2.0637742078573604e-09, 'epoch': 19.43}\n","{'loss': 35.5398, 'lr': 1.8883274462806463e-09, 'epoch': 19.45}\n","{'loss': 33.4219, 'lr': 1.720659109225364e-09, 'epoch': 19.48}\n","{'loss': 35.2168, 'lr': 1.5607718149708848e-09, 'epoch': 19.5}\n","{'loss': 31.1644, 'lr': 1.4086680602891642e-09, 'epoch': 19.52}\n","{'loss': 34.3518, 'lr': 1.2643502204057189e-09, 'epoch': 19.55}\n","{'loss': 32.8023, 'lr': 1.1278205489626546e-09, 'epoch': 19.57}\n","{'loss': 33.9833, 'grad_norm': 10.99036693572998, 'learning_rate': 9.99081177983363e-10, 'epoch': 19.6}\n","{'loss': 33.7788, 'lr': 9.99081177983363e-10, 'epoch': 19.6}\n","{'loss': 34.5791, 'lr': 8.781341178393242e-10, 'epoch': 19.62}\n","{'loss': 32.5685, 'lr': 7.649812572185222e-10, 'epoch': 19.65}\n","{'loss': 33.1937, 'lr': 6.596243630963005e-10, 'epoch': 19.68}\n","{'loss': 33.6204, 'lr': 5.620650807073856e-10, 'epoch': 19.7}\n","{'loss': 33.6884, 'lr': 4.723049335204066e-10, 'epoch': 19.73}\n","{'loss': 31.7246, 'lr': 3.903453232140808e-10, 'epoch': 19.75}\n","{'loss': 34.0217, 'lr': 3.161875296553429e-10, 'epoch': 19.77}\n","{'loss': 33.3969, 'grad_norm': 11.664361000061035, 'learning_rate': 2.4983271087924975e-10, 'epoch': 19.8}\n","{'loss': 31.846, 'lr': 2.4983271087924975e-10, 'epoch': 19.8}\n","{'loss': 34.0393, 'lr': 1.9128190307104996e-10, 'epoch': 19.82}\n","{'loss': 32.8468, 'lr': 1.4053602054991954e-10, 'epoch': 19.85}\n","{'loss': 33.2897, 'lr': 9.759585575458417e-11, 'epoch': 19.88}\n","{'loss': 33.5349, 'lr': 6.246207923116254e-11, 'epoch': 19.9}\n","{'loss': 30.4892, 'lr': 3.513523962256348e-11, 'epoch': 19.93}\n","{'loss': 34.3289, 'lr': 1.561576365988193e-11, 'epoch': 19.95}\n","{'loss': 33.4599, 'lr': 3.903956155848487e-12, 'epoch': 19.98}\n","{'loss': 32.9794, 'grad_norm': 10.661575317382812, 'learning_rate': 0.0, 'epoch': 20.0}\n","{'train_runtime': 227.829, 'train_samples_per_second': 7.023, 'train_steps_per_second': 3.511, 'train_loss': 35.21610029697418, 'epoch': 20.0}\n","100% 800/800 [03:47<00:00,  3.51it/s]\n","******model_save_path is model7b_M1_family_epoch_20_r_64_moreData_lr_1e-6/adapter_model.safetensors******\n","/bin/bash: line 1: ./predict_M1k1.sh: Permission denied\n","/bin/bash: line 1: ./predict_M1k2.sh: Permission denied\n","/bin/bash: line 1: ./predict_M1k3.sh: Permission denied\n","/bin/bash: line 1: ./predict_M2k3.sh: Permission denied\n"]}],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 20 \\\n","    --json_path \"./dataset/family_k1k2.json\" \\\n","    --output_dir \"model7b_M1_family_epoch_20_r_64_moreData_lr_1e-6\" \\\n","    --save_steps 40 \\\n","    --LORA_R 64 \\\n","    --lr 1e-6\n","\n","!./predict_M1k1.sh\n","!./predict_M1k2.sh\n","!./predict_M1k3.sh\n","\n","!./predict_M2k3.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWWhm2ub0hzh"},"outputs":[],"source":["!cat ./model7b_M1_family_epoch_30_r_64_moreData/checkpoint-480/"]},{"cell_type":"markdown","metadata":{"id":"AQczJuiDQ54f"},"source":["## Train LoRA_R = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":895280,"status":"ok","timestamp":1724478170486,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"FJdEQfhtQ54f","outputId":"3577e56e-534f-4bfe-ce95-2e09d51473b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","auto\n","False\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:02<00:00,  1.43it/s]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","Generating train split: 80 examples [00:00, 3918.81 examples/s]\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 80/80 [00:00<00:00, 2060.73 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","  0% 0/1200 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 40.2183, 'lr': 0.0, 'epoch': 0}\n","{'loss': 40.2183, 'grad_norm': 8.461231231689453, 'learning_rate': 4e-05, 'epoch': 0.03}\n","{'loss': 40.1084, 'lr': 4e-05, 'epoch': 0.03}\n","{'loss': 38.7283, 'lr': 8e-05, 'epoch': 0.05}\n","{'loss': 38.3996, 'lr': 0.00012, 'epoch': 0.07}\n","{'loss': 34.6132, 'lr': 0.00016, 'epoch': 0.1}\n","{'loss': 35.489, 'lr': 0.0002, 'epoch': 0.12}\n","{'loss': 31.7058, 'lr': 0.0001999996544318651, 'epoch': 0.15}\n","{'loss': 32.0306, 'lr': 0.00019999861772984878, 'epoch': 0.17}\n","{'loss': 35.8678, 'grad_norm': 13.000596046447754, 'learning_rate': 0.00019999688990111601, 'epoch': 0.2}\n","{'loss': 31.2783, 'lr': 0.00019999688990111601, 'epoch': 0.2}\n","{'loss': 30.2826, 'lr': 0.00019999447095760847, 'epoch': 0.23}\n","{'loss': 28.1695, 'lr': 0.00019999136091604434, 'epoch': 0.25}\n","{'loss': 26.6191, 'lr': 0.00019998755979791827, 'epoch': 0.28}\n","{'loss': 23.6183, 'lr': 0.0001999830676295012, 'epoch': 0.3}\n","{'loss': 23.5241, 'lr': 0.00019997788444184002, 'epoch': 0.33}\n","{'loss': 22.4222, 'lr': 0.00019997201027075772, 'epoch': 0.35}\n","{'loss': 21.4256, 'lr': 0.00019996544515685281, 'epoch': 0.38}\n","{'loss': 25.9175, 'grad_norm': 20.94915008544922, 'learning_rate': 0.00019995818914549912, 'epoch': 0.4}\n","{'loss': 19.4249, 'lr': 0.00019995818914549912, 'epoch': 0.4}\n","{'loss': 17.407, 'lr': 0.00019995024228684565, 'epoch': 0.42}\n","{'loss': 15.7797, 'lr': 0.000199941604635816, 'epoch': 0.45}\n","{'loss': 13.1189, 'lr': 0.0001999322762521081, 'epoch': 0.47}\n","{'loss': 10.5864, 'lr': 0.00019992225720019376, 'epoch': 0.5}\n","{'loss': 9.6716, 'lr': 0.00019991154754931832, 'epoch': 0.53}\n","{'loss': 7.3993, 'lr': 0.00019990014737350005, 'epoch': 0.55}\n","{'loss': 5.0441, 'lr': 0.0001998880567515297, 'epoch': 0.57}\n","{'loss': 12.304, 'grad_norm': 12.274964332580566, 'learning_rate': 0.00019987527576696996, 'epoch': 0.6}\n","{'loss': 4.9655, 'lr': 0.00019987527576696996, 'epoch': 0.6}\n","{'loss': 4.8606, 'lr': 0.00019986180450815485, 'epoch': 0.62}\n","{'loss': 4.6419, 'lr': 0.0001998476430681891, 'epoch': 0.65}\n","{'loss': 4.5371, 'lr': 0.00019983279154494758, 'epoch': 0.68}\n","{'loss': 5.0218, 'lr': 0.00019981725004107454, 'epoch': 0.7}\n","{'loss': 4.8865, 'lr': 0.00019980101866398298, 'epoch': 0.72}\n","{'loss': 4.2063, 'lr': 0.00019978409752585376, 'epoch': 0.75}\n","{'loss': 4.7881, 'lr': 0.0001997664867436351, 'epoch': 0.78}\n","{'loss': 4.7385, 'grad_norm': 5.903540134429932, 'learning_rate': 0.00019974818643904142, 'epoch': 0.8}\n","{'loss': 4.6383, 'lr': 0.00019974818643904142, 'epoch': 0.8}\n","{'loss': 4.165, 'lr': 0.00019972919673855283, 'epoch': 0.82}\n","{'loss': 3.8515, 'lr': 0.00019970951777341397, 'epoch': 0.85}\n","{'loss': 3.8239, 'lr': 0.00019968914967963337, 'epoch': 0.88}\n","{'loss': 4.0876, 'lr': 0.00019966809259798227, 'epoch': 0.9}\n","{'loss': 3.9172, 'lr': 0.0001996463466739938, 'epoch': 0.93}\n","{'loss': 3.8181, 'lr': 0.000199623912057962, 'epoch': 0.95}\n","{'loss': 3.4808, 'lr': 0.00019960078890494053, 'epoch': 0.97}\n","{'loss': 3.9728, 'grad_norm': 4.398442268371582, 'learning_rate': 0.00019957697737474196, 'epoch': 1.0}\n","  3% 40/1200 [00:10<04:35,  4.21it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 3.8416, 'lr': 0.00019957697737474196, 'epoch': 1.0}\n","{'loss': 4.1397, 'lr': 0.00019955247763193637, 'epoch': 1.02}\n","{'loss': 4.0412, 'lr': 0.0001995272898458504, 'epoch': 1.05}\n","{'loss': 3.3355, 'lr': 0.00019950141419056597, 'epoch': 1.07}\n","{'loss': 4.2409, 'lr': 0.0001994748508449191, 'epoch': 1.1}\n","{'loss': 3.3095, 'lr': 0.00019944759999249872, 'epoch': 1.12}\n","{'loss': 3.2951, 'lr': 0.00019941966182164533, 'epoch': 1.15}\n","{'loss': 3.2367, 'lr': 0.00019939103652544984, 'epoch': 1.18}\n","{'loss': 3.68, 'grad_norm': 3.615325689315796, 'learning_rate': 0.00019936172430175196, 'epoch': 1.2}\n","{'loss': 3.3965, 'lr': 0.00019936172430175196, 'epoch': 1.2}\n","{'loss': 3.4435, 'lr': 0.00019933172535313914, 'epoch': 1.23}\n","{'loss': 3.4792, 'lr': 0.000199301039886945, 'epoch': 1.25}\n","{'loss': 3.0676, 'lr': 0.0001992696681152479, 'epoch': 1.27}\n","{'loss': 2.8852, 'lr': 0.00019923761025486956, 'epoch': 1.3}\n","{'loss': 3.1028, 'lr': 0.00019920486652737348, 'epoch': 1.32}\n","{'loss': 3.5143, 'lr': 0.00019917143715906345, 'epoch': 1.35}\n","{'loss': 3.1003, 'lr': 0.0001991373223809819, 'epoch': 1.38}\n","{'loss': 3.2487, 'grad_norm': 3.675260305404663, 'learning_rate': 0.0001991025224289085, 'epoch': 1.4}\n","{'loss': 3.2252, 'lr': 0.0001991025224289085, 'epoch': 1.4}\n","{'loss': 2.9862, 'lr': 0.00019906703754335832, 'epoch': 1.43}\n","{'loss': 3.7942, 'lr': 0.0001990308679695802, 'epoch': 1.45}\n","{'loss': 2.9826, 'lr': 0.00019899401395755533, 'epoch': 1.48}\n","{'loss': 3.3939, 'lr': 0.00019895647576199506, 'epoch': 1.5}\n","{'loss': 2.8655, 'lr': 0.00019891825364233947, 'epoch': 1.52}\n","{'loss': 3.1201, 'lr': 0.00019887934786275553, 'epoch': 1.55}\n","{'loss': 3.19, 'lr': 0.00019883975869213516, 'epoch': 1.57}\n","{'loss': 3.1947, 'grad_norm': 3.166445255279541, 'learning_rate': 0.00019879948640409348, 'epoch': 1.6}\n","{'loss': 3.3251, 'lr': 0.00019879948640409348, 'epoch': 1.6}\n","{'loss': 3.9085, 'lr': 0.00019875853127696692, 'epoch': 1.62}\n","{'loss': 3.3263, 'lr': 0.00019871689359381116, 'epoch': 1.65}\n","{'loss': 2.7515, 'lr': 0.0001986745736423994, 'epoch': 1.68}\n","{'loss': 3.1248, 'lr': 0.0001986315717152201, 'epoch': 1.7}\n","{'loss': 3.0151, 'lr': 0.0001985878881094752, 'epoch': 1.73}\n","{'loss': 3.2891, 'lr': 0.00019854352312707798, 'epoch': 1.75}\n","{'loss': 2.4661, 'lr': 0.00019849847707465088, 'epoch': 1.77}\n","{'loss': 3.1508, 'grad_norm': 6.960079669952393, 'learning_rate': 0.0001984527502635235, 'epoch': 1.8}\n","{'loss': 3.2759, 'lr': 0.0001984527502635235, 'epoch': 1.8}\n","{'loss': 3.3512, 'lr': 0.0001984063430097305, 'epoch': 1.82}\n","{'loss': 3.1363, 'lr': 0.00019835925563400914, 'epoch': 1.85}\n","{'loss': 2.8593, 'lr': 0.0001983114884617974, 'epoch': 1.88}\n","{'loss': 2.7504, 'lr': 0.00019826304182323155, 'epoch': 1.9}\n","{'loss': 2.5876, 'lr': 0.00019821391605314383, 'epoch': 1.93}\n","{'loss': 2.9414, 'lr': 0.00019816411149106028, 'epoch': 1.95}\n","{'loss': 3.5645, 'lr': 0.00019811362848119832, 'epoch': 1.98}\n","{'loss': 3.0583, 'grad_norm': 3.9066669940948486, 'learning_rate': 0.0001980624673724643, 'epoch': 2.0}\n","  7% 80/1200 [00:22<04:40,  3.99it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 2.9789, 'lr': 0.0001980624673724643, 'epoch': 2.0}\n","{'loss': 3.0848, 'lr': 0.00019801062851845122, 'epoch': 2.02}\n","{'loss': 2.4712, 'lr': 0.00019795811227743619, 'epoch': 2.05}\n","{'loss': 2.3705, 'lr': 0.000197904919012378, 'epoch': 2.08}\n","{'loss': 2.3724, 'lr': 0.0001978510490909146, 'epoch': 2.1}\n","{'loss': 2.9931, 'lr': 0.00019779650288536058, 'epoch': 2.12}\n","{'loss': 2.7344, 'lr': 0.00019774128077270451, 'epoch': 2.15}\n","{'loss': 2.7998, 'lr': 0.00019768538313460647, 'epoch': 2.17}\n","{'loss': 2.7256, 'grad_norm': 4.165497779846191, 'learning_rate': 0.00019762881035739532, 'epoch': 2.2}\n","{'loss': 2.636, 'lr': 0.00019762881035739532, 'epoch': 2.2}\n","{'loss': 2.2056, 'lr': 0.00019757156283206598, 'epoch': 2.23}\n","{'loss': 2.6989, 'lr': 0.00019751364095427692, 'epoch': 2.25}\n","{'loss': 2.4313, 'lr': 0.00019745504512434724, 'epoch': 2.27}\n","{'loss': 2.338, 'lr': 0.00019739577574725395, 'epoch': 2.3}\n","{'loss': 2.2544, 'lr': 0.00019733583323262925, 'epoch': 2.33}\n","{'loss': 2.5522, 'lr': 0.00019727521799475754, 'epoch': 2.35}\n","{'loss': 2.1379, 'lr': 0.00019721393045257277, 'epoch': 2.38}\n","{'loss': 2.4068, 'grad_norm': 3.413935422897339, 'learning_rate': 0.00019715197102965535, 'epoch': 2.4}\n","{'loss': 2.3077, 'lr': 0.00019715197102965535, 'epoch': 2.4}\n","{'loss': 2.8475, 'lr': 0.0001970893401542293, 'epoch': 2.42}\n","{'loss': 1.9576, 'lr': 0.00019702603825915934, 'epoch': 2.45}\n","{'loss': 2.3086, 'lr': 0.00019696206578194786, 'epoch': 2.48}\n","{'loss': 3.4226, 'lr': 0.00019689742316473182, 'epoch': 2.5}\n","{'loss': 2.2585, 'lr': 0.00019683211085427975, 'epoch': 2.52}\n","{'loss': 2.5726, 'lr': 0.0001967661293019888, 'epoch': 2.55}\n","{'loss': 2.1808, 'lr': 0.00019669947896388136, 'epoch': 2.58}\n","{'loss': 2.482, 'grad_norm': 3.7493369579315186, 'learning_rate': 0.00019663216030060208, 'epoch': 2.6}\n","{'loss': 2.1155, 'lr': 0.00019663216030060208, 'epoch': 2.6}\n","{'loss': 1.9652, 'lr': 0.0001965641737774147, 'epoch': 2.62}\n","{'loss': 2.3125, 'lr': 0.0001964955198641987, 'epoch': 2.65}\n","{'loss': 2.6397, 'lr': 0.00019642619903544623, 'epoch': 2.67}\n","{'loss': 2.6593, 'lr': 0.00019635621177025857, 'epoch': 2.7}\n","{'loss': 2.6256, 'lr': 0.00019628555855234322, 'epoch': 2.73}\n","{'loss': 2.7638, 'lr': 0.00019621423987001014, 'epoch': 2.75}\n","{'loss': 2.2701, 'lr': 0.0001961422562161686, 'epoch': 2.77}\n","{'loss': 2.419, 'grad_norm': 3.3878746032714844, 'learning_rate': 0.00019606960808832375, 'epoch': 2.8}\n","{'loss': 2.3319, 'lr': 0.00019606960808832375, 'epoch': 2.8}\n","{'loss': 2.1177, 'lr': 0.00019599629598857315, 'epoch': 2.83}\n","{'loss': 2.3764, 'lr': 0.00019592232042360333, 'epoch': 2.85}\n","{'loss': 1.9089, 'lr': 0.00019584768190468625, 'epoch': 2.88}\n","{'loss': 2.1994, 'lr': 0.00019577238094767573, 'epoch': 2.9}\n","{'loss': 2.3364, 'lr': 0.00019569641807300406, 'epoch': 2.92}\n","{'loss': 2.2429, 'lr': 0.0001956197938056782, 'epoch': 2.95}\n","{'loss': 2.1595, 'lr': 0.00019554250867527624, 'epoch': 2.98}\n","{'loss': 2.2091, 'grad_norm': 4.2732367515563965, 'learning_rate': 0.00019546456321594376, 'epoch': 3.0}\n"," 10% 120/1200 [00:33<04:18,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.948, 'lr': 0.00019546456321594376, 'epoch': 3.0}\n","{'loss': 2.5798, 'lr': 0.00019538595796639007, 'epoch': 3.02}\n","{'loss': 1.6026, 'lr': 0.0001953066934698846, 'epoch': 3.05}\n","{'loss': 1.8244, 'lr': 0.000195226770274253, 'epoch': 3.08}\n","{'loss': 1.8387, 'lr': 0.00019514618893187353, 'epoch': 3.1}\n","{'loss': 1.6488, 'lr': 0.00019506494999967298, 'epoch': 3.12}\n","{'loss': 1.9218, 'lr': 0.00019498305403912314, 'epoch': 3.15}\n","{'loss': 1.8959, 'lr': 0.0001949005016162367, 'epoch': 3.17}\n","{'loss': 1.9075, 'grad_norm': 5.190861701965332, 'learning_rate': 0.00019481729330156334, 'epoch': 3.2}\n","{'loss': 1.6651, 'lr': 0.00019481729330156334, 'epoch': 3.2}\n","{'loss': 2.0041, 'lr': 0.00019473342967018593, 'epoch': 3.23}\n","{'loss': 1.5034, 'lr': 0.00019464891130171647, 'epoch': 3.25}\n","{'loss': 1.6523, 'lr': 0.00019456373878029202, 'epoch': 3.27}\n","{'loss': 2.0241, 'lr': 0.0001944779126945708, 'epoch': 3.3}\n","{'loss': 1.5617, 'lr': 0.00019439143363772802, 'epoch': 3.33}\n","{'loss': 1.5682, 'lr': 0.00019430430220745176, 'epoch': 3.35}\n","{'loss': 1.7027, 'lr': 0.000194216519005939, 'epoch': 3.38}\n","{'loss': 1.7102, 'grad_norm': 4.156645774841309, 'learning_rate': 0.00019412808463989121, 'epoch': 3.4}\n","{'loss': 2.0093, 'lr': 0.00019412808463989121, 'epoch': 3.4}\n","{'loss': 1.8858, 'lr': 0.00019403899972051045, 'epoch': 3.42}\n","{'loss': 1.6224, 'lr': 0.00019394926486349486, 'epoch': 3.45}\n","{'loss': 1.8179, 'lr': 0.0001938588806890346, 'epoch': 3.48}\n","{'loss': 1.4979, 'lr': 0.00019376784782180746, 'epoch': 3.5}\n","{'loss': 1.8369, 'lr': 0.00019367616689097467, 'epoch': 3.52}\n","{'loss': 2.2003, 'lr': 0.00019358383853017628, 'epoch': 3.55}\n","{'loss': 1.7911, 'lr': 0.00019349086337752715, 'epoch': 3.58}\n","{'loss': 1.8327, 'grad_norm': 6.088191032409668, 'learning_rate': 0.00019339724207561231, 'epoch': 3.6}\n","{'loss': 2.183, 'lr': 0.00019339724207561231, 'epoch': 3.6}\n","{'loss': 2.2337, 'lr': 0.00019330297527148246, 'epoch': 3.62}\n","{'loss': 1.9694, 'lr': 0.00019320806361664973, 'epoch': 3.65}\n","{'loss': 2.3281, 'lr': 0.000193112507767083, 'epoch': 3.67}\n","{'loss': 1.9387, 'lr': 0.00019301630838320332, 'epoch': 3.7}\n","{'loss': 2.1067, 'lr': 0.00019291946612987962, 'epoch': 3.73}\n","{'loss': 2.1535, 'lr': 0.0001928219816764238, 'epoch': 3.75}\n","{'loss': 1.9584, 'lr': 0.00019272385569658627, 'epoch': 3.77}\n","{'loss': 2.109, 'grad_norm': 4.156242847442627, 'learning_rate': 0.00019262508886855125, 'epoch': 3.8}\n","{'loss': 1.9903, 'lr': 0.00019262508886855125, 'epoch': 3.8}\n","{'loss': 1.879, 'lr': 0.00019252568187493215, 'epoch': 3.83}\n","{'loss': 1.7028, 'lr': 0.00019242563540276675, 'epoch': 3.85}\n","{'loss': 1.9652, 'lr': 0.00019232495014351246, 'epoch': 3.88}\n","{'loss': 2.085, 'lr': 0.0001922236267930417, 'epoch': 3.9}\n","{'loss': 1.6255, 'lr': 0.00019212166605163682, 'epoch': 3.92}\n","{'loss': 1.8739, 'lr': 0.00019201906862398551, 'epoch': 3.95}\n","{'loss': 1.8687, 'lr': 0.00019191583521917585, 'epoch': 3.98}\n","{'loss': 1.8738, 'grad_norm': 3.1298391819000244, 'learning_rate': 0.00019181196655069127, 'epoch': 4.0}\n"," 13% 160/1200 [00:45<03:58,  4.36it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.5732, 'lr': 0.00019181196655069127, 'epoch': 4.0}\n","{'loss': 1.4837, 'lr': 0.0001917074633364059, 'epoch': 4.03}\n","{'loss': 1.2853, 'lr': 0.00019160232629857925, 'epoch': 4.05}\n","{'loss': 1.4034, 'lr': 0.0001914965561638516, 'epoch': 4.08}\n","{'loss': 1.6182, 'lr': 0.00019139015366323868, 'epoch': 4.1}\n","{'loss': 1.8082, 'lr': 0.00019128311953212678, 'epoch': 4.12}\n","{'loss': 1.4542, 'lr': 0.0001911754545102676, 'epoch': 4.15}\n","{'loss': 1.446, 'lr': 0.00019106715934177315, 'epoch': 4.17}\n","{'loss': 1.509, 'grad_norm': 2.961940050125122, 'learning_rate': 0.00019095823477511062, 'epoch': 4.2}\n","{'loss': 1.597, 'lr': 0.00019095823477511062, 'epoch': 4.2}\n","{'loss': 1.5937, 'lr': 0.00019084868156309717, 'epoch': 4.22}\n","{'loss': 1.4449, 'lr': 0.00019073850046289484, 'epoch': 4.25}\n","{'loss': 2.14, 'lr': 0.0001906276922360051, 'epoch': 4.28}\n","{'loss': 1.6842, 'lr': 0.0001905162576482639, 'epoch': 4.3}\n","{'loss': 1.4604, 'lr': 0.000190404197469836, 'epoch': 4.33}\n","{'loss': 1.3912, 'lr': 0.00019029151247520998, 'epoch': 4.35}\n","{'loss': 1.8561, 'lr': 0.0001901782034431927, 'epoch': 4.38}\n","{'loss': 1.6459, 'grad_norm': 4.317859649658203, 'learning_rate': 0.00019006427115690397, 'epoch': 4.4}\n","{'loss': 1.6009, 'lr': 0.00019006427115690397, 'epoch': 4.4}\n","{'loss': 1.5682, 'lr': 0.00018994971640377112, 'epoch': 4.42}\n","{'loss': 1.3021, 'lr': 0.00018983453997552368, 'epoch': 4.45}\n","{'loss': 1.7921, 'lr': 0.00018971874266818762, 'epoch': 4.47}\n","{'loss': 1.4544, 'lr': 0.00018960232528208022, 'epoch': 4.5}\n","{'loss': 1.4778, 'lr': 0.0001894852886218042, 'epoch': 4.53}\n","{'loss': 1.4012, 'lr': 0.00018936763349624238, 'epoch': 4.55}\n","{'loss': 1.5707, 'lr': 0.000189249360718552, 'epoch': 4.58}\n","{'loss': 1.5209, 'grad_norm': 3.3487234115600586, 'learning_rate': 0.00018913047110615916, 'epoch': 4.6}\n","{'loss': 1.8146, 'lr': 0.00018913047110615916, 'epoch': 4.6}\n","{'loss': 1.6686, 'lr': 0.00018901096548075305, 'epoch': 4.62}\n","{'loss': 1.311, 'lr': 0.00018889084466828043, 'epoch': 4.65}\n","{'loss': 1.6928, 'lr': 0.00018877010949893975, 'epoch': 4.67}\n","{'loss': 1.5847, 'lr': 0.00018864876080717561, 'epoch': 4.7}\n","{'loss': 1.8257, 'lr': 0.0001885267994316728, 'epoch': 4.72}\n","{'loss': 1.9291, 'lr': 0.00018840422621535066, 'epoch': 4.75}\n","{'loss': 1.4568, 'lr': 0.0001882810420053571, 'epoch': 4.78}\n","{'loss': 1.6604, 'grad_norm': 3.9738759994506836, 'learning_rate': 0.00018815724765306287, 'epoch': 4.8}\n","{'loss': 1.4797, 'lr': 0.00018815724765306287, 'epoch': 4.8}\n","{'loss': 1.664, 'lr': 0.00018803284401405564, 'epoch': 4.83}\n","{'loss': 1.659, 'lr': 0.00018790783194813412, 'epoch': 4.85}\n","{'loss': 1.7555, 'lr': 0.00018778221231930203, 'epoch': 4.88}\n","{'loss': 1.6413, 'lr': 0.00018765598599576215, 'epoch': 4.9}\n","{'loss': 1.6087, 'lr': 0.00018752915384991043, 'epoch': 4.92}\n","{'loss': 1.4948, 'lr': 0.00018740171675832982, 'epoch': 4.95}\n","{'loss': 1.566, 'lr': 0.00018727367560178424, 'epoch': 4.97}\n","{'loss': 1.6086, 'grad_norm': 3.8684003353118896, 'learning_rate': 0.0001871450312652126, 'epoch': 5.0}\n"," 17% 200/1200 [00:56<03:58,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.2401, 'lr': 0.0001871450312652126, 'epoch': 5.0}\n","{'loss': 1.219, 'lr': 0.0001870157846377226, 'epoch': 5.03}\n","{'loss': 1.3683, 'lr': 0.0001868859366125845, 'epoch': 5.05}\n","{'loss': 1.0969, 'lr': 0.0001867554880872251, 'epoch': 5.08}\n","{'loss': 1.2639, 'lr': 0.0001866244399632215, 'epoch': 5.1}\n","{'loss': 1.2425, 'lr': 0.00018649279314629483, 'epoch': 5.12}\n","{'loss': 1.3575, 'lr': 0.00018636054854630395, 'epoch': 5.15}\n","{'loss': 1.1625, 'lr': 0.00018622770707723928, 'epoch': 5.17}\n","{'loss': 1.2438, 'grad_norm': 3.858201265335083, 'learning_rate': 0.00018609426965721638, 'epoch': 5.2}\n","{'loss': 1.2195, 'lr': 0.00018609426965721638, 'epoch': 5.2}\n","{'loss': 1.1465, 'lr': 0.00018596023720846968, 'epoch': 5.22}\n","{'loss': 1.5201, 'lr': 0.00018582561065734604, 'epoch': 5.25}\n","{'loss': 1.2972, 'lr': 0.00018569039093429835, 'epoch': 5.28}\n","{'loss': 1.6713, 'lr': 0.0001855545789738792, 'epoch': 5.3}\n","{'loss': 1.4104, 'lr': 0.00018541817571473429, 'epoch': 5.33}\n","{'loss': 1.7069, 'lr': 0.00018528118209959599, 'epoch': 5.35}\n","{'loss': 1.6942, 'lr': 0.0001851435990752769, 'epoch': 5.38}\n","{'loss': 1.4582, 'grad_norm': 4.800097942352295, 'learning_rate': 0.00018500542759266323, 'epoch': 5.4}\n","{'loss': 1.6697, 'lr': 0.00018500542759266323, 'epoch': 5.4}\n","{'loss': 1.3224, 'lr': 0.00018486666860670817, 'epoch': 5.42}\n","{'loss': 1.3653, 'lr': 0.0001847273230764254, 'epoch': 5.45}\n","{'loss': 1.0991, 'lr': 0.0001845873919648824, 'epoch': 5.47}\n","{'loss': 1.2355, 'lr': 0.00018444687623919386, 'epoch': 5.5}\n","{'loss': 1.2399, 'lr': 0.00018430577687051494, 'epoch': 5.53}\n","{'loss': 1.2706, 'lr': 0.00018416409483403454, 'epoch': 5.55}\n","{'loss': 1.3871, 'lr': 0.00018402183110896857, 'epoch': 5.58}\n","{'loss': 1.3237, 'grad_norm': 3.045224905014038, 'learning_rate': 0.0001838789866785533, 'epoch': 5.6}\n","{'loss': 1.5066, 'lr': 0.0001838789866785533, 'epoch': 5.6}\n","{'loss': 1.9462, 'lr': 0.0001837355625300383, 'epoch': 5.62}\n","{'loss': 1.8228, 'lr': 0.00018359155965468, 'epoch': 5.65}\n","{'loss': 1.3781, 'lr': 0.00018344697904773444, 'epoch': 5.67}\n","{'loss': 1.4364, 'lr': 0.0001833018217084506, 'epoch': 5.7}\n","{'loss': 1.2349, 'lr': 0.00018315608864006354, 'epoch': 5.72}\n","{'loss': 1.5489, 'lr': 0.00018300978084978735, 'epoch': 5.75}\n","{'loss': 1.5716, 'lr': 0.00018286289934880826, 'epoch': 5.78}\n","{'loss': 1.5557, 'grad_norm': 5.099490165710449, 'learning_rate': 0.00018271544515227755, 'epoch': 5.8}\n","{'loss': 1.2865, 'lr': 0.00018271544515227755, 'epoch': 5.8}\n","{'loss': 1.1815, 'lr': 0.00018256741927930462, 'epoch': 5.83}\n","{'loss': 1.5738, 'lr': 0.0001824188227529501, 'epoch': 5.85}\n","{'loss': 1.4722, 'lr': 0.00018226965660021836, 'epoch': 5.88}\n","{'loss': 1.4704, 'lr': 0.0001821199218520508, 'epoch': 5.9}\n","{'loss': 1.3368, 'lr': 0.00018196961954331862, 'epoch': 5.92}\n","{'loss': 1.3226, 'lr': 0.00018181875071281555, 'epoch': 5.95}\n","{'loss': 1.2465, 'lr': 0.00018166731640325084, 'epoch': 5.97}\n","{'loss': 1.3613, 'grad_norm': 3.8720903396606445, 'learning_rate': 0.00018151531766124186, 'epoch': 6.0}\n"," 20% 240/1200 [01:07<03:48,  4.21it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.2584, 'lr': 0.00018151531766124186, 'epoch': 6.0}\n","{'loss': 1.2734, 'lr': 0.0001813627555373071, 'epoch': 6.03}\n","{'loss': 1.1047, 'lr': 0.00018120963108585872, 'epoch': 6.05}\n","{'loss': 1.1213, 'lr': 0.0001810559453651954, 'epoch': 6.08}\n","{'loss': 1.1838, 'lr': 0.00018090169943749476, 'epoch': 6.1}\n","{'loss': 0.9977, 'lr': 0.00018074689436880644, 'epoch': 6.12}\n","{'loss': 1.1719, 'lr': 0.00018059153122904442, 'epoch': 6.15}\n","{'loss': 1.1064, 'lr': 0.00018043561109197967, 'epoch': 6.17}\n","{'loss': 1.1522, 'grad_norm': 2.4971351623535156, 'learning_rate': 0.00018027913503523287, 'epoch': 6.2}\n","{'loss': 1.3037, 'lr': 0.00018027913503523287, 'epoch': 6.2}\n","{'loss': 1.1655, 'lr': 0.00018012210414026676, 'epoch': 6.22}\n","{'loss': 1.1574, 'lr': 0.0001799645194923788, 'epoch': 6.25}\n","{'loss': 1.0935, 'lr': 0.00017980638218069366, 'epoch': 6.28}\n","{'loss': 1.0581, 'lr': 0.00017964769329815568, 'epoch': 6.3}\n","{'loss': 1.2217, 'lr': 0.00017948845394152123, 'epoch': 6.33}\n","{'loss': 1.2445, 'lr': 0.0001793286652113513, 'epoch': 6.35}\n","{'loss': 1.2285, 'lr': 0.00017916832821200375, 'epoch': 6.38}\n","{'loss': 1.1841, 'grad_norm': 3.38478946685791, 'learning_rate': 0.00017900744405162578, 'epoch': 6.4}\n","{'loss': 1.3097, 'lr': 0.00017900744405162578, 'epoch': 6.4}\n","{'loss': 1.1783, 'lr': 0.00017884601384214607, 'epoch': 6.42}\n","{'loss': 1.6251, 'lr': 0.00017868403869926748, 'epoch': 6.45}\n","{'loss': 1.3176, 'lr': 0.00017852151974245882, 'epoch': 6.47}\n","{'loss': 1.2696, 'lr': 0.00017835845809494768, 'epoch': 6.5}\n","{'loss': 1.0997, 'lr': 0.0001781948548837122, 'epoch': 6.53}\n","{'loss': 1.2108, 'lr': 0.0001780307112394735, 'epoch': 6.55}\n","{'loss': 1.2725, 'lr': 0.00017786602829668782, 'epoch': 6.58}\n","{'loss': 1.2854, 'grad_norm': 3.0583066940307617, 'learning_rate': 0.00017770080719353874, 'epoch': 6.6}\n","{'loss': 1.2076, 'lr': 0.00017770080719353874, 'epoch': 6.6}\n","{'loss': 1.2126, 'lr': 0.00017753504907192923, 'epoch': 6.62}\n","{'loss': 1.4218, 'lr': 0.00017736875507747379, 'epoch': 6.65}\n","{'loss': 1.1957, 'lr': 0.0001772019263594905, 'epoch': 6.67}\n","{'loss': 1.251, 'lr': 0.0001770345640709932, 'epoch': 6.7}\n","{'loss': 1.3898, 'lr': 0.0001768666693686833, 'epoch': 6.72}\n","{'loss': 1.1544, 'lr': 0.00017669824341294202, 'epoch': 6.75}\n","{'loss': 1.4768, 'lr': 0.0001765292873678222, 'epoch': 6.78}\n","{'loss': 1.2887, 'grad_norm': 3.437854051589966, 'learning_rate': 0.0001763598024010404, 'epoch': 6.8}\n","{'loss': 1.244, 'lr': 0.0001763598024010404, 'epoch': 6.8}\n","{'loss': 1.2729, 'lr': 0.00017618978968396865, 'epoch': 6.83}\n","{'loss': 1.2107, 'lr': 0.00017601925039162648, 'epoch': 6.85}\n","{'loss': 1.2058, 'lr': 0.00017584818570267284, 'epoch': 6.88}\n","{'loss': 1.0796, 'lr': 0.00017567659679939787, 'epoch': 6.9}\n","{'loss': 1.3074, 'lr': 0.00017550448486771466, 'epoch': 6.92}\n","{'loss': 1.2018, 'lr': 0.0001753318510971512, 'epoch': 6.95}\n","{'loss': 1.3212, 'lr': 0.00017515869668084208, 'epoch': 6.97}\n","{'loss': 1.2304, 'grad_norm': 10.652750015258789, 'learning_rate': 0.0001749850228155203, 'epoch': 7.0}\n"," 23% 280/1200 [01:19<03:40,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0787, 'lr': 0.0001749850228155203, 'epoch': 7.0}\n","{'loss': 1.1309, 'lr': 0.00017481083070150895, 'epoch': 7.03}\n","{'loss': 1.0827, 'lr': 0.0001746361215427129, 'epoch': 7.05}\n","{'loss': 1.217, 'lr': 0.00017446089654661048, 'epoch': 7.08}\n","{'loss': 1.0983, 'lr': 0.0001742851569242452, 'epoch': 7.1}\n","{'loss': 1.003, 'lr': 0.00017410890389021736, 'epoch': 7.12}\n","{'loss': 1.0691, 'lr': 0.0001739321386626756, 'epoch': 7.15}\n","{'loss': 1.1565, 'lr': 0.00017375486246330848, 'epoch': 7.17}\n","{'loss': 1.1045, 'grad_norm': 2.536456823348999, 'learning_rate': 0.00017357707651733618, 'epoch': 7.2}\n","{'loss': 1.176, 'lr': 0.00017357707651733618, 'epoch': 7.2}\n","{'loss': 1.0373, 'lr': 0.00017339878205350177, 'epoch': 7.22}\n","{'loss': 1.3961, 'lr': 0.000173219980304063, 'epoch': 7.25}\n","{'loss': 1.255, 'lr': 0.00017304067250478364, 'epoch': 7.28}\n","{'loss': 1.4781, 'lr': 0.0001728608598949249, 'epoch': 7.3}\n","{'loss': 1.2969, 'lr': 0.00017268054371723693, 'epoch': 7.33}\n","{'loss': 1.0594, 'lr': 0.00017249972521795023, 'epoch': 7.35}\n","{'loss': 1.0621, 'lr': 0.0001723184056467671, 'epoch': 7.38}\n","{'loss': 1.2201, 'grad_norm': 1.5785709619522095, 'learning_rate': 0.00017213658625685275, 'epoch': 7.4}\n","{'loss': 1.2705, 'lr': 0.00017213658625685275, 'epoch': 7.4}\n","{'loss': 1.083, 'lr': 0.00017195426830482705, 'epoch': 7.42}\n","{'loss': 1.2259, 'lr': 0.0001717714530507554, 'epoch': 7.45}\n","{'loss': 1.2713, 'lr': 0.00017158814175814033, 'epoch': 7.47}\n","{'loss': 1.1192, 'lr': 0.00017140433569391275, 'epoch': 7.5}\n","{'loss': 1.4281, 'lr': 0.00017122003612842296, 'epoch': 7.53}\n","{'loss': 1.0636, 'lr': 0.0001710352443354321, 'epoch': 7.55}\n","{'loss': 1.1845, 'lr': 0.00017084996159210332, 'epoch': 7.58}\n","{'loss': 1.2058, 'grad_norm': 2.3170504570007324, 'learning_rate': 0.00017066418917899284, 'epoch': 7.6}\n","{'loss': 1.074, 'lr': 0.00017066418917899284, 'epoch': 7.6}\n","{'loss': 1.0767, 'lr': 0.0001704779283800412, 'epoch': 7.62}\n","{'loss': 1.0516, 'lr': 0.00017029118048256435, 'epoch': 7.65}\n","{'loss': 1.0758, 'lr': 0.0001701039467772447, 'epoch': 7.67}\n","{'loss': 1.0787, 'lr': 0.0001699162285581223, 'epoch': 7.7}\n","{'loss': 1.1834, 'lr': 0.00016972802712258585, 'epoch': 7.72}\n","{'loss': 1.1662, 'lr': 0.00016953934377136377, 'epoch': 7.75}\n","{'loss': 1.148, 'lr': 0.00016935017980851508, 'epoch': 7.78}\n","{'loss': 1.1068, 'grad_norm': 2.340367555618286, 'learning_rate': 0.0001691605365414206, 'epoch': 7.8}\n","{'loss': 1.065, 'lr': 0.0001691605365414206, 'epoch': 7.8}\n","{'loss': 1.1375, 'lr': 0.00016897041528077372, 'epoch': 7.83}\n","{'loss': 1.2727, 'lr': 0.00016877981734057137, 'epoch': 7.85}\n","{'loss': 1.2485, 'lr': 0.00016858874403810509, 'epoch': 7.88}\n","{'loss': 1.2173, 'lr': 0.00016839719669395174, 'epoch': 7.9}\n","{'loss': 1.0171, 'lr': 0.00016820517663196455, 'epoch': 7.92}\n","{'loss': 1.5583, 'lr': 0.00016801268517926377, 'epoch': 7.95}\n","{'loss': 1.0843, 'lr': 0.00016781972366622765, 'epoch': 7.97}\n","{'loss': 1.2001, 'grad_norm': 5.767415523529053, 'learning_rate': 0.00016762629342648318, 'epoch': 8.0}\n"," 27% 320/1200 [01:31<03:46,  3.88it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0772, 'lr': 0.00016762629342648318, 'epoch': 8.0}\n","{'loss': 0.9694, 'lr': 0.00016743239579689699, 'epoch': 8.03}\n","{'loss': 1.3784, 'lr': 0.0001672380321175658, 'epoch': 8.05}\n","{'loss': 1.0579, 'lr': 0.00016704320373180754, 'epoch': 8.07}\n","{'loss': 1.1187, 'lr': 0.0001668479119861519, 'epoch': 8.1}\n","{'loss': 0.9257, 'lr': 0.0001666521582303309, 'epoch': 8.12}\n","{'loss': 1.0631, 'lr': 0.00016645594381726979, 'epoch': 8.15}\n","{'loss': 1.18, 'lr': 0.0001662592701030775, 'epoch': 8.18}\n","{'loss': 1.0963, 'grad_norm': 9.287618637084961, 'learning_rate': 0.00016606213844703744, 'epoch': 8.2}\n","{'loss': 1.1727, 'lr': 0.00016606213844703744, 'epoch': 8.2}\n","{'loss': 0.976, 'lr': 0.00016586455021159799, 'epoch': 8.22}\n","{'loss': 0.95, 'lr': 0.00016566650676236305, 'epoch': 8.25}\n","{'loss': 1.0495, 'lr': 0.0001654680094680828, 'epoch': 8.28}\n","{'loss': 1.1276, 'lr': 0.00016526905970064396, 'epoch': 8.3}\n","{'loss': 1.0873, 'lr': 0.00016506965883506065, 'epoch': 8.32}\n","{'loss': 1.124, 'lr': 0.00016486980824946445, 'epoch': 8.35}\n","{'loss': 1.1104, 'lr': 0.0001646695093250953, 'epoch': 8.38}\n","{'loss': 1.0747, 'grad_norm': 1.4624651670455933, 'learning_rate': 0.00016446876344629172, 'epoch': 8.4}\n","{'loss': 1.2697, 'lr': 0.00016446876344629172, 'epoch': 8.4}\n","{'loss': 1.1838, 'lr': 0.00016426757200048125, 'epoch': 8.43}\n","{'loss': 1.8733, 'lr': 0.00016406593637817101, 'epoch': 8.45}\n","{'loss': 0.984, 'lr': 0.00016386385797293783, 'epoch': 8.47}\n","{'loss': 1.4056, 'lr': 0.00016366133818141893, 'epoch': 8.5}\n","{'loss': 1.0413, 'lr': 0.000163458378403302, 'epoch': 8.53}\n","{'loss': 1.0245, 'lr': 0.00016325498004131575, 'epoch': 8.55}\n","{'loss': 1.0695, 'lr': 0.00016305114450121993, 'epoch': 8.57}\n","{'loss': 1.2314, 'grad_norm': 1.9297990798950195, 'learning_rate': 0.000162846873191796, 'epoch': 8.6}\n","{'loss': 1.0664, 'lr': 0.000162846873191796, 'epoch': 8.6}\n","{'loss': 0.9917, 'lr': 0.00016264216752483697, 'epoch': 8.62}\n","{'loss': 1.2101, 'lr': 0.000162437028915138, 'epoch': 8.65}\n","{'loss': 0.9844, 'lr': 0.00016223145878048643, 'epoch': 8.68}\n","{'loss': 0.996, 'lr': 0.000162025458541652, 'epoch': 8.7}\n","{'loss': 1.1973, 'lr': 0.00016181902962237706, 'epoch': 8.72}\n","{'loss': 1.1443, 'lr': 0.0001616121734493668, 'epoch': 8.75}\n","{'loss': 1.5285, 'lr': 0.00016140489145227918, 'epoch': 8.78}\n","{'loss': 1.1398, 'grad_norm': 16.534862518310547, 'learning_rate': 0.00016119718506371535, 'epoch': 8.8}\n","{'loss': 1.0681, 'lr': 0.00016119718506371535, 'epoch': 8.8}\n","{'loss': 1.2776, 'lr': 0.00016098905571920944, 'epoch': 8.82}\n","{'loss': 1.1222, 'lr': 0.00016078050485721882, 'epoch': 8.85}\n","{'loss': 0.9828, 'lr': 0.00016057153391911422, 'epoch': 8.88}\n","{'loss': 1.0801, 'lr': 0.0001603621443491695, 'epoch': 8.9}\n","{'loss': 1.1655, 'lr': 0.00016015233759455194, 'epoch': 8.93}\n","{'loss': 1.1097, 'lr': 0.00015994211510531216, 'epoch': 8.95}\n","{'loss': 1.1522, 'lr': 0.00015973147833437401, 'epoch': 8.97}\n","{'loss': 1.1198, 'grad_norm': 2.109744071960449, 'learning_rate': 0.0001595204287375246, 'epoch': 9.0}\n"," 30% 360/1200 [01:42<03:13,  4.33it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0332, 'lr': 0.0001595204287375246, 'epoch': 9.0}\n","{'loss': 1.1743, 'lr': 0.00015930896777340426, 'epoch': 9.03}\n","{'loss': 0.9978, 'lr': 0.0001590970969034964, 'epoch': 9.05}\n","{'loss': 1.0092, 'lr': 0.0001588848175921174, 'epoch': 9.07}\n","{'loss': 1.3137, 'lr': 0.00015867213130640667, 'epoch': 9.1}\n","{'loss': 0.9928, 'lr': 0.00015845903951631623, 'epoch': 9.12}\n","{'loss': 1.0064, 'lr': 0.00015824554369460068, 'epoch': 9.15}\n","{'loss': 0.9909, 'lr': 0.00015803164531680714, 'epoch': 9.18}\n","{'loss': 1.0648, 'grad_norm': 1.8061426877975464, 'learning_rate': 0.00015781734586126485, 'epoch': 9.2}\n","{'loss': 0.9742, 'lr': 0.00015781734586126485, 'epoch': 9.2}\n","{'loss': 0.9773, 'lr': 0.00015760264680907506, 'epoch': 9.22}\n","{'loss': 1.0971, 'lr': 0.00015738754964410084, 'epoch': 9.25}\n","{'loss': 1.0313, 'lr': 0.00015717205585295666, 'epoch': 9.28}\n","{'loss': 0.8682, 'lr': 0.00015695616692499833, 'epoch': 9.3}\n","{'loss': 1.1019, 'lr': 0.00015673988435231247, 'epoch': 9.32}\n","{'loss': 0.9291, 'lr': 0.0001565232096297064, 'epoch': 9.35}\n","{'loss': 1.1267, 'lr': 0.00015630614425469775, 'epoch': 9.38}\n","{'loss': 1.0132, 'grad_norm': 1.699741244316101, 'learning_rate': 0.00015608868972750403, 'epoch': 9.4}\n","{'loss': 1.05, 'lr': 0.00015608868972750403, 'epoch': 9.4}\n","{'loss': 1.1968, 'lr': 0.00015587084755103235, 'epoch': 9.43}\n","{'loss': 1.0875, 'lr': 0.00015565261923086902, 'epoch': 9.45}\n","{'loss': 0.9502, 'lr': 0.00015543400627526907, 'epoch': 9.47}\n","{'loss': 1.2056, 'lr': 0.00015521501019514597, 'epoch': 9.5}\n","{'loss': 1.0654, 'lr': 0.00015499563250406104, 'epoch': 9.53}\n","{'loss': 0.9282, 'lr': 0.00015477587471821304, 'epoch': 9.55}\n","{'loss': 1.0856, 'lr': 0.00015455573835642777, 'epoch': 9.57}\n","{'loss': 1.0712, 'grad_norm': 7.134257793426514, 'learning_rate': 0.00015433522494014753, 'epoch': 9.6}\n","{'loss': 1.0421, 'lr': 0.00015433522494014753, 'epoch': 9.6}\n","{'loss': 1.1231, 'lr': 0.00015411433599342038, 'epoch': 9.62}\n","{'loss': 1.0609, 'lr': 0.00015389307304289007, 'epoch': 9.65}\n","{'loss': 1.0578, 'lr': 0.00015367143761778503, 'epoch': 9.68}\n","{'loss': 0.9987, 'lr': 0.0001534494312499081, 'epoch': 9.7}\n","{'loss': 1.0301, 'lr': 0.0001532270554736258, 'epoch': 9.72}\n","{'loss': 1.1151, 'lr': 0.00015300431182585777, 'epoch': 9.75}\n","{'loss': 1.0621, 'lr': 0.00015278120184606617, 'epoch': 9.78}\n","{'loss': 1.0612, 'grad_norm': 1.2842090129852295, 'learning_rate': 0.000152557727076245, 'epoch': 9.8}\n","{'loss': 1.1147, 'lr': 0.000152557727076245, 'epoch': 9.8}\n","{'loss': 1.3787, 'lr': 0.00015233388906090939, 'epoch': 9.82}\n","{'loss': 1.5222, 'lr': 0.0001521096893470851, 'epoch': 9.85}\n","{'loss': 1.0357, 'lr': 0.00015188512948429765, 'epoch': 9.88}\n","{'loss': 1.0664, 'lr': 0.00015166021102456172, 'epoch': 9.9}\n","{'loss': 1.003, 'lr': 0.00015143493552237033, 'epoch': 9.93}\n","{'loss': 1.0867, 'lr': 0.0001512093045346842, 'epoch': 9.95}\n","{'loss': 1.0104, 'lr': 0.00015098331962092097, 'epoch': 9.97}\n","{'loss': 1.1522, 'grad_norm': 1.451741337776184, 'learning_rate': 0.00015075698234294423, 'epoch': 10.0}\n"," 33% 400/1200 [01:54<03:18,  4.04it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0678, 'lr': 0.00015075698234294423, 'epoch': 10.0}\n","{'loss': 1.0159, 'lr': 0.0001505302942650531, 'epoch': 10.03}\n","{'loss': 0.9783, 'lr': 0.00015030325695397108, 'epoch': 10.05}\n","{'loss': 0.9819, 'lr': 0.00015007587197883533, 'epoch': 10.07}\n","{'loss': 1.0777, 'lr': 0.00014984814091118594, 'epoch': 10.1}\n","{'loss': 0.9333, 'lr': 0.00014962006532495488, 'epoch': 10.12}\n","{'loss': 1.2808, 'lr': 0.00014939164679645528, 'epoch': 10.15}\n","{'loss': 1.0063, 'lr': 0.0001491628869043704, 'epoch': 10.18}\n","{'loss': 1.0427, 'grad_norm': 1.2266489267349243, 'learning_rate': 0.00014893378722974284, 'epoch': 10.2}\n","{'loss': 0.9776, 'lr': 0.00014893378722974284, 'epoch': 10.2}\n","{'loss': 1.1322, 'lr': 0.0001487043493559636, 'epoch': 10.22}\n","{'loss': 0.9551, 'lr': 0.00014847457486876097, 'epoch': 10.25}\n","{'loss': 0.9502, 'lr': 0.0001482444653561898, 'epoch': 10.28}\n","{'loss': 0.9187, 'lr': 0.00014801402240862034, 'epoch': 10.3}\n","{'loss': 0.9824, 'lr': 0.0001477832476187275, 'epoch': 10.32}\n","{'loss': 1.0798, 'lr': 0.00014755214258147943, 'epoch': 10.35}\n","{'loss': 1.4271, 'lr': 0.00014732070889412693, 'epoch': 10.38}\n","{'loss': 1.0529, 'grad_norm': 13.18154525756836, 'learning_rate': 0.00014708894815619212, 'epoch': 10.4}\n","{'loss': 0.9598, 'lr': 0.00014708894815619212, 'epoch': 10.4}\n","{'loss': 1.0223, 'lr': 0.0001468568619694576, 'epoch': 10.43}\n","{'loss': 0.9651, 'lr': 0.00014662445193795506, 'epoch': 10.45}\n","{'loss': 0.994, 'lr': 0.0001463917196679546, 'epoch': 10.47}\n","{'loss': 0.9824, 'lr': 0.00014615866676795334, 'epoch': 10.5}\n","{'loss': 1.0657, 'lr': 0.0001459252948486644, 'epoch': 10.53}\n","{'loss': 0.9936, 'lr': 0.00014569160552300574, 'epoch': 10.55}\n","{'loss': 1.0506, 'lr': 0.00014545760040608904, 'epoch': 10.57}\n","{'loss': 1.0042, 'grad_norm': 1.1787070035934448, 'learning_rate': 0.00014522328111520857, 'epoch': 10.6}\n","{'loss': 0.9951, 'lr': 0.00014522328111520857, 'epoch': 10.6}\n","{'loss': 1.0989, 'lr': 0.00014498864926982996, 'epoch': 10.62}\n","{'loss': 1.0611, 'lr': 0.0001447537064915789, 'epoch': 10.65}\n","{'loss': 1.0123, 'lr': 0.00014451845440423025, 'epoch': 10.68}\n","{'loss': 0.9313, 'lr': 0.00014428289463369644, 'epoch': 10.7}\n","{'loss': 1.008, 'lr': 0.00014404702880801648, 'epoch': 10.72}\n","{'loss': 0.9601, 'lr': 0.00014381085855734468, 'epoch': 10.75}\n","{'loss': 1.0785, 'lr': 0.00014357438551393928, 'epoch': 10.78}\n","{'loss': 1.0182, 'grad_norm': 10.955737113952637, 'learning_rate': 0.00014333761131215126, 'epoch': 10.8}\n","{'loss': 1.0881, 'lr': 0.00014333761131215126, 'epoch': 10.8}\n","{'loss': 0.9337, 'lr': 0.000143100537588413, 'epoch': 10.82}\n","{'loss': 1.0289, 'lr': 0.000142863165981227, 'epoch': 10.85}\n","{'loss': 1.0391, 'lr': 0.0001426254981311545, 'epoch': 10.88}\n","{'loss': 0.9678, 'lr': 0.00014238753568080422, 'epoch': 10.9}\n","{'loss': 1.0495, 'lr': 0.000142149280274821, 'epoch': 10.93}\n","{'loss': 0.9572, 'lr': 0.00014191073355987436, 'epoch': 10.95}\n","{'loss': 1.089, 'lr': 0.00014167189718464714, 'epoch': 10.97}\n","{'loss': 1.0192, 'grad_norm': 1.7587077617645264, 'learning_rate': 0.00014143277279982414, 'epoch': 11.0}\n"," 37% 440/1200 [02:06<03:07,  4.05it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.901, 'lr': 0.00014143277279982414, 'epoch': 11.0}\n","{'loss': 0.8734, 'lr': 0.00014119336205808077, 'epoch': 11.03}\n","{'loss': 0.966, 'lr': 0.00014095366661407148, 'epoch': 11.05}\n","{'loss': 1.0066, 'lr': 0.0001407136881244184, 'epoch': 11.07}\n","{'loss': 0.9883, 'lr': 0.0001404734282476999, 'epoch': 11.1}\n","{'loss': 1.0878, 'lr': 0.00014023288864443916, 'epoch': 11.12}\n","{'loss': 0.952, 'lr': 0.0001399920709770926, 'epoch': 11.15}\n","{'loss': 1.0403, 'lr': 0.00013975097691003852, 'epoch': 11.18}\n","{'loss': 0.9769, 'grad_norm': 1.3758714199066162, 'learning_rate': 0.00013950960810956537, 'epoch': 11.2}\n","{'loss': 1.1351, 'lr': 0.00013950960810956537, 'epoch': 11.2}\n","{'loss': 0.9011, 'lr': 0.00013926796624386052, 'epoch': 11.22}\n","{'loss': 0.9453, 'lr': 0.0001390260529829986, 'epoch': 11.25}\n","{'loss': 0.991, 'lr': 0.0001387838699989298, 'epoch': 11.28}\n","{'loss': 0.9871, 'lr': 0.00013854141896546863, 'epoch': 11.3}\n","{'loss': 1.0004, 'lr': 0.00013829870155828209, 'epoch': 11.32}\n","{'loss': 1.0039, 'lr': 0.0001380557194548782, 'epoch': 11.35}\n","{'loss': 1.1049, 'lr': 0.00013781247433459449, 'epoch': 11.38}\n","{'loss': 1.0086, 'grad_norm': 6.907852649688721, 'learning_rate': 0.00013756896787858608, 'epoch': 11.4}\n","{'loss': 0.9322, 'lr': 0.00013756896787858608, 'epoch': 11.4}\n","{'loss': 1.0277, 'lr': 0.0001373252017698145, 'epoch': 11.43}\n","{'loss': 1.0818, 'lr': 0.0001370811776930357, 'epoch': 11.45}\n","{'loss': 1.1273, 'lr': 0.0001368368973347886, 'epoch': 11.47}\n","{'loss': 1.0739, 'lr': 0.0001365923623833834, 'epoch': 11.5}\n","{'loss': 0.9479, 'lr': 0.00013634757452888973, 'epoch': 11.53}\n","{'loss': 1.12, 'lr': 0.00013610253546312532, 'epoch': 11.55}\n","{'loss': 1.0245, 'lr': 0.00013585724687964403, 'epoch': 11.57}\n","{'loss': 1.0419, 'grad_norm': 1.1112693548202515, 'learning_rate': 0.00013561171047372418, 'epoch': 11.6}\n","{'loss': 0.8748, 'lr': 0.00013561171047372418, 'epoch': 11.6}\n","{'loss': 1.1164, 'lr': 0.00013536592794235696, 'epoch': 11.62}\n","{'loss': 1.1272, 'lr': 0.0001351199009842346, 'epoch': 11.65}\n","{'loss': 1.0615, 'lr': 0.0001348736312997386, 'epoch': 11.68}\n","{'loss': 0.9399, 'lr': 0.0001346271205909281, 'epoch': 11.7}\n","{'loss': 0.977, 'lr': 0.00013438037056152802, 'epoch': 11.72}\n","{'loss': 1.0252, 'lr': 0.00013413338291691726, 'epoch': 11.75}\n","{'loss': 1.0526, 'lr': 0.00013388615936411712, 'epoch': 11.78}\n","{'loss': 1.0218, 'grad_norm': 1.7302228212356567, 'learning_rate': 0.00013363870161177914, 'epoch': 11.8}\n","{'loss': 1.0185, 'lr': 0.00013363870161177914, 'epoch': 11.8}\n","{'loss': 0.9791, 'lr': 0.0001333910113701736, 'epoch': 11.82}\n","{'loss': 0.975, 'lr': 0.00013314309035117767, 'epoch': 11.85}\n","{'loss': 1.0974, 'lr': 0.00013289494026826336, 'epoch': 11.88}\n","{'loss': 0.9417, 'lr': 0.000132646562836486, 'epoch': 11.9}\n","{'loss': 1.0692, 'lr': 0.00013239795977247197, 'epoch': 11.93}\n","{'loss': 1.0191, 'lr': 0.0001321491327944073, 'epoch': 11.95}\n","{'loss': 1.0069, 'lr': 0.0001319000836220255, 'epoch': 11.97}\n","{'loss': 1.0134, 'grad_norm': 2.256408214569092, 'learning_rate': 0.00013165081397659563, 'epoch': 12.0}\n"," 40% 480/1200 [02:17<03:07,  3.85it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9143, 'lr': 0.00013165081397659563, 'epoch': 12.0}\n","{'loss': 0.9399, 'lr': 0.0001314013255809107, 'epoch': 12.03}\n","{'loss': 0.9004, 'lr': 0.0001311516201592755, 'epoch': 12.05}\n","{'loss': 1.0785, 'lr': 0.00013090169943749476, 'epoch': 12.07}\n","{'loss': 0.9292, 'lr': 0.0001306515651428612, 'epoch': 12.1}\n","{'loss': 0.9481, 'lr': 0.0001304012190041437, 'epoch': 12.12}\n","{'loss': 1.3045, 'lr': 0.00013015066275157518, 'epoch': 12.15}\n","{'loss': 1.0024, 'lr': 0.0001298998981168408, 'epoch': 12.18}\n","{'loss': 1.0022, 'grad_norm': 7.630790710449219, 'learning_rate': 0.00012964892683306592, 'epoch': 12.2}\n","{'loss': 0.959, 'lr': 0.00012964892683306592, 'epoch': 12.2}\n","{'loss': 0.8401, 'lr': 0.0001293977506348041, 'epoch': 12.22}\n","{'loss': 1.0079, 'lr': 0.00012914637125802512, 'epoch': 12.25}\n","{'loss': 0.9059, 'lr': 0.00012889479044010308, 'epoch': 12.28}\n","{'loss': 1.062, 'lr': 0.0001286430099198042, 'epoch': 12.3}\n","{'loss': 0.9156, 'lr': 0.000128391031437275, 'epoch': 12.32}\n","{'loss': 0.9119, 'lr': 0.00012813885673403015, 'epoch': 12.35}\n","{'loss': 0.9777, 'lr': 0.00012788648755294055, 'epoch': 12.38}\n","{'loss': 0.9475, 'grad_norm': 1.1310150623321533, 'learning_rate': 0.00012763392563822106, 'epoch': 12.4}\n","{'loss': 1.1357, 'lr': 0.00012763392563822106, 'epoch': 12.4}\n","{'loss': 1.0798, 'lr': 0.00012738117273541875, 'epoch': 12.43}\n","{'loss': 1.1575, 'lr': 0.00012712823059140055, 'epoch': 12.45}\n","{'loss': 1.0757, 'lr': 0.00012687510095434138, 'epoch': 12.47}\n","{'loss': 1.1166, 'lr': 0.00012662178557371198, 'epoch': 12.5}\n","{'loss': 0.9683, 'lr': 0.00012636828620026682, 'epoch': 12.53}\n","{'loss': 1.0428, 'lr': 0.000126114604586032, 'epoch': 12.55}\n","{'loss': 0.9571, 'lr': 0.00012586074248429317, 'epoch': 12.57}\n","{'loss': 1.0667, 'grad_norm': 4.385100364685059, 'learning_rate': 0.00012560670164958338, 'epoch': 12.6}\n","{'loss': 1.0591, 'lr': 0.00012560670164958338, 'epoch': 12.6}\n","{'loss': 1.0958, 'lr': 0.000125352483837671, 'epoch': 12.62}\n","{'loss': 0.9818, 'lr': 0.00012509809080554754, 'epoch': 12.65}\n","{'loss': 0.9654, 'lr': 0.00012484352431141552, 'epoch': 12.68}\n","{'loss': 1.0877, 'lr': 0.00012458878611467622, 'epoch': 12.7}\n","{'loss': 1.0289, 'lr': 0.00012433387797591782, 'epoch': 12.72}\n","{'loss': 0.9517, 'lr': 0.00012407880165690287, 'epoch': 12.75}\n","{'loss': 1.0383, 'lr': 0.00012382355892055636, 'epoch': 12.78}\n","{'loss': 1.0261, 'grad_norm': 1.0186928510665894, 'learning_rate': 0.00012356815153095337, 'epoch': 12.8}\n","{'loss': 0.9343, 'lr': 0.00012356815153095337, 'epoch': 12.8}\n","{'loss': 0.939, 'lr': 0.00012331258125330706, 'epoch': 12.82}\n","{'loss': 1.1057, 'lr': 0.00012305684985395625, 'epoch': 12.85}\n","{'loss': 1.0204, 'lr': 0.00012280095910035342, 'epoch': 12.88}\n","{'loss': 1.139, 'lr': 0.00012254491076105242, 'epoch': 12.9}\n","{'loss': 1.0381, 'lr': 0.00012228870660569615, 'epoch': 12.93}\n","{'loss': 0.9619, 'lr': 0.00012203234840500446, 'epoch': 12.95}\n","{'loss': 1.091, 'lr': 0.00012177583793076184, 'epoch': 12.97}\n","{'loss': 1.0287, 'grad_norm': 1.4894230365753174, 'learning_rate': 0.00012151917695580523, 'epoch': 13.0}\n"," 43% 520/1200 [02:30<02:56,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9823, 'lr': 0.00012151917695580523, 'epoch': 13.0}\n","{'loss': 1.0144, 'lr': 0.00012126236725401173, 'epoch': 13.03}\n","{'loss': 1.0041, 'lr': 0.00012100541060028631, 'epoch': 13.05}\n","{'loss': 0.9506, 'lr': 0.00012074830877054963, 'epoch': 13.07}\n","{'loss': 0.9068, 'lr': 0.00012049106354172566, 'epoch': 13.1}\n","{'loss': 1.2271, 'lr': 0.00012023367669172946, 'epoch': 13.12}\n","{'loss': 0.9885, 'lr': 0.00011997614999945499, 'epoch': 13.15}\n","{'loss': 0.8921, 'lr': 0.0001197184852447625, 'epoch': 13.18}\n","{'loss': 0.9957, 'grad_norm': 5.6626973152160645, 'learning_rate': 0.00011946068420846666, 'epoch': 13.2}\n","{'loss': 0.9677, 'lr': 0.00011946068420846666, 'epoch': 13.2}\n","{'loss': 0.9522, 'lr': 0.0001192027486723239, 'epoch': 13.22}\n","{'loss': 0.9352, 'lr': 0.0001189446804190203, 'epoch': 13.25}\n","{'loss': 0.8847, 'lr': 0.00011868648123215908, 'epoch': 13.28}\n","{'loss': 0.9103, 'lr': 0.00011842815289624853, 'epoch': 13.3}\n","{'loss': 1.0162, 'lr': 0.00011816969719668949, 'epoch': 13.32}\n","{'loss': 1.014, 'lr': 0.00011791111591976298, 'epoch': 13.35}\n","{'loss': 0.9195, 'lr': 0.00011765241085261802, 'epoch': 13.38}\n","{'loss': 0.95, 'grad_norm': 1.117492914199829, 'learning_rate': 0.0001173935837832592, 'epoch': 13.4}\n","{'loss': 0.9474, 'lr': 0.0001173935837832592, 'epoch': 13.4}\n","{'loss': 1.026, 'lr': 0.00011713463650053423, 'epoch': 13.43}\n","{'loss': 1.1629, 'lr': 0.00011687557079412171, 'epoch': 13.45}\n","{'loss': 1.0028, 'lr': 0.00011661638845451866, 'epoch': 13.47}\n","{'loss': 0.92, 'lr': 0.00011635709127302829, 'epoch': 13.5}\n","{'loss': 1.0028, 'lr': 0.00011609768104174743, 'epoch': 13.53}\n","{'loss': 0.9098, 'lr': 0.00011583815955355435, 'epoch': 13.55}\n","{'loss': 0.9132, 'lr': 0.00011557852860209608, 'epoch': 13.57}\n","{'loss': 0.9856, 'grad_norm': 0.9866473078727722, 'learning_rate': 0.00011531878998177635, 'epoch': 13.6}\n","{'loss': 0.9138, 'lr': 0.00011531878998177635, 'epoch': 13.6}\n","{'loss': 1.0312, 'lr': 0.00011505894548774294, 'epoch': 13.62}\n","{'loss': 1.0243, 'lr': 0.00011479899691587546, 'epoch': 13.65}\n","{'loss': 1.0005, 'lr': 0.00011453894606277271, 'epoch': 13.68}\n","{'loss': 0.9319, 'lr': 0.00011427879472574048, 'epoch': 13.7}\n","{'loss': 0.9538, 'lr': 0.000114018544702779, 'epoch': 13.72}\n","{'loss': 0.9635, 'lr': 0.00011375819779257057, 'epoch': 13.75}\n","{'loss': 0.9231, 'lr': 0.00011349775579446716, 'epoch': 13.78}\n","{'loss': 0.9678, 'grad_norm': 1.1245367527008057, 'learning_rate': 0.00011323722050847782, 'epoch': 13.8}\n","{'loss': 0.9465, 'lr': 0.00011323722050847782, 'epoch': 13.8}\n","{'loss': 1.0857, 'lr': 0.00011297659373525645, 'epoch': 13.82}\n","{'loss': 1.0246, 'lr': 0.00011271587727608917, 'epoch': 13.85}\n","{'loss': 1.1742, 'lr': 0.00011245507293288204, 'epoch': 13.88}\n","{'loss': 1.2442, 'lr': 0.00011219418250814841, 'epoch': 13.9}\n","{'loss': 0.9238, 'lr': 0.0001119332078049967, 'epoch': 13.93}\n","{'loss': 0.987, 'lr': 0.00011167215062711767, 'epoch': 13.95}\n","{'loss': 0.9616, 'lr': 0.00011141101277877222, 'epoch': 13.97}\n","{'loss': 1.0434, 'grad_norm': 1.005894660949707, 'learning_rate': 0.00011114979606477866, 'epoch': 14.0}\n"," 47% 560/1200 [02:42<02:47,  3.81it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9285, 'lr': 0.00011114979606477866, 'epoch': 14.0}\n","{'loss': 0.9332, 'lr': 0.00011088850229050052, 'epoch': 14.03}\n","{'loss': 1.0001, 'lr': 0.00011062713326183383, 'epoch': 14.05}\n","{'loss': 0.9457, 'lr': 0.0001103656907851947, 'epoch': 14.07}\n","{'loss': 0.9014, 'lr': 0.00011010417666750695, 'epoch': 14.1}\n","{'loss': 0.9116, 'lr': 0.00010984259271618947, 'epoch': 14.12}\n","{'loss': 0.8768, 'lr': 0.00010958094073914384, 'epoch': 14.15}\n","{'loss': 0.9898, 'lr': 0.0001093192225447418, 'epoch': 14.18}\n","{'loss': 0.9359, 'grad_norm': 1.2471728324890137, 'learning_rate': 0.00010905743994181267, 'epoch': 14.2}\n","{'loss': 0.9742, 'lr': 0.00010905743994181267, 'epoch': 14.2}\n","{'loss': 0.9141, 'lr': 0.00010879559473963099, 'epoch': 14.22}\n","{'loss': 0.9048, 'lr': 0.00010853368874790392, 'epoch': 14.25}\n","{'loss': 0.8828, 'lr': 0.00010827172377675876, 'epoch': 14.28}\n","{'loss': 0.8841, 'lr': 0.00010800970163673048, 'epoch': 14.3}\n","{'loss': 0.9758, 'lr': 0.00010774762413874904, 'epoch': 14.32}\n","{'loss': 0.9141, 'lr': 0.00010748549309412714, 'epoch': 14.35}\n","{'loss': 0.8902, 'lr': 0.00010722331031454748, 'epoch': 14.38}\n","{'loss': 0.9175, 'grad_norm': 1.0943458080291748, 'learning_rate': 0.00010696107761205038, 'epoch': 14.4}\n","{'loss': 0.9664, 'lr': 0.00010696107761205038, 'epoch': 14.4}\n","{'loss': 0.8527, 'lr': 0.00010669879679902115, 'epoch': 14.43}\n","{'loss': 1.2819, 'lr': 0.00010643646968817757, 'epoch': 14.45}\n","{'loss': 1.1977, 'lr': 0.00010617409809255748, 'epoch': 14.47}\n","{'loss': 1.0789, 'lr': 0.00010591168382550616, 'epoch': 14.5}\n","{'loss': 0.9412, 'lr': 0.00010564922870066376, 'epoch': 14.53}\n","{'loss': 0.9662, 'lr': 0.00010538673453195285, 'epoch': 14.55}\n","{'loss': 0.9842, 'lr': 0.00010512420313356581, 'epoch': 14.57}\n","{'loss': 1.0336, 'grad_norm': 1.1458253860473633, 'learning_rate': 0.0001048616363199524, 'epoch': 14.6}\n","{'loss': 0.945, 'lr': 0.0001048616363199524, 'epoch': 14.6}\n","{'loss': 0.9264, 'lr': 0.00010459903590580706, 'epoch': 14.62}\n","{'loss': 0.9275, 'lr': 0.00010433640370605652, 'epoch': 14.65}\n","{'loss': 0.937, 'lr': 0.00010407374153584716, 'epoch': 14.68}\n","{'loss': 0.9556, 'lr': 0.00010381105121053255, 'epoch': 14.7}\n","{'loss': 0.9824, 'lr': 0.00010354833454566071, 'epoch': 14.72}\n","{'loss': 1.0092, 'lr': 0.00010328559335696188, 'epoch': 14.75}\n","{'loss': 0.8969, 'lr': 0.00010302282946033565, 'epoch': 14.78}\n","{'loss': 0.9475, 'grad_norm': 0.7897231578826904, 'learning_rate': 0.00010276004467183868, 'epoch': 14.8}\n","{'loss': 0.9377, 'lr': 0.00010276004467183868, 'epoch': 14.8}\n","{'loss': 0.9204, 'lr': 0.00010249724080767192, 'epoch': 14.82}\n","{'loss': 0.9513, 'lr': 0.00010223441968416818, 'epoch': 14.85}\n","{'loss': 0.9426, 'lr': 0.00010197158311777957, 'epoch': 14.88}\n","{'loss': 0.9936, 'lr': 0.00010170873292506495, 'epoch': 14.9}\n","{'loss': 0.9579, 'lr': 0.00010144587092267736, 'epoch': 14.93}\n","{'loss': 1.0089, 'lr': 0.0001011829989273514, 'epoch': 14.95}\n","{'loss': 1.0248, 'lr': 0.00010092011875589076, 'epoch': 14.97}\n","{'loss': 0.9672, 'grad_norm': 1.42998206615448, 'learning_rate': 0.00010065723222515566, 'epoch': 15.0}\n"," 50% 600/1200 [02:54<02:22,  4.21it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9194, 'lr': 0.00010065723222515566, 'epoch': 15.0}\n","{'loss': 0.897, 'lr': 0.00010039434115205034, 'epoch': 15.03}\n","{'loss': 0.9461, 'lr': 0.00010013144735351023, 'epoch': 15.05}\n","{'loss': 0.8046, 'lr': 9.986855264648982e-05, 'epoch': 15.07}\n","{'loss': 0.8368, 'lr': 9.960565884794973e-05, 'epoch': 15.1}\n","{'loss': 0.9181, 'lr': 9.934276777484436e-05, 'epoch': 15.12}\n","{'loss': 0.9791, 'lr': 9.907988124410928e-05, 'epoch': 15.15}\n","{'loss': 0.9874, 'lr': 9.881700107264863e-05, 'epoch': 15.18}\n","{'loss': 0.9111, 'grad_norm': 1.624420166015625, 'learning_rate': 9.855412907732266e-05, 'epoch': 15.2}\n","{'loss': 0.8453, 'lr': 9.855412907732266e-05, 'epoch': 15.2}\n","{'loss': 0.9119, 'lr': 9.829126707493504e-05, 'epoch': 15.22}\n","{'loss': 0.9836, 'lr': 9.802841688222043e-05, 'epoch': 15.25}\n","{'loss': 0.9151, 'lr': 9.776558031583186e-05, 'epoch': 15.28}\n","{'loss': 1.0316, 'lr': 9.750275919232812e-05, 'epoch': 15.3}\n","{'loss': 0.8693, 'lr': 9.723995532816134e-05, 'epoch': 15.32}\n","{'loss': 0.8445, 'lr': 9.697717053966436e-05, 'epoch': 15.35}\n","{'loss': 0.9765, 'lr': 9.671440664303814e-05, 'epoch': 15.38}\n","{'loss': 0.9222, 'grad_norm': 1.3366472721099854, 'learning_rate': 9.645166545433931e-05, 'epoch': 15.4}\n","{'loss': 0.8839, 'lr': 9.645166545433931e-05, 'epoch': 15.4}\n","{'loss': 0.9239, 'lr': 9.618894878946748e-05, 'epoch': 15.43}\n","{'loss': 0.8878, 'lr': 9.592625846415283e-05, 'epoch': 15.45}\n","{'loss': 0.9675, 'lr': 9.566359629394348e-05, 'epoch': 15.47}\n","{'loss': 0.854, 'lr': 9.540096409419296e-05, 'epoch': 15.5}\n","{'loss': 0.9733, 'lr': 9.513836368004763e-05, 'epoch': 15.53}\n","{'loss': 0.9026, 'lr': 9.487579686643422e-05, 'epoch': 15.55}\n","{'loss': 0.9007, 'lr': 9.461326546804719e-05, 'epoch': 15.57}\n","{'loss': 0.9117, 'grad_norm': 0.7510390877723694, 'learning_rate': 9.435077129933627e-05, 'epoch': 15.6}\n","{'loss': 0.9981, 'lr': 9.435077129933627e-05, 'epoch': 15.6}\n","{'loss': 1.1079, 'lr': 9.408831617449385e-05, 'epoch': 15.62}\n","{'loss': 0.9664, 'lr': 9.382590190744252e-05, 'epoch': 15.65}\n","{'loss': 0.9561, 'lr': 9.356353031182244e-05, 'epoch': 15.68}\n","{'loss': 0.913, 'lr': 9.33012032009789e-05, 'epoch': 15.7}\n","{'loss': 0.9922, 'lr': 9.303892238794964e-05, 'epoch': 15.72}\n","{'loss': 1.0602, 'lr': 9.277668968545253e-05, 'epoch': 15.75}\n","{'loss': 0.9375, 'lr': 9.251450690587288e-05, 'epoch': 15.78}\n","{'loss': 0.9914, 'grad_norm': 1.1529226303100586, 'learning_rate': 9.225237586125099e-05, 'epoch': 15.8}\n","{'loss': 0.9092, 'lr': 9.225237586125099e-05, 'epoch': 15.8}\n","{'loss': 0.9457, 'lr': 9.199029836326956e-05, 'epoch': 15.82}\n","{'loss': 0.8771, 'lr': 9.172827622324123e-05, 'epoch': 15.85}\n","{'loss': 1.059, 'lr': 9.146631125209607e-05, 'epoch': 15.88}\n","{'loss': 0.9584, 'lr': 9.120440526036903e-05, 'epoch': 15.9}\n","{'loss': 0.9485, 'lr': 9.094256005818736e-05, 'epoch': 15.93}\n","{'loss': 0.9772, 'lr': 9.068077745525823e-05, 'epoch': 15.95}\n","{'loss': 0.9179, 'lr': 9.041905926085618e-05, 'epoch': 15.97}\n","{'loss': 0.9491, 'grad_norm': 0.890102744102478, 'learning_rate': 9.015740728381054e-05, 'epoch': 16.0}\n"," 53% 640/1200 [03:05<02:24,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.854, 'lr': 9.015740728381054e-05, 'epoch': 16.0}\n","{'loss': 0.8598, 'lr': 8.989582333249306e-05, 'epoch': 16.02}\n","{'loss': 0.8658, 'lr': 8.963430921480533e-05, 'epoch': 16.05}\n","{'loss': 0.9935, 'lr': 8.937286673816621e-05, 'epoch': 16.07}\n","{'loss': 0.853, 'lr': 8.911149770949948e-05, 'epoch': 16.1}\n","{'loss': 0.9565, 'lr': 8.885020393522135e-05, 'epoch': 16.12}\n","{'loss': 0.9018, 'lr': 8.858898722122782e-05, 'epoch': 16.15}\n","{'loss': 0.9782, 'lr': 8.832784937288237e-05, 'epoch': 16.18}\n","{'loss': 0.9078, 'grad_norm': 1.3245429992675781, 'learning_rate': 8.806679219500333e-05, 'epoch': 16.2}\n","{'loss': 0.9911, 'lr': 8.806679219500333e-05, 'epoch': 16.2}\n","{'loss': 0.8993, 'lr': 8.78058174918516e-05, 'epoch': 16.23}\n","{'loss': 0.9062, 'lr': 8.754492706711798e-05, 'epoch': 16.25}\n","{'loss': 0.9065, 'lr': 8.728412272391083e-05, 'epoch': 16.27}\n","{'loss': 0.8813, 'lr': 8.702340626474355e-05, 'epoch': 16.3}\n","{'loss': 0.856, 'lr': 8.676277949152223e-05, 'epoch': 16.32}\n","{'loss': 0.9185, 'lr': 8.650224420553288e-05, 'epoch': 16.35}\n","{'loss': 0.9482, 'lr': 8.624180220742946e-05, 'epoch': 16.38}\n","{'loss': 0.9134, 'grad_norm': 5.022780895233154, 'learning_rate': 8.598145529722104e-05, 'epoch': 16.4}\n","{'loss': 1.0568, 'lr': 8.598145529722104e-05, 'epoch': 16.4}\n","{'loss': 0.8522, 'lr': 8.572120527425955e-05, 'epoch': 16.43}\n","{'loss': 0.9558, 'lr': 8.546105393722731e-05, 'epoch': 16.45}\n","{'loss': 0.9663, 'lr': 8.520100308412455e-05, 'epoch': 16.48}\n","{'loss': 0.8802, 'lr': 8.494105451225704e-05, 'epoch': 16.5}\n","{'loss': 0.9526, 'lr': 8.46812100182237e-05, 'epoch': 16.52}\n","{'loss': 0.8984, 'lr': 8.442147139790397e-05, 'epoch': 16.55}\n","{'loss': 0.9143, 'lr': 8.416184044644571e-05, 'epoch': 16.57}\n","{'loss': 0.9346, 'grad_norm': 1.1509184837341309, 'learning_rate': 8.390231895825258e-05, 'epoch': 16.6}\n","{'loss': 0.8403, 'lr': 8.390231895825258e-05, 'epoch': 16.6}\n","{'loss': 1.0535, 'lr': 8.364290872697173e-05, 'epoch': 16.62}\n","{'loss': 0.8965, 'lr': 8.338361154548137e-05, 'epoch': 16.65}\n","{'loss': 0.883, 'lr': 8.312442920587833e-05, 'epoch': 16.68}\n","{'loss': 0.8972, 'lr': 8.286536349946581e-05, 'epoch': 16.7}\n","{'loss': 0.869, 'lr': 8.260641621674082e-05, 'epoch': 16.73}\n","{'loss': 0.9244, 'lr': 8.234758914738199e-05, 'epoch': 16.75}\n","{'loss': 0.9106, 'lr': 8.208888408023704e-05, 'epoch': 16.77}\n","{'loss': 0.9093, 'grad_norm': 1.2890774011611938, 'learning_rate': 8.183030280331054e-05, 'epoch': 16.8}\n","{'loss': 1.0305, 'lr': 8.183030280331054e-05, 'epoch': 16.8}\n","{'loss': 0.8531, 'lr': 8.157184710375149e-05, 'epoch': 16.82}\n","{'loss': 0.9428, 'lr': 8.131351876784094e-05, 'epoch': 16.85}\n","{'loss': 0.902, 'lr': 8.105531958097972e-05, 'epoch': 16.88}\n","{'loss': 0.9311, 'lr': 8.07972513276761e-05, 'epoch': 16.9}\n","{'loss': 0.9177, 'lr': 8.053931579153333e-05, 'epoch': 16.93}\n","{'loss': 0.9111, 'lr': 8.028151475523753e-05, 'epoch': 16.95}\n","{'loss': 0.9465, 'lr': 8.002385000054506e-05, 'epoch': 16.98}\n","{'loss': 0.9293, 'grad_norm': 0.9685531258583069, 'learning_rate': 7.976632330827056e-05, 'epoch': 17.0}\n"," 57% 680/1200 [03:17<02:05,  4.13it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9103, 'lr': 7.976632330827056e-05, 'epoch': 17.0}\n","{'loss': 0.8808, 'lr': 7.950893645827437e-05, 'epoch': 17.02}\n","{'loss': 0.8853, 'lr': 7.925169122945039e-05, 'epoch': 17.05}\n","{'loss': 0.8737, 'lr': 7.89945893997137e-05, 'epoch': 17.07}\n","{'loss': 0.8952, 'lr': 7.873763274598827e-05, 'epoch': 17.1}\n","{'loss': 0.8456, 'lr': 7.848082304419478e-05, 'epoch': 17.12}\n","{'loss': 1.1216, 'lr': 7.822416206923816e-05, 'epoch': 17.15}\n","{'loss': 0.8575, 'lr': 7.796765159499558e-05, 'epoch': 17.18}\n","{'loss': 0.9088, 'grad_norm': 2.8294260501861572, 'learning_rate': 7.771129339430389e-05, 'epoch': 17.2}\n","{'loss': 0.8129, 'lr': 7.771129339430389e-05, 'epoch': 17.2}\n","{'loss': 0.8984, 'lr': 7.74550892389476e-05, 'epoch': 17.23}\n","{'loss': 0.96, 'lr': 7.719904089964658e-05, 'epoch': 17.25}\n","{'loss': 0.8968, 'lr': 7.694315014604376e-05, 'epoch': 17.27}\n","{'loss': 0.8385, 'lr': 7.668741874669297e-05, 'epoch': 17.3}\n","{'loss': 0.8846, 'lr': 7.643184846904662e-05, 'epoch': 17.32}\n","{'loss': 0.8558, 'lr': 7.617644107944366e-05, 'epoch': 17.35}\n","{'loss': 0.9, 'lr': 7.592119834309715e-05, 'epoch': 17.38}\n","{'loss': 0.8809, 'grad_norm': 4.259962558746338, 'learning_rate': 7.56661220240822e-05, 'epoch': 17.4}\n","{'loss': 0.9173, 'lr': 7.56661220240822e-05, 'epoch': 17.4}\n","{'loss': 0.9186, 'lr': 7.541121388532379e-05, 'epoch': 17.43}\n","{'loss': 0.9319, 'lr': 7.515647568858452e-05, 'epoch': 17.45}\n","{'loss': 0.8965, 'lr': 7.490190919445248e-05, 'epoch': 17.48}\n","{'loss': 0.9179, 'lr': 7.464751616232902e-05, 'epoch': 17.5}\n","{'loss': 0.8936, 'lr': 7.439329835041661e-05, 'epoch': 17.52}\n","{'loss': 0.9482, 'lr': 7.413925751570684e-05, 'epoch': 17.55}\n","{'loss': 1.0314, 'lr': 7.388539541396802e-05, 'epoch': 17.57}\n","{'loss': 0.9319, 'grad_norm': 1.754729986190796, 'learning_rate': 7.363171379973322e-05, 'epoch': 17.6}\n","{'loss': 0.8915, 'lr': 7.363171379973322e-05, 'epoch': 17.6}\n","{'loss': 0.9331, 'lr': 7.337821442628805e-05, 'epoch': 17.62}\n","{'loss': 0.9218, 'lr': 7.312489904565864e-05, 'epoch': 17.65}\n","{'loss': 0.9682, 'lr': 7.287176940859948e-05, 'epoch': 17.68}\n","{'loss': 0.8336, 'lr': 7.261882726458126e-05, 'epoch': 17.7}\n","{'loss': 0.866, 'lr': 7.236607436177893e-05, 'epoch': 17.73}\n","{'loss': 0.8569, 'lr': 7.211351244705946e-05, 'epoch': 17.75}\n","{'loss': 0.8345, 'lr': 7.186114326596984e-05, 'epoch': 17.77}\n","{'loss': 0.8882, 'grad_norm': 0.891715943813324, 'learning_rate': 7.160896856272506e-05, 'epoch': 17.8}\n","{'loss': 0.8887, 'lr': 7.160896856272506e-05, 'epoch': 17.8}\n","{'loss': 0.9347, 'lr': 7.135699008019585e-05, 'epoch': 17.82}\n","{'loss': 0.9144, 'lr': 7.110520955989697e-05, 'epoch': 17.85}\n","{'loss': 0.87, 'lr': 7.08536287419749e-05, 'epoch': 17.88}\n","{'loss': 0.8849, 'lr': 7.060224936519592e-05, 'epoch': 17.9}\n","{'loss': 0.8695, 'lr': 7.035107316693408e-05, 'epoch': 17.93}\n","{'loss': 0.8731, 'lr': 7.010010188315919e-05, 'epoch': 17.95}\n","{'loss': 0.855, 'lr': 6.984933724842481e-05, 'epoch': 17.98}\n","{'loss': 0.8863, 'grad_norm': 2.0067994594573975, 'learning_rate': 6.959878099585635e-05, 'epoch': 18.0}\n"," 60% 720/1200 [03:29<02:07,  3.76it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8502, 'lr': 6.959878099585635e-05, 'epoch': 18.0}\n","{'loss': 0.8628, 'lr': 6.934843485713882e-05, 'epoch': 18.02}\n","{'loss': 0.8996, 'lr': 6.909830056250527e-05, 'epoch': 18.05}\n","{'loss': 0.7643, 'lr': 6.884837984072451e-05, 'epoch': 18.07}\n","{'loss': 0.9166, 'lr': 6.85986744190893e-05, 'epoch': 18.1}\n","{'loss': 0.8346, 'lr': 6.834918602340438e-05, 'epoch': 18.12}\n","{'loss': 0.9019, 'lr': 6.809991637797455e-05, 'epoch': 18.15}\n","{'loss': 0.9209, 'lr': 6.78508672055927e-05, 'epoch': 18.18}\n","{'loss': 0.8688, 'grad_norm': 0.9885587096214294, 'learning_rate': 6.760204022752805e-05, 'epoch': 18.2}\n","{'loss': 0.8969, 'lr': 6.760204022752805e-05, 'epoch': 18.2}\n","{'loss': 0.8747, 'lr': 6.735343716351405e-05, 'epoch': 18.23}\n","{'loss': 0.8392, 'lr': 6.710505973173664e-05, 'epoch': 18.25}\n","{'loss': 0.9847, 'lr': 6.685690964882236e-05, 'epoch': 18.27}\n","{'loss': 0.8476, 'lr': 6.660898862982641e-05, 'epoch': 18.3}\n","{'loss': 0.892, 'lr': 6.63612983882209e-05, 'epoch': 18.32}\n","{'loss': 0.8662, 'lr': 6.611384063588291e-05, 'epoch': 18.35}\n","{'loss': 0.928, 'lr': 6.586661708308272e-05, 'epoch': 18.38}\n","{'loss': 0.8912, 'grad_norm': 3.943999767303467, 'learning_rate': 6.5619629438472e-05, 'epoch': 18.4}\n","{'loss': 0.9089, 'lr': 6.5619629438472e-05, 'epoch': 18.4}\n","{'loss': 0.8544, 'lr': 6.537287940907194e-05, 'epoch': 18.43}\n","{'loss': 0.8967, 'lr': 6.512636870026143e-05, 'epoch': 18.45}\n","{'loss': 1.0145, 'lr': 6.488009901576544e-05, 'epoch': 18.48}\n","{'loss': 0.8954, 'lr': 6.463407205764305e-05, 'epoch': 18.5}\n","{'loss': 0.8628, 'lr': 6.438828952627582e-05, 'epoch': 18.52}\n","{'loss': 0.8708, 'lr': 6.414275312035598e-05, 'epoch': 18.55}\n","{'loss': 0.9392, 'lr': 6.389746453687468e-05, 'epoch': 18.57}\n","{'loss': 0.9053, 'grad_norm': 1.1302106380462646, 'learning_rate': 6.365242547111027e-05, 'epoch': 18.6}\n","{'loss': 0.8941, 'lr': 6.365242547111027e-05, 'epoch': 18.6}\n","{'loss': 0.9292, 'lr': 6.340763761661665e-05, 'epoch': 18.62}\n","{'loss': 0.9214, 'lr': 6.316310266521142e-05, 'epoch': 18.65}\n","{'loss': 0.8694, 'lr': 6.291882230696432e-05, 'epoch': 18.68}\n","{'loss': 0.8884, 'lr': 6.267479823018553e-05, 'epoch': 18.7}\n","{'loss': 0.9094, 'lr': 6.243103212141394e-05, 'epoch': 18.73}\n","{'loss': 0.8501, 'lr': 6.218752566540554e-05, 'epoch': 18.75}\n","{'loss': 1.0261, 'lr': 6.19442805451218e-05, 'epoch': 18.77}\n","{'loss': 0.911, 'grad_norm': 5.675540447235107, 'learning_rate': 6.170129844171792e-05, 'epoch': 18.8}\n","{'loss': 0.8549, 'lr': 6.170129844171792e-05, 'epoch': 18.8}\n","{'loss': 0.8964, 'lr': 6.145858103453141e-05, 'epoch': 18.82}\n","{'loss': 0.903, 'lr': 6.121613000107023e-05, 'epoch': 18.85}\n","{'loss': 0.8135, 'lr': 6.097394701700145e-05, 'epoch': 18.88}\n","{'loss': 0.8807, 'lr': 6.073203375613949e-05, 'epoch': 18.9}\n","{'loss': 0.8993, 'lr': 6.049039189043466e-05, 'epoch': 18.93}\n","{'loss': 0.8331, 'lr': 6.024902308996152e-05, 'epoch': 18.95}\n","{'loss': 0.8552, 'lr': 6.000792902290741e-05, 'epoch': 18.98}\n","{'loss': 0.867, 'grad_norm': 1.0041224956512451, 'learning_rate': 5.976711135556086e-05, 'epoch': 19.0}\n"," 63% 760/1200 [03:41<01:51,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8174, 'lr': 5.976711135556086e-05, 'epoch': 19.0}\n","{'loss': 0.8774, 'lr': 5.952657175230012e-05, 'epoch': 19.02}\n","{'loss': 0.8422, 'lr': 5.9286311875581656e-05, 'epoch': 19.05}\n","{'loss': 0.8928, 'lr': 5.904633338592857e-05, 'epoch': 19.07}\n","{'loss': 0.7982, 'lr': 5.880663794191925e-05, 'epoch': 19.1}\n","{'loss': 0.8197, 'lr': 5.8567227200175865e-05, 'epoch': 19.12}\n","{'loss': 1.2111, 'lr': 5.832810281535288e-05, 'epoch': 19.15}\n","{'loss': 0.8346, 'lr': 5.8089266440125645e-05, 'epoch': 19.18}\n","{'loss': 0.8867, 'grad_norm': 0.7353324890136719, 'learning_rate': 5.7850719725178994e-05, 'epoch': 19.2}\n","{'loss': 0.9032, 'lr': 5.7850719725178994e-05, 'epoch': 19.2}\n","{'loss': 0.8928, 'lr': 5.761246431919576e-05, 'epoch': 19.23}\n","{'loss': 0.8276, 'lr': 5.7374501868845544e-05, 'epoch': 19.25}\n","{'loss': 0.8467, 'lr': 5.7136834018773047e-05, 'epoch': 19.27}\n","{'loss': 1.0537, 'lr': 5.6899462411587027e-05, 'epoch': 19.3}\n","{'loss': 0.9201, 'lr': 5.666238868784875e-05, 'epoch': 19.32}\n","{'loss': 0.8518, 'lr': 5.642561448606072e-05, 'epoch': 19.35}\n","{'loss': 0.7863, 'lr': 5.6189141442655325e-05, 'epoch': 19.38}\n","{'loss': 0.8853, 'grad_norm': 2.3681564331054688, 'learning_rate': 5.595297119198352e-05, 'epoch': 19.4}\n","{'loss': 0.8269, 'lr': 5.595297119198352e-05, 'epoch': 19.4}\n","{'loss': 0.9287, 'lr': 5.571710536630357e-05, 'epoch': 19.43}\n","{'loss': 0.875, 'lr': 5.548154559576979e-05, 'epoch': 19.45}\n","{'loss': 0.9772, 'lr': 5.524629350842111e-05, 'epoch': 19.48}\n","{'loss': 0.8389, 'lr': 5.501135073017008e-05, 'epoch': 19.5}\n","{'loss': 0.9595, 'lr': 5.477671888479142e-05, 'epoch': 19.52}\n","{'loss': 0.8589, 'lr': 5.454239959391095e-05, 'epoch': 19.55}\n","{'loss': 0.8072, 'lr': 5.430839447699427e-05, 'epoch': 19.57}\n","{'loss': 0.884, 'grad_norm': 0.8159210085868835, 'learning_rate': 5.4074705151335615e-05, 'epoch': 19.6}\n","{'loss': 0.9287, 'lr': 5.4074705151335615e-05, 'epoch': 19.6}\n","{'loss': 0.8115, 'lr': 5.3841333232046654e-05, 'epoch': 19.62}\n","{'loss': 0.9077, 'lr': 5.360828033204539e-05, 'epoch': 19.65}\n","{'loss': 1.0376, 'lr': 5.337554806204497e-05, 'epoch': 19.68}\n","{'loss': 1.0763, 'lr': 5.3143138030542436e-05, 'epoch': 19.7}\n","{'loss': 0.8285, 'lr': 5.291105184380787e-05, 'epoch': 19.73}\n","{'loss': 0.9466, 'lr': 5.267929110587307e-05, 'epoch': 19.75}\n","{'loss': 0.8808, 'lr': 5.2447857418520564e-05, 'epoch': 19.77}\n","{'loss': 0.9272, 'grad_norm': 1.001754641532898, 'learning_rate': 5.221675238127255e-05, 'epoch': 19.8}\n","{'loss': 0.9436, 'lr': 5.221675238127255e-05, 'epoch': 19.8}\n","{'loss': 0.9175, 'lr': 5.1985977591379664e-05, 'epoch': 19.82}\n","{'loss': 0.846, 'lr': 5.175553464381023e-05, 'epoch': 19.85}\n","{'loss': 0.9081, 'lr': 5.1525425131239056e-05, 'epoch': 19.88}\n","{'loss': 0.9139, 'lr': 5.1295650644036406e-05, 'epoch': 19.9}\n","{'loss': 0.9246, 'lr': 5.1066212770257185e-05, 'epoch': 19.93}\n","{'loss': 0.8539, 'lr': 5.083711309562963e-05, 'epoch': 19.95}\n","{'loss': 0.8339, 'lr': 5.0608353203544754e-05, 'epoch': 19.98}\n","{'loss': 0.8927, 'grad_norm': 0.9327153563499451, 'learning_rate': 5.0379934675045145e-05, 'epoch': 20.0}\n"," 67% 800/1200 [03:53<01:37,  4.10it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8223, 'lr': 5.0379934675045145e-05, 'epoch': 20.0}\n","{'loss': 0.8264, 'lr': 5.015185908881409e-05, 'epoch': 20.02}\n","{'loss': 0.8888, 'lr': 4.992412802116469e-05, 'epoch': 20.05}\n","{'loss': 0.8452, 'lr': 4.969674304602895e-05, 'epoch': 20.07}\n","{'loss': 0.7824, 'lr': 4.9469705734946946e-05, 'epoch': 20.1}\n","{'loss': 0.7994, 'lr': 4.92430176570558e-05, 'epoch': 20.12}\n","{'loss': 0.789, 'lr': 4.9016680379079085e-05, 'epoch': 20.15}\n","{'loss': 0.8837, 'lr': 4.879069546531581e-05, 'epoch': 20.18}\n","{'loss': 0.8296, 'grad_norm': 0.897709310054779, 'learning_rate': 4.8565064477629683e-05, 'epoch': 20.2}\n","{'loss': 0.886, 'lr': 4.8565064477629683e-05, 'epoch': 20.2}\n","{'loss': 0.8331, 'lr': 4.833978897543829e-05, 'epoch': 20.23}\n","{'loss': 0.8875, 'lr': 4.811487051570235e-05, 'epoch': 20.25}\n","{'loss': 0.9122, 'lr': 4.78903106529149e-05, 'epoch': 20.27}\n","{'loss': 0.8553, 'lr': 4.7666110939090644e-05, 'epoch': 20.3}\n","{'loss': 0.8957, 'lr': 4.744227292375505e-05, 'epoch': 20.32}\n","{'loss': 0.8685, 'lr': 4.721879815393385e-05, 'epoch': 20.35}\n","{'loss': 0.8683, 'lr': 4.699568817414224e-05, 'epoch': 20.38}\n","{'loss': 0.8758, 'grad_norm': 1.0276384353637695, 'learning_rate': 4.677294452637422e-05, 'epoch': 20.4}\n","{'loss': 0.8357, 'lr': 4.677294452637422e-05, 'epoch': 20.4}\n","{'loss': 0.8534, 'lr': 4.655056875009192e-05, 'epoch': 20.43}\n","{'loss': 0.8212, 'lr': 4.632856238221498e-05, 'epoch': 20.45}\n","{'loss': 0.83, 'lr': 4.610692695710994e-05, 'epoch': 20.48}\n","{'loss': 0.8887, 'lr': 4.5885664006579645e-05, 'epoch': 20.5}\n","{'loss': 0.8669, 'lr': 4.5664775059852515e-05, 'epoch': 20.52}\n","{'loss': 0.853, 'lr': 4.5444261643572226e-05, 'epoch': 20.55}\n","{'loss': 0.9703, 'lr': 4.522412528178698e-05, 'epoch': 20.57}\n","{'loss': 0.8649, 'grad_norm': 6.3976335525512695, 'learning_rate': 4.500436749593899e-05, 'epoch': 20.6}\n","{'loss': 0.9117, 'lr': 4.500436749593899e-05, 'epoch': 20.6}\n","{'loss': 0.891, 'lr': 4.478498980485405e-05, 'epoch': 20.62}\n","{'loss': 0.8171, 'lr': 4.456599372473094e-05, 'epoch': 20.65}\n","{'loss': 0.9315, 'lr': 4.4347380769130995e-05, 'epoch': 20.68}\n","{'loss': 0.9221, 'lr': 4.412915244896764e-05, 'epoch': 20.7}\n","{'loss': 0.8471, 'lr': 4.3911310272495996e-05, 'epoch': 20.73}\n","{'loss': 0.8409, 'lr': 4.369385574530227e-05, 'epoch': 20.75}\n","{'loss': 0.8517, 'lr': 4.347679037029361e-05, 'epoch': 20.77}\n","{'loss': 0.8766, 'grad_norm': 4.629057884216309, 'learning_rate': 4.326011564768755e-05, 'epoch': 20.8}\n","{'loss': 0.8607, 'lr': 4.326011564768755e-05, 'epoch': 20.8}\n","{'loss': 0.8158, 'lr': 4.304383307500169e-05, 'epoch': 20.82}\n","{'loss': 0.772, 'lr': 4.282794414704333e-05, 'epoch': 20.85}\n","{'loss': 0.8752, 'lr': 4.261245035589917e-05, 'epoch': 20.88}\n","{'loss': 0.9488, 'lr': 4.2397353190924926e-05, 'epoch': 20.9}\n","{'loss': 0.7857, 'lr': 4.218265413873519e-05, 'epoch': 20.93}\n","{'loss': 0.8384, 'lr': 4.196835468319289e-05, 'epoch': 20.95}\n","{'loss': 0.9223, 'lr': 4.1754456305399336e-05, 'epoch': 20.98}\n","{'loss': 0.8524, 'grad_norm': 1.2978726625442505, 'learning_rate': 4.15409604836838e-05, 'epoch': 21.0}\n"," 70% 840/1200 [04:05<01:36,  3.72it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8285, 'lr': 4.15409604836838e-05, 'epoch': 21.0}\n","{'loss': 0.8809, 'lr': 4.132786869359333e-05, 'epoch': 21.02}\n","{'loss': 0.7669, 'lr': 4.111518240788258e-05, 'epoch': 21.05}\n","{'loss': 0.8181, 'lr': 4.090290309650361e-05, 'epoch': 21.07}\n","{'loss': 0.8246, 'lr': 4.069103222659576e-05, 'epoch': 21.1}\n","{'loss': 0.8361, 'lr': 4.047957126247541e-05, 'epoch': 21.12}\n","{'loss': 0.7898, 'lr': 4.0268521665626e-05, 'epoch': 21.15}\n","{'loss': 0.9358, 'lr': 4.005788489468784e-05, 'epoch': 21.18}\n","{'loss': 0.8351, 'grad_norm': 6.753229141235352, 'learning_rate': 3.984766240544805e-05, 'epoch': 21.2}\n","{'loss': 0.9158, 'lr': 3.984766240544805e-05, 'epoch': 21.2}\n","{'loss': 0.8236, 'lr': 3.9637855650830544e-05, 'epoch': 21.23}\n","{'loss': 0.8742, 'lr': 3.942846608088583e-05, 'epoch': 21.25}\n","{'loss': 0.9009, 'lr': 3.921949514278119e-05, 'epoch': 21.27}\n","{'loss': 0.7976, 'lr': 3.90109442807906e-05, 'epoch': 21.3}\n","{'loss': 0.8002, 'lr': 3.880281493628467e-05, 'epoch': 21.32}\n","{'loss': 0.8175, 'lr': 3.859510854772086e-05, 'epoch': 21.35}\n","{'loss': 0.7885, 'lr': 3.838782655063325e-05, 'epoch': 21.38}\n","{'loss': 0.8398, 'grad_norm': 0.7917744517326355, 'learning_rate': 3.818097037762297e-05, 'epoch': 21.4}\n","{'loss': 0.8842, 'lr': 3.818097037762297e-05, 'epoch': 21.4}\n","{'loss': 0.8004, 'lr': 3.7974541458348046e-05, 'epoch': 21.43}\n","{'loss': 0.7961, 'lr': 3.776854121951361e-05, 'epoch': 21.45}\n","{'loss': 0.8975, 'lr': 3.756297108486202e-05, 'epoch': 21.48}\n","{'loss': 0.8536, 'lr': 3.7357832475163045e-05, 'epoch': 21.5}\n","{'loss': 0.8596, 'lr': 3.715312680820402e-05, 'epoch': 21.52}\n","{'loss': 0.9362, 'lr': 3.6948855498780086e-05, 'epoch': 21.55}\n","{'loss': 0.8772, 'lr': 3.674501995868429e-05, 'epoch': 21.57}\n","{'loss': 0.8631, 'grad_norm': 1.2088749408721924, 'learning_rate': 3.654162159669802e-05, 'epoch': 21.6}\n","{'loss': 0.886, 'lr': 3.654162159669802e-05, 'epoch': 21.6}\n","{'loss': 0.8232, 'lr': 3.63386618185811e-05, 'epoch': 21.62}\n","{'loss': 0.854, 'lr': 3.613614202706219e-05, 'epoch': 21.65}\n","{'loss': 0.7902, 'lr': 3.593406362182903e-05, 'epoch': 21.68}\n","{'loss': 0.8508, 'lr': 3.573242799951876e-05, 'epoch': 21.7}\n","{'loss': 0.8247, 'lr': 3.5531236553708305e-05, 'epoch': 21.73}\n","{'loss': 0.8567, 'lr': 3.5330490674904735e-05, 'epoch': 21.75}\n","{'loss': 0.9238, 'lr': 3.513019175053559e-05, 'epoch': 21.77}\n","{'loss': 0.8512, 'grad_norm': 7.496747016906738, 'learning_rate': 3.49303411649394e-05, 'epoch': 21.8}\n","{'loss': 0.8395, 'lr': 3.49303411649394e-05, 'epoch': 21.8}\n","{'loss': 0.9091, 'lr': 3.473094029935603e-05, 'epoch': 21.82}\n","{'loss': 0.7959, 'lr': 3.453199053191722e-05, 'epoch': 21.85}\n","{'loss': 0.8436, 'lr': 3.433349323763696e-05, 'epoch': 21.88}\n","{'loss': 0.77, 'lr': 3.4135449788402044e-05, 'epoch': 21.9}\n","{'loss': 0.8852, 'lr': 3.3937861552962555e-05, 'epoch': 21.93}\n","{'loss': 0.895, 'lr': 3.3740729896922496e-05, 'epoch': 21.95}\n","{'loss': 0.881, 'lr': 3.3544056182730244e-05, 'epoch': 21.98}\n","{'loss': 0.8524, 'grad_norm': 2.5967798233032227, 'learning_rate': 3.334784176966912e-05, 'epoch': 22.0}\n"," 73% 880/1200 [04:17<01:19,  4.03it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7959, 'lr': 3.334784176966912e-05, 'epoch': 22.0}\n","{'loss': 0.8462, 'lr': 3.315208801384811e-05, 'epoch': 22.02}\n","{'loss': 0.7903, 'lr': 3.2956796268192456e-05, 'epoch': 22.05}\n","{'loss': 0.8834, 'lr': 3.276196788243422e-05, 'epoch': 22.07}\n","{'loss': 0.8386, 'lr': 3.256760420310303e-05, 'epoch': 22.1}\n","{'loss': 0.7927, 'lr': 3.2373706573516794e-05, 'epoch': 22.12}\n","{'loss': 0.7701, 'lr': 3.218027633377235e-05, 'epoch': 22.15}\n","{'loss': 0.8435, 'lr': 3.1987314820736257e-05, 'epoch': 22.18}\n","{'loss': 0.8201, 'grad_norm': 1.3122155666351318, 'learning_rate': 3.1794823368035476e-05, 'epoch': 22.2}\n","{'loss': 0.8423, 'lr': 3.1794823368035476e-05, 'epoch': 22.2}\n","{'loss': 0.8278, 'lr': 3.160280330604827e-05, 'epoch': 22.23}\n","{'loss': 0.7654, 'lr': 3.141125596189494e-05, 'epoch': 22.25}\n","{'loss': 0.8481, 'lr': 3.1220182659428655e-05, 'epoch': 22.27}\n","{'loss': 0.8787, 'lr': 3.10295847192263e-05, 'epoch': 22.3}\n","{'loss': 0.7841, 'lr': 3.083946345857939e-05, 'epoch': 22.32}\n","{'loss': 0.7987, 'lr': 3.0649820191484904e-05, 'epoch': 22.35}\n","{'loss': 0.8789, 'lr': 3.0460656228636254e-05, 'epoch': 22.38}\n","{'loss': 0.828, 'grad_norm': 3.913419008255005, 'learning_rate': 3.0271972877414167e-05, 'epoch': 22.4}\n","{'loss': 0.8501, 'lr': 3.0271972877414167e-05, 'epoch': 22.4}\n","{'loss': 0.7429, 'lr': 3.0083771441877728e-05, 'epoch': 22.43}\n","{'loss': 0.8894, 'lr': 2.9896053222755338e-05, 'epoch': 22.45}\n","{'loss': 0.799, 'lr': 2.970881951743567e-05, 'epoch': 22.48}\n","{'loss': 0.8535, 'lr': 2.952207161995879e-05, 'epoch': 22.5}\n","{'loss': 0.8855, 'lr': 2.933581082100717e-05, 'epoch': 22.52}\n","{'loss': 0.7892, 'lr': 2.9150038407896696e-05, 'epoch': 22.55}\n","{'loss': 0.8173, 'lr': 2.8964755664567923e-05, 'epoch': 22.57}\n","{'loss': 0.8284, 'grad_norm': 0.9124160408973694, 'learning_rate': 2.8779963871577076e-05, 'epoch': 22.6}\n","{'loss': 0.8177, 'lr': 2.8779963871577076e-05, 'epoch': 22.6}\n","{'loss': 0.8669, 'lr': 2.8595664306087312e-05, 'epoch': 22.62}\n","{'loss': 0.8078, 'lr': 2.84118582418597e-05, 'epoch': 22.65}\n","{'loss': 0.8169, 'lr': 2.822854694924465e-05, 'epoch': 22.68}\n","{'loss': 0.8245, 'lr': 2.8045731695172994e-05, 'epoch': 22.7}\n","{'loss': 0.8324, 'lr': 2.786341374314725e-05, 'epoch': 22.73}\n","{'loss': 0.8416, 'lr': 2.7681594353232932e-05, 'epoch': 22.75}\n","{'loss': 0.8116, 'lr': 2.7500274782049762e-05, 'epoch': 22.77}\n","{'loss': 0.8274, 'grad_norm': 1.108984351158142, 'learning_rate': 2.7319456282763113e-05, 'epoch': 22.8}\n","{'loss': 0.8342, 'lr': 2.7319456282763113e-05, 'epoch': 22.8}\n","{'loss': 0.8708, 'lr': 2.7139140105075134e-05, 'epoch': 22.82}\n","{'loss': 0.8819, 'lr': 2.695932749521638e-05, 'epoch': 22.85}\n","{'loss': 0.8549, 'lr': 2.6780019695937008e-05, 'epoch': 22.88}\n","{'loss': 0.8514, 'lr': 2.660121794649826e-05, 'epoch': 22.9}\n","{'loss': 0.7958, 'lr': 2.6422923482663863e-05, 'epoch': 22.93}\n","{'loss': 0.8306, 'lr': 2.6245137536691532e-05, 'epoch': 22.95}\n","{'loss': 0.8053, 'lr': 2.606786133732442e-05, 'epoch': 22.98}\n","{'loss': 0.8406, 'grad_norm': 0.8317959308624268, 'learning_rate': 2.5891096109782642e-05, 'epoch': 23.0}\n"," 77% 920/1200 [04:28<01:07,  4.14it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8312, 'lr': 2.5891096109782642e-05, 'epoch': 23.0}\n","{'loss': 0.7923, 'lr': 2.5714843075754823e-05, 'epoch': 23.02}\n","{'loss': 0.8988, 'lr': 2.5539103453389556e-05, 'epoch': 23.05}\n","{'loss': 0.918, 'lr': 2.5363878457287138e-05, 'epoch': 23.07}\n","{'loss': 0.8106, 'lr': 2.5189169298491054e-05, 'epoch': 23.1}\n","{'loss': 0.7994, 'lr': 2.5014977184479694e-05, 'epoch': 23.12}\n","{'loss': 0.7731, 'lr': 2.4841303319157925e-05, 'epoch': 23.15}\n","{'loss': 0.7649, 'lr': 2.466814890284882e-05, 'epoch': 23.18}\n","{'loss': 0.8235, 'grad_norm': 0.7690761685371399, 'learning_rate': 2.449551513228535e-05, 'epoch': 23.2}\n","{'loss': 0.8043, 'lr': 2.449551513228535e-05, 'epoch': 23.2}\n","{'loss': 0.7685, 'lr': 2.432340320060216e-05, 'epoch': 23.23}\n","{'loss': 0.8245, 'lr': 2.4151814297327158e-05, 'epoch': 23.25}\n","{'loss': 0.7956, 'lr': 2.3980749608373533e-05, 'epoch': 23.27}\n","{'loss': 0.8608, 'lr': 2.381021031603139e-05, 'epoch': 23.3}\n","{'loss': 0.8137, 'lr': 2.3640197598959622e-05, 'epoch': 23.32}\n","{'loss': 0.8197, 'lr': 2.3470712632177804e-05, 'epoch': 23.35}\n","{'loss': 0.8184, 'lr': 2.3301756587057987e-05, 'epoch': 23.38}\n","{'loss': 0.8132, 'grad_norm': 0.8999014496803284, 'learning_rate': 2.3133330631316708e-05, 'epoch': 23.4}\n","{'loss': 0.8374, 'lr': 2.3133330631316708e-05, 'epoch': 23.4}\n","{'loss': 0.7933, 'lr': 2.296543592900683e-05, 'epoch': 23.43}\n","{'loss': 0.7959, 'lr': 2.27980736405095e-05, 'epoch': 23.45}\n","{'loss': 0.809, 'lr': 2.263124492252623e-05, 'epoch': 23.48}\n","{'loss': 0.8212, 'lr': 2.246495092807077e-05, 'epoch': 23.5}\n","{'loss': 0.8799, 'lr': 2.2299192806461268e-05, 'epoch': 23.52}\n","{'loss': 0.8124, 'lr': 2.2133971703312194e-05, 'epoch': 23.55}\n","{'loss': 0.888, 'lr': 2.196928876052652e-05, 'epoch': 23.57}\n","{'loss': 0.8296, 'grad_norm': 1.4482700824737549, 'learning_rate': 2.1805145116287807e-05, 'epoch': 23.6}\n","{'loss': 0.8293, 'lr': 2.1805145116287807e-05, 'epoch': 23.6}\n","{'loss': 0.7975, 'lr': 2.164154190505231e-05, 'epoch': 23.62}\n","{'loss': 0.7295, 'lr': 2.1478480257541177e-05, 'epoch': 23.65}\n","{'loss': 0.8688, 'lr': 2.1315961300732557e-05, 'epoch': 23.68}\n","{'loss': 0.8186, 'lr': 2.1153986157853923e-05, 'epoch': 23.7}\n","{'loss': 0.8249, 'lr': 2.099255594837424e-05, 'epoch': 23.73}\n","{'loss': 0.8389, 'lr': 2.083167178799623e-05, 'epoch': 23.75}\n","{'loss': 0.8534, 'lr': 2.067133478864869e-05, 'epoch': 23.77}\n","{'loss': 0.8201, 'grad_norm': 1.6789214611053467, 'learning_rate': 2.0511546058478793e-05, 'epoch': 23.8}\n","{'loss': 0.795, 'lr': 2.0511546058478793e-05, 'epoch': 23.8}\n","{'loss': 0.7853, 'lr': 2.035230670184436e-05, 'epoch': 23.82}\n","{'loss': 0.8312, 'lr': 2.0193617819306353e-05, 'epoch': 23.85}\n","{'loss': 0.7959, 'lr': 2.0035480507621218e-05, 'epoch': 23.88}\n","{'loss': 0.9319, 'lr': 1.9877895859733252e-05, 'epoch': 23.9}\n","{'loss': 0.7713, 'lr': 1.972086496476716e-05, 'epoch': 23.93}\n","{'loss': 0.8036, 'lr': 1.956438890802034e-05, 'epoch': 23.95}\n","{'loss': 0.726, 'lr': 1.94084687709556e-05, 'epoch': 23.98}\n","{'loss': 0.805, 'grad_norm': 0.7456883192062378, 'learning_rate': 1.925310563119358e-05, 'epoch': 24.0}\n"," 80% 960/1200 [04:39<00:57,  4.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8125, 'lr': 1.925310563119358e-05, 'epoch': 24.0}\n","{'loss': 0.8408, 'lr': 1.9098300562505266e-05, 'epoch': 24.02}\n","{'loss': 0.8186, 'lr': 1.8944054634804666e-05, 'epoch': 24.05}\n","{'loss': 0.8525, 'lr': 1.879036891414129e-05, 'epoch': 24.07}\n","{'loss': 0.78, 'lr': 1.8637244462692915e-05, 'epoch': 24.1}\n","{'loss': 0.867, 'lr': 1.8484682338758152e-05, 'epoch': 24.12}\n","{'loss': 0.7756, 'lr': 1.833268359674919e-05, 'epoch': 24.15}\n","{'loss': 0.838, 'lr': 1.8181249287184466e-05, 'epoch': 24.18}\n","{'loss': 0.8231, 'grad_norm': 0.9394375085830688, 'learning_rate': 1.8030380456681396e-05, 'epoch': 24.2}\n","{'loss': 0.7967, 'lr': 1.8030380456681396e-05, 'epoch': 24.2}\n","{'loss': 0.7598, 'lr': 1.7880078147949207e-05, 'epoch': 24.23}\n","{'loss': 0.7849, 'lr': 1.7730343399781668e-05, 'epoch': 24.25}\n","{'loss': 0.8105, 'lr': 1.758117724704995e-05, 'epoch': 24.27}\n","{'loss': 0.7933, 'lr': 1.743258072069539e-05, 'epoch': 24.3}\n","{'loss': 0.7108, 'lr': 1.72845548477225e-05, 'epoch': 24.32}\n","{'loss': 0.8087, 'lr': 1.7137100651191773e-05, 'epoch': 24.35}\n","{'loss': 0.8046, 'lr': 1.699021915021266e-05, 'epoch': 24.38}\n","{'loss': 0.7837, 'grad_norm': 1.49302077293396, 'learning_rate': 1.684391135993647e-05, 'epoch': 24.4}\n","{'loss': 0.7824, 'lr': 1.684391135993647e-05, 'epoch': 24.4}\n","{'loss': 0.9696, 'lr': 1.6698178291549417e-05, 'epoch': 24.43}\n","{'loss': 0.7691, 'lr': 1.6553020952265587e-05, 'epoch': 24.45}\n","{'loss': 0.7663, 'lr': 1.640844034532002e-05, 'epoch': 24.48}\n","{'loss': 0.7768, 'lr': 1.6264437469961703e-05, 'epoch': 24.5}\n","{'loss': 0.8207, 'lr': 1.6121013321446733e-05, 'epoch': 24.52}\n","{'loss': 0.7814, 'lr': 1.597816889103144e-05, 'epoch': 24.55}\n","{'loss': 0.7656, 'lr': 1.583590516596548e-05, 'epoch': 24.57}\n","{'loss': 0.804, 'grad_norm': 1.0355387926101685, 'learning_rate': 1.5694223129485075e-05, 'epoch': 24.6}\n","{'loss': 0.7869, 'lr': 1.5694223129485075e-05, 'epoch': 24.6}\n","{'loss': 0.7332, 'lr': 1.5553123760806143e-05, 'epoch': 24.62}\n","{'loss': 0.7649, 'lr': 1.5412608035117604e-05, 'epoch': 24.65}\n","{'loss': 0.7866, 'lr': 1.5272676923574637e-05, 'epoch': 24.68}\n","{'loss': 0.8403, 'lr': 1.513333139329186e-05, 'epoch': 24.7}\n","{'loss': 0.8552, 'lr': 1.4994572407336783e-05, 'epoch': 24.73}\n","{'loss': 0.7844, 'lr': 1.4856400924723079e-05, 'epoch': 24.75}\n","{'loss': 0.806, 'lr': 1.4718817900404014e-05, 'epoch': 24.77}\n","{'loss': 0.7947, 'grad_norm': 1.2456119060516357, 'learning_rate': 1.4581824285265733e-05, 'epoch': 24.8}\n","{'loss': 0.7525, 'lr': 1.4581824285265733e-05, 'epoch': 24.8}\n","{'loss': 0.8516, 'lr': 1.444542102612082e-05, 'epoch': 24.82}\n","{'loss': 0.9917, 'lr': 1.4309609065701656e-05, 'epoch': 24.85}\n","{'loss': 0.7557, 'lr': 1.4174389342653971e-05, 'epoch': 24.88}\n","{'loss': 0.7921, 'lr': 1.4039762791530343e-05, 'epoch': 24.9}\n","{'loss': 0.8404, 'lr': 1.3905730342783629e-05, 'epoch': 24.93}\n","{'loss': 0.8373, 'lr': 1.3772292922760732e-05, 'epoch': 24.95}\n","{'loss': 0.8013, 'lr': 1.363945145369606e-05, 'epoch': 24.98}\n","{'loss': 0.8278, 'grad_norm': 1.1476290225982666, 'learning_rate': 1.3507206853705178e-05, 'epoch': 25.0}\n"," 83% 1000/1200 [04:51<00:48,  4.15it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7888, 'lr': 1.3507206853705178e-05, 'epoch': 25.0}\n","{'loss': 0.777, 'lr': 1.3375560036778501e-05, 'epoch': 25.02}\n","{'loss': 0.7777, 'lr': 1.3244511912774905e-05, 'epoch': 25.05}\n","{'loss': 0.7748, 'lr': 1.311406338741551e-05, 'epoch': 25.07}\n","{'loss': 0.843, 'lr': 1.298421536227743e-05, 'epoch': 25.1}\n","{'loss': 0.7488, 'lr': 1.2854968734787398e-05, 'epoch': 25.12}\n","{'loss': 0.849, 'lr': 1.2726324398215762e-05, 'epoch': 25.15}\n","{'loss': 0.766, 'lr': 1.2598283241670195e-05, 'epoch': 25.18}\n","{'loss': 0.7906, 'grad_norm': 1.0758745670318604, 'learning_rate': 1.2470846150089566e-05, 'epoch': 25.2}\n","{'loss': 0.773, 'lr': 1.2470846150089566e-05, 'epoch': 25.2}\n","{'loss': 0.7838, 'lr': 1.2344014004237859e-05, 'epoch': 25.23}\n","{'loss': 0.8041, 'lr': 1.221778768069799e-05, 'epoch': 25.25}\n","{'loss': 0.7679, 'lr': 1.2092168051865892e-05, 'epoch': 25.27}\n","{'loss': 0.6972, 'lr': 1.1967155985944367e-05, 'epoch': 25.3}\n","{'loss': 0.7017, 'lr': 1.184275234693717e-05, 'epoch': 25.32}\n","{'loss': 0.7514, 'lr': 1.1718957994642965e-05, 'epoch': 25.35}\n","{'loss': 0.8808, 'lr': 1.1595773784649389e-05, 'epoch': 25.38}\n","{'loss': 0.77, 'grad_norm': 6.8205246925354, 'learning_rate': 1.1473200568327224e-05, 'epoch': 25.4}\n","{'loss': 0.7775, 'lr': 1.1473200568327224e-05, 'epoch': 25.4}\n","{'loss': 0.7993, 'lr': 1.1351239192824415e-05, 'epoch': 25.43}\n","{'loss': 0.7999, 'lr': 1.1229890501060258e-05, 'epoch': 25.45}\n","{'loss': 0.7136, 'lr': 1.11091553317196e-05, 'epoch': 25.48}\n","{'loss': 0.81, 'lr': 1.0989034519246954e-05, 'epoch': 25.5}\n","{'loss': 0.8214, 'lr': 1.0869528893840875e-05, 'epoch': 25.52}\n","{'loss': 0.7176, 'lr': 1.0750639281448017e-05, 'epoch': 25.55}\n","{'loss': 0.7445, 'lr': 1.0632366503757652e-05, 'epoch': 25.57}\n","{'loss': 0.773, 'grad_norm': 0.8475699424743652, 'learning_rate': 1.051471137819583e-05, 'epoch': 25.6}\n","{'loss': 0.7564, 'lr': 1.051471137819583e-05, 'epoch': 25.6}\n","{'loss': 0.7658, 'lr': 1.0397674717919802e-05, 'epoch': 25.62}\n","{'loss': 0.7775, 'lr': 1.0281257331812377e-05, 'epoch': 25.65}\n","{'loss': 0.7695, 'lr': 1.0165460024476337e-05, 'epoch': 25.68}\n","{'loss': 0.7757, 'lr': 1.0050283596228871e-05, 'epoch': 25.7}\n","{'loss': 0.8001, 'lr': 9.93572884309607e-06, 'epoch': 25.73}\n","{'loss': 0.7709, 'lr': 9.821796556807339e-06, 'epoch': 25.75}\n","{'loss': 0.8104, 'lr': 9.708487524790045e-06, 'epoch': 25.77}\n","{'loss': 0.7783, 'grad_norm': 0.8498784899711609, 'learning_rate': 9.595802530164011e-06, 'epoch': 25.8}\n","{'loss': 0.8528, 'lr': 9.595802530164011e-06, 'epoch': 25.8}\n","{'loss': 0.8016, 'lr': 9.483742351736113e-06, 'epoch': 25.82}\n","{'loss': 0.7523, 'lr': 9.372307763994891e-06, 'epoch': 25.85}\n","{'loss': 0.9647, 'lr': 9.261499537105177e-06, 'epoch': 25.88}\n","{'loss': 0.78, 'lr': 9.151318436902834e-06, 'epoch': 25.9}\n","{'loss': 0.801, 'lr': 9.041765224889399e-06, 'epoch': 25.93}\n","{'loss': 0.8008, 'lr': 8.93284065822687e-06, 'epoch': 25.95}\n","{'loss': 0.7611, 'lr': 8.824545489732417e-06, 'epoch': 25.98}\n","{'loss': 0.8143, 'grad_norm': 1.1111817359924316, 'learning_rate': 8.716880467873234e-06, 'epoch': 26.0}\n"," 87% 1040/1200 [05:03<00:37,  4.31it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7428, 'lr': 8.716880467873234e-06, 'epoch': 26.0}\n","{'loss': 0.8011, 'lr': 8.609846336761329e-06, 'epoch': 26.02}\n","{'loss': 0.8259, 'lr': 8.503443836148416e-06, 'epoch': 26.05}\n","{'loss': 0.7851, 'lr': 8.39767370142076e-06, 'epoch': 26.07}\n","{'loss': 0.7289, 'lr': 8.29253666359413e-06, 'epoch': 26.1}\n","{'loss': 0.7867, 'lr': 8.188033449308719e-06, 'epoch': 26.12}\n","{'loss': 0.7772, 'lr': 8.084164780824178e-06, 'epoch': 26.15}\n","{'loss': 0.7534, 'lr': 7.9809313760145e-06, 'epoch': 26.18}\n","{'loss': 0.7751, 'grad_norm': 2.5431861877441406, 'learning_rate': 7.878333948363203e-06, 'epoch': 26.2}\n","{'loss': 0.776, 'lr': 7.878333948363203e-06, 'epoch': 26.2}\n","{'loss': 0.8142, 'lr': 7.776373206958332e-06, 'epoch': 26.23}\n","{'loss': 0.7727, 'lr': 7.675049856487549e-06, 'epoch': 26.25}\n","{'loss': 0.7438, 'lr': 7.574364597233275e-06, 'epoch': 26.27}\n","{'loss': 0.7488, 'lr': 7.4743181250678475e-06, 'epoch': 26.3}\n","{'loss': 0.7742, 'lr': 7.374911131448747e-06, 'epoch': 26.32}\n","{'loss': 0.749, 'lr': 7.276144303413757e-06, 'epoch': 26.35}\n","{'loss': 0.7506, 'lr': 7.178018323576208e-06, 'epoch': 26.38}\n","{'loss': 0.7662, 'grad_norm': 1.0651143789291382, 'learning_rate': 7.080533870120376e-06, 'epoch': 26.4}\n","{'loss': 0.8021, 'lr': 7.080533870120376e-06, 'epoch': 26.4}\n","{'loss': 0.7436, 'lr': 6.983691616796672e-06, 'epoch': 26.43}\n","{'loss': 0.7249, 'lr': 6.887492232917025e-06, 'epoch': 26.45}\n","{'loss': 0.7676, 'lr': 6.791936383350261e-06, 'epoch': 26.48}\n","{'loss': 0.8073, 'lr': 6.6970247285175315e-06, 'epoch': 26.5}\n","{'loss': 0.7414, 'lr': 6.602757924387715e-06, 'epoch': 26.52}\n","{'loss': 0.8608, 'lr': 6.5091366224728554e-06, 'epoch': 26.55}\n","{'loss': 0.8074, 'lr': 6.416161469823734e-06, 'epoch': 26.57}\n","{'loss': 0.7819, 'grad_norm': 1.1004804372787476, 'learning_rate': 6.323833109025368e-06, 'epoch': 26.6}\n","{'loss': 0.795, 'lr': 6.323833109025368e-06, 'epoch': 26.6}\n","{'loss': 0.7599, 'lr': 6.23215217819253e-06, 'epoch': 26.62}\n","{'loss': 0.825, 'lr': 6.1411193109654155e-06, 'epoch': 26.65}\n","{'loss': 0.7886, 'lr': 6.0507351365051475e-06, 'epoch': 26.68}\n","{'loss': 0.7272, 'lr': 5.961000279489559e-06, 'epoch': 26.7}\n","{'loss': 0.9128, 'lr': 5.871915360108781e-06, 'epoch': 26.73}\n","{'loss': 0.7547, 'lr': 5.7834809940610195e-06, 'epoch': 26.75}\n","{'loss': 0.7202, 'lr': 5.69569779254826e-06, 'epoch': 26.77}\n","{'loss': 0.7854, 'grad_norm': 1.1841115951538086, 'learning_rate': 5.608566362272017e-06, 'epoch': 26.8}\n","{'loss': 0.7741, 'lr': 5.608566362272017e-06, 'epoch': 26.8}\n","{'loss': 0.7894, 'lr': 5.522087305429214e-06, 'epoch': 26.82}\n","{'loss': 0.8349, 'lr': 5.436261219707994e-06, 'epoch': 26.85}\n","{'loss': 0.8341, 'lr': 5.351088698283558e-06, 'epoch': 26.88}\n","{'loss': 0.75, 'lr': 5.266570329814091e-06, 'epoch': 26.9}\n","{'loss': 0.7828, 'lr': 5.18270669843669e-06, 'epoch': 26.93}\n","{'loss': 0.8021, 'lr': 5.099498383763346e-06, 'epoch': 26.95}\n","{'loss': 0.8226, 'lr': 5.016945960876884e-06, 'epoch': 26.98}\n","{'loss': 0.7988, 'grad_norm': 1.0387957096099854, 'learning_rate': 4.9350500003270465e-06, 'epoch': 27.0}\n"," 90% 1080/1200 [05:15<00:30,  3.90it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7694, 'lr': 4.9350500003270465e-06, 'epoch': 27.0}\n","{'loss': 0.7358, 'lr': 4.853811068126502e-06, 'epoch': 27.02}\n","{'loss': 0.7507, 'lr': 4.773229725747008e-06, 'epoch': 27.05}\n","{'loss': 0.8576, 'lr': 4.693306530115415e-06, 'epoch': 27.07}\n","{'loss': 0.7733, 'lr': 4.614042033609945e-06, 'epoch': 27.1}\n","{'loss': 0.7265, 'lr': 4.535436784056269e-06, 'epoch': 27.12}\n","{'loss': 0.8612, 'lr': 4.457491324723784e-06, 'epoch': 27.15}\n","{'loss': 0.7866, 'lr': 4.380206194321812e-06, 'epoch': 27.18}\n","{'loss': 0.7826, 'grad_norm': 1.1529045104980469, 'learning_rate': 4.303581926995959e-06, 'epoch': 27.2}\n","{'loss': 0.8334, 'lr': 4.303581926995959e-06, 'epoch': 27.2}\n","{'loss': 0.7681, 'lr': 4.227619052324283e-06, 'epoch': 27.23}\n","{'loss': 0.817, 'lr': 4.152318095313778e-06, 'epoch': 27.25}\n","{'loss': 0.7741, 'lr': 4.077679576396676e-06, 'epoch': 27.27}\n","{'loss': 0.7864, 'lr': 4.003704011426845e-06, 'epoch': 27.3}\n","{'loss': 0.7713, 'lr': 3.930391911676256e-06, 'epoch': 27.32}\n","{'loss': 0.8154, 'lr': 3.857743783831413e-06, 'epoch': 27.35}\n","{'loss': 0.7682, 'lr': 3.785760129989868e-06, 'epoch': 27.38}\n","{'loss': 0.7917, 'grad_norm': 1.2795332670211792, 'learning_rate': 3.714441447656791e-06, 'epoch': 27.4}\n","{'loss': 0.7493, 'lr': 3.714441447656791e-06, 'epoch': 27.4}\n","{'loss': 0.8065, 'lr': 3.6437882297414292e-06, 'epoch': 27.43}\n","{'loss': 0.7634, 'lr': 3.5738009645538195e-06, 'epoch': 27.45}\n","{'loss': 0.7423, 'lr': 3.5044801358013136e-06, 'epoch': 27.48}\n","{'loss': 0.7524, 'lr': 3.4358262225853254e-06, 'epoch': 27.5}\n","{'loss': 0.7734, 'lr': 3.3678396993979343e-06, 'epoch': 27.52}\n","{'loss': 0.7539, 'lr': 3.3005210361186643e-06, 'epoch': 27.55}\n","{'loss': 0.8009, 'lr': 3.23387069801121e-06, 'epoch': 27.57}\n","{'loss': 0.7678, 'grad_norm': 1.1937283277511597, 'learning_rate': 3.1678891457202597e-06, 'epoch': 27.6}\n","{'loss': 0.8084, 'lr': 3.1678891457202597e-06, 'epoch': 27.6}\n","{'loss': 0.7092, 'lr': 3.102576835268212e-06, 'epoch': 27.62}\n","{'loss': 0.7135, 'lr': 3.037934218052152e-06, 'epoch': 27.65}\n","{'loss': 0.7882, 'lr': 2.9739617408406585e-06, 'epoch': 27.68}\n","{'loss': 0.7506, 'lr': 2.9106598457707134e-06, 'epoch': 27.7}\n","{'loss': 0.7395, 'lr': 2.8480289703446737e-06, 'epoch': 27.73}\n","{'loss': 0.8029, 'lr': 2.7860695474272392e-06, 'epoch': 27.75}\n","{'loss': 0.8021, 'lr': 2.7247820052424657e-06, 'epoch': 27.77}\n","{'loss': 0.7643, 'grad_norm': 1.9571030139923096, 'learning_rate': 2.664166767370757e-06, 'epoch': 27.8}\n","{'loss': 0.7258, 'lr': 2.664166767370757e-06, 'epoch': 27.8}\n","{'loss': 0.7273, 'lr': 2.6042242527460457e-06, 'epoch': 27.82}\n","{'loss': 0.7632, 'lr': 2.544954875652761e-06, 'epoch': 27.85}\n","{'loss': 0.8029, 'lr': 2.4863590457230743e-06, 'epoch': 27.88}\n","{'loss': 0.7712, 'lr': 2.4284371679340166e-06, 'epoch': 27.9}\n","{'loss': 0.8154, 'lr': 2.3711896426046987e-06, 'epoch': 27.93}\n","{'loss': 0.7737, 'lr': 2.3146168653935374e-06, 'epoch': 27.95}\n","{'loss': 0.7564, 'lr': 2.2587192272954917e-06, 'epoch': 27.98}\n","{'loss': 0.767, 'grad_norm': 5.3875508308410645, 'learning_rate': 2.20349711463943e-06, 'epoch': 28.0}\n"," 93% 1120/1200 [05:27<00:20,  3.86it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7929, 'lr': 2.20349711463943e-06, 'epoch': 28.0}\n","{'loss': 0.7389, 'lr': 2.148950909085401e-06, 'epoch': 28.02}\n","{'loss': 0.7679, 'lr': 2.0950809876220113e-06, 'epoch': 28.05}\n","{'loss': 0.743, 'lr': 2.0418877225638288e-06, 'epoch': 28.07}\n","{'loss': 0.7913, 'lr': 1.989371481548807e-06, 'epoch': 28.1}\n","{'loss': 0.7367, 'lr': 1.9375326275357208e-06, 'epoch': 28.12}\n","{'loss': 0.7404, 'lr': 1.8863715188016994e-06, 'epoch': 28.15}\n","{'loss': 0.752, 'lr': 1.8358885089397205e-06, 'epoch': 28.18}\n","{'loss': 0.7579, 'grad_norm': 0.9977635145187378, 'learning_rate': 1.7860839468561874e-06, 'epoch': 28.2}\n","{'loss': 0.8253, 'lr': 1.7860839468561874e-06, 'epoch': 28.2}\n","{'loss': 0.7009, 'lr': 1.7369581767684774e-06, 'epoch': 28.23}\n","{'loss': 0.7995, 'lr': 1.6885115382026085e-06, 'epoch': 28.25}\n","{'loss': 0.7305, 'lr': 1.6407443659908762e-06, 'epoch': 28.27}\n","{'loss': 0.7196, 'lr': 1.593656990269521e-06, 'epoch': 28.3}\n","{'loss': 0.7529, 'lr': 1.5472497364764859e-06, 'epoch': 28.32}\n","{'loss': 0.8018, 'lr': 1.5015229253491302e-06, 'epoch': 28.35}\n","{'loss': 0.7378, 'lr': 1.4564768729220412e-06, 'epoch': 28.38}\n","{'loss': 0.7585, 'grad_norm': 0.790642261505127, 'learning_rate': 1.4121118905248031e-06, 'epoch': 28.4}\n","{'loss': 0.8051, 'lr': 1.4121118905248031e-06, 'epoch': 28.4}\n","{'loss': 0.7778, 'lr': 1.3684282847799213e-06, 'epoch': 28.43}\n","{'loss': 0.7307, 'lr': 1.3254263576006343e-06, 'epoch': 28.45}\n","{'loss': 0.7585, 'lr': 1.2831064061888498e-06, 'epoch': 28.48}\n","{'loss': 0.811, 'lr': 1.2414687230331123e-06, 'epoch': 28.5}\n","{'loss': 0.7334, 'lr': 1.2005135959065383e-06, 'epoch': 28.52}\n","{'loss': 0.7363, 'lr': 1.1602413078648733e-06, 'epoch': 28.55}\n","{'loss': 0.7967, 'lr': 1.1206521372445044e-06, 'epoch': 28.57}\n","{'loss': 0.7687, 'grad_norm': 0.8695819973945618, 'learning_rate': 1.0817463576605513e-06, 'epoch': 28.6}\n","{'loss': 0.7498, 'lr': 1.0817463576605513e-06, 'epoch': 28.6}\n","{'loss': 0.7561, 'lr': 1.0435242380049559e-06, 'epoch': 28.62}\n","{'loss': 0.7845, 'lr': 1.0059860424446842e-06, 'epoch': 28.65}\n","{'loss': 0.7819, 'lr': 9.691320304197837e-07, 'epoch': 28.68}\n","{'loss': 0.7895, 'lr': 9.329624566417172e-07, 'epoch': 28.7}\n","{'loss': 0.7575, 'lr': 8.974775710915207e-07, 'epoch': 28.73}\n","{'loss': 0.7434, 'lr': 8.626776190181041e-07, 'epoch': 28.75}\n","{'loss': 0.7204, 'lr': 8.285628409365753e-07, 'epoch': 28.77}\n","{'loss': 0.7604, 'grad_norm': 0.7090493440628052, 'learning_rate': 7.951334726265191e-07, 'epoch': 28.8}\n","{'loss': 0.8391, 'lr': 7.951334726265191e-07, 'epoch': 28.8}\n","{'loss': 0.7696, 'lr': 7.623897451304318e-07, 'epoch': 28.82}\n","{'loss': 0.7933, 'lr': 7.303318847521001e-07, 'epoch': 28.85}\n","{'loss': 0.7735, 'lr': 6.98960113055025e-07, 'epoch': 28.88}\n","{'loss': 0.7382, 'lr': 6.682746468608781e-07, 'epoch': 28.9}\n","{'loss': 0.8257, 'lr': 6.382756982480587e-07, 'epoch': 28.93}\n","{'loss': 0.805, 'lr': 6.089634745501838e-07, 'epoch': 28.95}\n","{'loss': 0.7444, 'lr': 5.803381783546669e-07, 'epoch': 28.98}\n","{'loss': 0.7861, 'grad_norm': 0.9644982814788818, 'learning_rate': 5.52400007501297e-07, 'epoch': 29.0}\n"," 97% 1160/1200 [05:38<00:10,  3.89it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7999, 'lr': 5.52400007501297e-07, 'epoch': 29.0}\n","{'loss': 0.7773, 'lr': 5.251491550809062e-07, 'epoch': 29.02}\n","{'loss': 0.746, 'lr': 4.985858094340379e-07, 'epoch': 29.05}\n","{'loss': 0.771, 'lr': 4.727101541496026e-07, 'epoch': 29.07}\n","{'loss': 0.7538, 'lr': 4.4752236806363535e-07, 'epoch': 29.1}\n","{'loss': 0.7887, 'lr': 4.230226252580516e-07, 'epoch': 29.12}\n","{'loss': 0.745, 'lr': 3.9921109505947075e-07, 'epoch': 29.15}\n","{'loss': 0.8075, 'lr': 3.76087942038017e-07, 'epoch': 29.18}\n","{'loss': 0.7737, 'grad_norm': 1.0861129760742188, 'learning_rate': 3.5365332600617586e-07, 'epoch': 29.2}\n","{'loss': 0.7803, 'lr': 3.5365332600617586e-07, 'epoch': 29.2}\n","{'loss': 0.7687, 'lr': 3.319074020177282e-07, 'epoch': 29.23}\n","{'loss': 0.7621, 'lr': 3.108503203666402e-07, 'epoch': 29.25}\n","{'loss': 0.7286, 'lr': 2.904822265860307e-07, 'epoch': 29.27}\n","{'loss': 0.7706, 'lr': 2.7080326144719447e-07, 'epoch': 29.3}\n","{'loss': 0.7618, 'lr': 2.5181356095858033e-07, 'epoch': 29.32}\n","{'loss': 0.7595, 'lr': 2.335132563649145e-07, 'epoch': 29.35}\n","{'loss': 0.8454, 'lr': 2.1590247414624566e-07, 'epoch': 29.38}\n","{'loss': 0.7721, 'grad_norm': 1.2001487016677856, 'learning_rate': 1.989813360170456e-07, 'epoch': 29.4}\n","{'loss': 0.7509, 'lr': 1.989813360170456e-07, 'epoch': 29.4}\n","{'loss': 0.7484, 'lr': 1.8274995892546555e-07, 'epoch': 29.43}\n","{'loss': 0.7538, 'lr': 1.6720845505242555e-07, 'epoch': 29.45}\n","{'loss': 0.7131, 'lr': 1.5235693181090417e-07, 'epoch': 29.48}\n","{'loss': 0.7737, 'lr': 1.3819549184516112e-07, 'epoch': 29.5}\n","{'loss': 0.8444, 'lr': 1.24724233030038e-07, 'epoch': 29.52}\n","{'loss': 0.7558, 'lr': 1.1194324847030313e-07, 'epoch': 29.55}\n","{'loss': 0.8178, 'lr': 9.98526264999633e-08, 'epoch': 29.57}\n","{'loss': 0.7697, 'grad_norm': 1.1846064329147339, 'learning_rate': 8.84524506817086e-08, 'epoch': 29.6}\n","{'loss': 0.7492, 'lr': 8.84524506817086e-08, 'epoch': 29.6}\n","{'loss': 0.7599, 'lr': 7.774279980626853e-08, 'epoch': 29.62}\n","{'loss': 0.7979, 'lr': 6.772374789194569e-08, 'epoch': 29.65}\n","{'loss': 0.7561, 'lr': 5.839536418401625e-08, 'epoch': 29.68}\n","{'loss': 0.7337, 'lr': 4.975771315435252e-08, 'epoch': 29.7}\n","{'loss': 0.74, 'lr': 4.181085450087885e-08, 'epoch': 29.73}\n","{'loss': 0.795, 'lr': 3.4554843147216464e-08, 'epoch': 29.75}\n","{'loss': 0.7221, 'lr': 2.7989729242294814e-08, 'epoch': 29.77}\n","{'loss': 0.7567, 'grad_norm': 1.1157734394073486, 'learning_rate': 2.2115558159985226e-08, 'epoch': 29.8}\n","{'loss': 0.7496, 'lr': 2.2115558159985226e-08, 'epoch': 29.8}\n","{'loss': 0.7689, 'lr': 1.693237049883445e-08, 'epoch': 29.82}\n","{'loss': 0.7766, 'lr': 1.2440202081731578e-08, 'epoch': 29.85}\n","{'loss': 0.8001, 'lr': 8.639083955663818e-09, 'epoch': 29.88}\n","{'loss': 0.7503, 'lr': 5.529042391549944e-09, 'epoch': 29.9}\n","{'loss': 0.778, 'lr': 3.110098884007151e-09, 'epoch': 29.93}\n","{'loss': 0.7428, 'lr': 1.3822701512400394e-09, 'epoch': 29.95}\n","{'loss': 0.7961, 'lr': 3.4556813489627914e-10, 'epoch': 29.98}\n","{'loss': 0.7703, 'grad_norm': 1.2565157413482666, 'learning_rate': 0.0, 'epoch': 30.0}\n","{'train_runtime': 352.7954, 'train_samples_per_second': 6.803, 'train_steps_per_second': 3.401, 'train_loss': 1.6476323358217875, 'epoch': 30.0}\n","100% 1200/1200 [05:52<00:00,  3.40it/s]\n","******model_save_path is model7b_M1_family_epoch_30_r_64_moreData/adapter_model.safetensors******\n","/content/drive/MyDrive/Colab Notebooks/Identity membership Leakage\n","auto\n","False\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","Parameter containing:\n","tensor([[-0.9414, -0.0864,  0.2773,  ..., -0.2969, -0.0210,  0.2910],\n","        [-0.2988, -0.0025,  0.0732,  ..., -0.1553,  0.1201,  0.0591],\n","        [ 0.2871,  0.0098, -0.0084,  ..., -0.0141, -0.0535, -0.0383],\n","        ...,\n","        [-0.7500, -0.0122,  0.1377,  ..., -0.1729, -0.0525,  0.2402],\n","        [-0.7266,  0.0110,  0.0593,  ..., -0.1279,  0.0209,  0.0898],\n","        [-0.9414, -0.0859,  0.2773,  ..., -0.2969, -0.0219,  0.2910]],\n","       device='cuda:0', dtype=torch.bfloat16)\n","None\n","Generating train split: 120 examples [00:00, 564.87 examples/s]\n","<start_of_turn>user\n","<end_of_turn>\n","\n","<start_of_turn>model\n","Ghadows Mala is Fright Ann's mom.<end_of_turn>\n","{'input_ids': [2, 106, 1645, 108, 107, 108], 'labels': [106, 2516, 108, 235319, 51369, 235256, 70337, 603, 215310, 7600, 235303, 235256, 3278, 235265, 107, 1]}\n","Map: 100% 120/120 [00:00<00:00, 1467.68 examples/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","  0% 0/1800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 40.6448, 'lr': 0.0, 'epoch': 0}\n","{'loss': 40.6448, 'grad_norm': 8.276817321777344, 'learning_rate': 4e-05, 'epoch': 0.02}\n","{'loss': 40.8365, 'lr': 4e-05, 'epoch': 0.02}\n","{'loss': 40.7903, 'lr': 8e-05, 'epoch': 0.03}\n","{'loss': 37.8943, 'lr': 0.00012, 'epoch': 0.05}\n","{'loss': 35.2245, 'lr': 0.00016, 'epoch': 0.07}\n","{'loss': 33.4988, 'lr': 0.0002, 'epoch': 0.08}\n","{'loss': 32.5983, 'lr': 0.00019999984684163097, 'epoch': 0.1}\n","{'loss': 30.2418, 'lr': 0.0001999993873669929, 'epoch': 0.12}\n","{'loss': 35.8692, 'grad_norm': 11.962006568908691, 'learning_rate': 0.0001999986215774933, 'epoch': 0.13}\n","{'loss': 29.4715, 'lr': 0.0001999986215774933, 'epoch': 0.13}\n","{'loss': 31.3936, 'lr': 0.00019999754947547797, 'epoch': 0.15}\n","{'loss': 27.0589, 'lr': 0.00019999617106423082, 'epoch': 0.17}\n","{'loss': 26.1201, 'lr': 0.00019999448634797423, 'epoch': 0.18}\n","{'loss': 24.801, 'lr': 0.00019999249533186875, 'epoch': 0.2}\n","{'loss': 24.8029, 'lr': 0.00019999019802201316, 'epoch': 0.22}\n","{'loss': 23.56, 'lr': 0.00019998759442544463, 'epoch': 0.23}\n","{'loss': 20.5912, 'lr': 0.00019998468455013823, 'epoch': 0.25}\n","{'loss': 25.9749, 'grad_norm': 15.852333068847656, 'learning_rate': 0.0001999814684050075, 'epoch': 0.27}\n","{'loss': 18.9593, 'lr': 0.0001999814684050075, 'epoch': 0.27}\n","{'loss': 16.7246, 'lr': 0.00019997794599990404, 'epoch': 0.28}\n","{'loss': 15.8963, 'lr': 0.00019997411734561755, 'epoch': 0.3}\n","{'loss': 15.7536, 'lr': 0.00019996998245387583, 'epoch': 0.32}\n","{'loss': 13.0341, 'lr': 0.00019996554133734474, 'epoch': 0.33}\n","{'loss': 10.8206, 'lr': 0.00019996079400962818, 'epoch': 0.35}\n","{'loss': 9.5808, 'lr': 0.00019995574048526797, 'epoch': 0.37}\n","{'loss': 6.9907, 'lr': 0.00019995038077974398, 'epoch': 0.38}\n","{'loss': 13.47, 'grad_norm': 21.114675521850586, 'learning_rate': 0.0001999447149094738, 'epoch': 0.4}\n","{'loss': 5.4792, 'lr': 0.0001999447149094738, 'epoch': 0.4}\n","{'loss': 5.2039, 'lr': 0.000199938742891813, 'epoch': 0.42}\n","{'loss': 4.6087, 'lr': 0.00019993246474505483, 'epoch': 0.43}\n","{'loss': 4.9165, 'lr': 0.00019992588048843032, 'epoch': 0.45}\n","{'loss': 5.931, 'lr': 0.00019991899014210816, 'epoch': 0.47}\n","{'loss': 5.1659, 'lr': 0.00019991179372719458, 'epoch': 0.48}\n","{'loss': 5.1976, 'lr': 0.0001999042912657335, 'epoch': 0.5}\n","{'loss': 4.6526, 'lr': 0.00019989648278070617, 'epoch': 0.52}\n","{'loss': 5.1444, 'grad_norm': 7.054017543792725, 'learning_rate': 0.00019988836829603127, 'epoch': 0.53}\n","{'loss': 4.948, 'lr': 0.00019988836829603127, 'epoch': 0.53}\n","{'loss': 4.5865, 'lr': 0.0001998799478365648, 'epoch': 0.55}\n","{'loss': 4.5715, 'lr': 0.0001998712214281001, 'epoch': 0.57}\n","{'loss': 4.3166, 'lr': 0.00019986218909736757, 'epoch': 0.58}\n","{'loss': 4.2867, 'lr': 0.00019985285087203481, 'epoch': 0.6}\n","{'loss': 4.1103, 'lr': 0.0001998432067807063, 'epoch': 0.62}\n","{'loss': 4.0073, 'lr': 0.00019983325685292354, 'epoch': 0.63}\n","{'loss': 4.0686, 'lr': 0.0001998230011191648, 'epoch': 0.65}\n","{'loss': 4.3619, 'grad_norm': 4.371563911437988, 'learning_rate': 0.00019981243961084515, 'epoch': 0.67}\n","{'loss': 3.8724, 'lr': 0.00019981243961084515, 'epoch': 0.67}\n","{'loss': 4.268, 'lr': 0.00019980157236031625, 'epoch': 0.68}\n","{'loss': 3.5816, 'lr': 0.00019979039940086626, 'epoch': 0.7}\n","{'loss': 3.8859, 'lr': 0.00019977892076671988, 'epoch': 0.72}\n","{'loss': 3.9049, 'lr': 0.00019976713649303806, 'epoch': 0.73}\n","{'loss': 3.8647, 'lr': 0.000199755046615918, 'epoch': 0.75}\n","{'loss': 3.8899, 'lr': 0.00019974265117239304, 'epoch': 0.77}\n","{'loss': 3.6875, 'lr': 0.00019972995020043247, 'epoch': 0.78}\n","{'loss': 3.8694, 'grad_norm': 2.944803237915039, 'learning_rate': 0.00019971694373894152, 'epoch': 0.8}\n","{'loss': 3.688, 'lr': 0.00019971694373894152, 'epoch': 0.8}\n","{'loss': 3.2394, 'lr': 0.00019970363182776114, 'epoch': 0.82}\n","{'loss': 3.7582, 'lr': 0.00019969001450766794, 'epoch': 0.83}\n","{'loss': 4.1587, 'lr': 0.0001996760918203741, 'epoch': 0.85}\n","{'loss': 3.4777, 'lr': 0.00019966186380852705, 'epoch': 0.87}\n","{'loss': 3.5473, 'lr': 0.00019964733051570964, 'epoch': 0.88}\n","{'loss': 3.9585, 'lr': 0.00019963249198643975, 'epoch': 0.9}\n","{'loss': 3.6148, 'lr': 0.00019961734826617035, 'epoch': 0.92}\n","{'loss': 3.6803, 'grad_norm': 18.775314331054688, 'learning_rate': 0.00019960189940128906, 'epoch': 0.93}\n","{'loss': 3.8281, 'lr': 0.00019960189940128906, 'epoch': 0.93}\n","{'loss': 4.1847, 'lr': 0.00019958614543911843, 'epoch': 0.95}\n","{'loss': 3.4623, 'lr': 0.00019957008642791543, 'epoch': 0.97}\n","{'loss': 3.3688, 'lr': 0.00019955372241687157, 'epoch': 0.98}\n","  3% 60/1800 [00:15<07:01,  4.12it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 3.2517, 'lr': 0.0001995370534561125, 'epoch': 1.0}\n","{'loss': 2.8425, 'lr': 0.0001995200795966981, 'epoch': 1.02}\n","{'loss': 2.6876, 'lr': 0.00019950280089062203, 'epoch': 1.03}\n","{'loss': 3.7738, 'lr': 0.00019948521739081193, 'epoch': 1.05}\n","{'loss': 3.4249, 'grad_norm': 3.9568536281585693, 'learning_rate': 0.00019946732915112903, 'epoch': 1.07}\n","{'loss': 2.9049, 'lr': 0.00019946732915112903, 'epoch': 1.07}\n","{'loss': 2.2416, 'lr': 0.00019944913622636795, 'epoch': 1.08}\n","{'loss': 2.7474, 'lr': 0.0001994306386722567, 'epoch': 1.1}\n","{'loss': 3.0275, 'lr': 0.00019941183654545634, 'epoch': 1.12}\n","{'loss': 2.9378, 'lr': 0.000199392729903561, 'epoch': 1.13}\n","{'loss': 2.8197, 'lr': 0.00019937331880509745, 'epoch': 1.15}\n","{'loss': 3.3243, 'lr': 0.00019935360330952518, 'epoch': 1.17}\n","{'loss': 2.55, 'lr': 0.000199333583477236, 'epoch': 1.18}\n","{'loss': 2.8191, 'grad_norm': 3.7259390354156494, 'learning_rate': 0.0001993132593695541, 'epoch': 1.2}\n","{'loss': 2.9701, 'lr': 0.0001993132593695541, 'epoch': 1.2}\n","{'loss': 2.8762, 'lr': 0.0001992926310487355, 'epoch': 1.22}\n","{'loss': 2.8822, 'lr': 0.00019927169857796834, 'epoch': 1.23}\n","{'loss': 2.9045, 'lr': 0.00019925046202137216, 'epoch': 1.25}\n","{'loss': 3.156, 'lr': 0.00019922892144399817, 'epoch': 1.27}\n","{'loss': 3.407, 'lr': 0.0001992070769118287, 'epoch': 1.28}\n","{'loss': 2.6908, 'lr': 0.00019918492849177726, 'epoch': 1.3}\n","{'loss': 3.1987, 'lr': 0.00019916247625168817, 'epoch': 1.32}\n","{'loss': 3.0107, 'grad_norm': 3.5629355907440186, 'learning_rate': 0.00019913972026033632, 'epoch': 1.33}\n","{'loss': 4.0669, 'lr': 0.00019913972026033632, 'epoch': 1.33}\n","{'loss': 3.5043, 'lr': 0.0001991166605874272, 'epoch': 1.35}\n","{'loss': 3.0122, 'lr': 0.00019909329730359644, 'epoch': 1.37}\n","{'loss': 2.9454, 'lr': 0.00019906963048040968, 'epoch': 1.38}\n","{'loss': 2.9134, 'lr': 0.00019904566019036233, 'epoch': 1.4}\n","{'loss': 2.8231, 'lr': 0.00019902138650687942, 'epoch': 1.42}\n","{'loss': 2.8242, 'lr': 0.00019899680950431534, 'epoch': 1.43}\n","{'loss': 2.7478, 'lr': 0.00019897192925795353, 'epoch': 1.45}\n","{'loss': 3.1047, 'grad_norm': 3.509880781173706, 'learning_rate': 0.00019894674584400635, 'epoch': 1.47}\n","{'loss': 2.7756, 'lr': 0.00019894674584400635, 'epoch': 1.47}\n","{'loss': 2.9151, 'lr': 0.0001989212593396148, 'epoch': 1.48}\n","{'loss': 3.1748, 'lr': 0.00019889546982284834, 'epoch': 1.5}\n","{'loss': 2.938, 'lr': 0.00019886937737270455, 'epoch': 1.52}\n","{'loss': 3.1344, 'lr': 0.000198842982069109, 'epoch': 1.53}\n","{'loss': 2.7069, 'lr': 0.00019881628399291487, 'epoch': 1.55}\n","{'loss': 2.5552, 'lr': 0.00019878928322590288, 'epoch': 1.57}\n","{'loss': 2.1768, 'lr': 0.0001987619798507809, 'epoch': 1.58}\n","{'loss': 2.7971, 'grad_norm': 3.3893284797668457, 'learning_rate': 0.00019873437395118374, 'epoch': 1.6}\n","{'loss': 2.4138, 'lr': 0.00019873437395118374, 'epoch': 1.6}\n","{'loss': 3.7142, 'lr': 0.00019870646561167288, 'epoch': 1.62}\n","{'loss': 2.5736, 'lr': 0.0001986782549177362, 'epoch': 1.63}\n","{'loss': 2.7385, 'lr': 0.00019864974195578787, 'epoch': 1.65}\n","{'loss': 2.7278, 'lr': 0.00019862092681316776, 'epoch': 1.67}\n","{'loss': 3.272, 'lr': 0.00019859180957814153, 'epoch': 1.68}\n","{'loss': 2.7306, 'lr': 0.00019856239033990016, 'epoch': 1.7}\n","{'loss': 2.962, 'lr': 0.00019853266918855967, 'epoch': 1.72}\n","{'loss': 2.8916, 'grad_norm': 3.5503499507904053, 'learning_rate': 0.0001985026462151609, 'epoch': 1.73}\n","{'loss': 2.6925, 'lr': 0.0001985026462151609, 'epoch': 1.73}\n","{'loss': 2.8209, 'lr': 0.0001984723215116693, 'epoch': 1.75}\n","{'loss': 2.8494, 'lr': 0.0001984416951709745, 'epoch': 1.77}\n","{'loss': 2.9479, 'lr': 0.00019841076728689005, 'epoch': 1.78}\n","{'loss': 2.5979, 'lr': 0.0001983795379541533, 'epoch': 1.8}\n","{'loss': 2.8489, 'lr': 0.0001983480072684249, 'epoch': 1.82}\n","{'loss': 2.6784, 'lr': 0.00019831617532628862, 'epoch': 1.83}\n","{'loss': 2.7151, 'lr': 0.00019828404222525103, 'epoch': 1.85}\n","{'loss': 2.7689, 'grad_norm': 3.0396056175231934, 'learning_rate': 0.0001982516080637412, 'epoch': 1.87}\n","{'loss': 1.9713, 'lr': 0.0001982516080637412, 'epoch': 1.87}\n","{'loss': 2.327, 'lr': 0.00019821887294111035, 'epoch': 1.88}\n","{'loss': 2.974, 'lr': 0.0001981858369576317, 'epoch': 1.9}\n","{'loss': 2.7233, 'lr': 0.00019815250021449997, 'epoch': 1.92}\n","{'loss': 2.7566, 'lr': 0.00019811886281383124, 'epoch': 1.93}\n","{'loss': 2.9974, 'lr': 0.0001980849248586624, 'epoch': 1.95}\n","{'loss': 2.4907, 'lr': 0.00019805068645295114, 'epoch': 1.97}\n","{'loss': 2.9764, 'lr': 0.00019801614770157544, 'epoch': 1.98}\n","{'loss': 2.6521, 'grad_norm': 4.018974304199219, 'learning_rate': 0.00019798130871033322, 'epoch': 2.0}\n","  7% 120/1800 [00:32<06:48,  4.11it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.7393, 'lr': 0.00019798130871033322, 'epoch': 2.0}\n","{'loss': 1.8376, 'lr': 0.00019794616958594222, 'epoch': 2.02}\n","{'loss': 2.412, 'lr': 0.00019791073043603943, 'epoch': 2.03}\n","{'loss': 2.3491, 'lr': 0.00019787499136918085, 'epoch': 2.05}\n","{'loss': 2.1303, 'lr': 0.00019783895249484129, 'epoch': 2.07}\n","{'loss': 1.7874, 'lr': 0.00019780261392341383, 'epoch': 2.08}\n","{'loss': 2.0048, 'lr': 0.0001977659757662096, 'epoch': 2.1}\n","{'loss': 2.1872, 'lr': 0.00019772903813545736, 'epoch': 2.12}\n","{'loss': 2.056, 'grad_norm': 4.053882122039795, 'learning_rate': 0.00019769180114430332, 'epoch': 2.13}\n","{'loss': 2.2755, 'lr': 0.00019769180114430332, 'epoch': 2.13}\n","{'loss': 2.0933, 'lr': 0.00019765426490681061, 'epoch': 2.15}\n","{'loss': 1.7396, 'lr': 0.00019761642953795895, 'epoch': 2.17}\n","{'loss': 1.9323, 'lr': 0.0001975782951536445, 'epoch': 2.18}\n","{'loss': 1.5353, 'lr': 0.00019753986187067918, 'epoch': 2.2}\n","{'loss': 2.3855, 'lr': 0.00019750112980679063, 'epoch': 2.22}\n","{'loss': 2.4529, 'lr': 0.0001974620990806216, 'epoch': 2.23}\n","{'loss': 2.176, 'lr': 0.00019742276981172976, 'epoch': 2.25}\n","{'loss': 2.0738, 'grad_norm': 5.399962425231934, 'learning_rate': 0.00019738314212058722, 'epoch': 2.27}\n","{'loss': 1.8339, 'lr': 0.00019738314212058722, 'epoch': 2.27}\n","{'loss': 2.1845, 'lr': 0.00019734321612858025, 'epoch': 2.28}\n","{'loss': 1.8741, 'lr': 0.00019730299195800885, 'epoch': 2.3}\n","{'loss': 2.1281, 'lr': 0.0001972624697320864, 'epoch': 2.32}\n","{'loss': 1.5938, 'lr': 0.00019722164957493922, 'epoch': 2.33}\n","{'loss': 2.154, 'lr': 0.0001971805316116063, 'epoch': 2.35}\n","{'loss': 1.6216, 'lr': 0.00019713911596803887, 'epoch': 2.37}\n","{'loss': 1.5998, 'lr': 0.00019709740277109995, 'epoch': 2.38}\n","{'loss': 1.8737, 'grad_norm': 2.7302753925323486, 'learning_rate': 0.00019705539214856404, 'epoch': 2.4}\n","{'loss': 1.9796, 'lr': 0.00019705539214856404, 'epoch': 2.4}\n","{'loss': 1.9809, 'lr': 0.00019701308422911672, 'epoch': 2.42}\n","{'loss': 2.5095, 'lr': 0.00019697047914235427, 'epoch': 2.43}\n","{'loss': 2.6401, 'lr': 0.00019692757701878313, 'epoch': 2.45}\n","{'loss': 1.9202, 'lr': 0.00019688437798981974, 'epoch': 2.47}\n","{'loss': 1.5085, 'lr': 0.00019684088218778988, 'epoch': 2.48}\n","{'loss': 2.1677, 'lr': 0.0001967970897459286, 'epoch': 2.5}\n","{'loss': 2.3368, 'lr': 0.00019675300079837935, 'epoch': 2.52}\n","{'loss': 2.1304, 'grad_norm': 4.609512805938721, 'learning_rate': 0.00019670861548019405, 'epoch': 2.53}\n","{'loss': 2.27, 'lr': 0.00019670861548019405, 'epoch': 2.53}\n","{'loss': 1.9535, 'lr': 0.00019666393392733228, 'epoch': 2.55}\n","{'loss': 2.2196, 'lr': 0.00019661895627666115, 'epoch': 2.57}\n","{'loss': 2.2281, 'lr': 0.00019657368266595476, 'epoch': 2.58}\n","{'loss': 1.6961, 'lr': 0.00019652811323389376, 'epoch': 2.6}\n","{'loss': 2.3025, 'lr': 0.0001964822481200649, 'epoch': 2.62}\n","{'loss': 2.2586, 'lr': 0.0001964360874649607, 'epoch': 2.63}\n","{'loss': 2.0937, 'lr': 0.00019638963140997906, 'epoch': 2.65}\n","{'loss': 2.1278, 'grad_norm': 3.689567804336548, 'learning_rate': 0.00019634288009742255, 'epoch': 2.67}\n","{'loss': 1.6393, 'lr': 0.00019634288009742255, 'epoch': 2.67}\n","{'loss': 2.0469, 'lr': 0.0001962958336704983, 'epoch': 2.68}\n","{'loss': 1.7428, 'lr': 0.0001962484922733174, 'epoch': 2.7}\n","{'loss': 2.2451, 'lr': 0.00019620085605089448, 'epoch': 2.72}\n","{'loss': 1.89, 'lr': 0.00019615292514914726, 'epoch': 2.73}\n","{'loss': 2.3495, 'lr': 0.00019610469971489608, 'epoch': 2.75}\n","{'loss': 1.955, 'lr': 0.00019605617989586357, 'epoch': 2.77}\n","{'loss': 2.4751, 'lr': 0.000196007365840674, 'epoch': 2.78}\n","{'loss': 2.043, 'grad_norm': 7.744349956512451, 'learning_rate': 0.00019595825769885304, 'epoch': 2.8}\n","{'loss': 1.9498, 'lr': 0.00019595825769885304, 'epoch': 2.8}\n","{'loss': 1.706, 'lr': 0.0001959088556208271, 'epoch': 2.82}\n","{'loss': 2.2947, 'lr': 0.0001958591597579231, 'epoch': 2.83}\n","{'loss': 2.312, 'lr': 0.0001958091702623677, 'epoch': 2.85}\n","{'loss': 1.8448, 'lr': 0.00019575888728728713, 'epoch': 2.87}\n","{'loss': 1.7975, 'lr': 0.00019570831098670652, 'epoch': 2.88}\n","{'loss': 1.5238, 'lr': 0.0001956574415155496, 'epoch': 2.9}\n","{'loss': 1.7186, 'lr': 0.00019560627902963807, 'epoch': 2.92}\n","{'loss': 1.8934, 'grad_norm': 4.560531139373779, 'learning_rate': 0.00019555482368569115, 'epoch': 2.93}\n","{'loss': 2.0366, 'lr': 0.00019555482368569115, 'epoch': 2.93}\n","{'loss': 1.7087, 'lr': 0.00019550307564132518, 'epoch': 2.95}\n","{'loss': 1.8853, 'lr': 0.00019545103505505312, 'epoch': 2.97}\n","{'loss': 1.9141, 'lr': 0.00019539870208628397, 'epoch': 2.98}\n"," 10% 180/1800 [00:48<06:45,  3.99it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.7284, 'lr': 0.00019534607689532233, 'epoch': 3.0}\n","{'loss': 1.5793, 'lr': 0.00019529315964336805, 'epoch': 3.02}\n","{'loss': 1.5917, 'lr': 0.00019523995049251547, 'epoch': 3.03}\n","{'loss': 1.4179, 'lr': 0.00019518644960575315, 'epoch': 3.05}\n","{'loss': 1.7327, 'grad_norm': 3.246076822280884, 'learning_rate': 0.00019513265714696324, 'epoch': 3.07}\n","{'loss': 1.67, 'lr': 0.00019513265714696324, 'epoch': 3.07}\n","{'loss': 1.4515, 'lr': 0.00019507857328092108, 'epoch': 3.08}\n","{'loss': 1.6004, 'lr': 0.00019502419817329458, 'epoch': 3.1}\n","{'loss': 1.4125, 'lr': 0.0001949695319906438, 'epoch': 3.12}\n","{'loss': 1.7466, 'lr': 0.00019491457490042043, 'epoch': 3.13}\n","{'loss': 1.3656, 'lr': 0.00019485932707096718, 'epoch': 3.15}\n","{'loss': 1.8241, 'lr': 0.00019480378867151746, 'epoch': 3.17}\n","{'loss': 1.299, 'lr': 0.00019474795987219464, 'epoch': 3.18}\n","{'loss': 1.5462, 'grad_norm': 3.120786428451538, 'learning_rate': 0.0001946918408440117, 'epoch': 3.2}\n","{'loss': 1.6602, 'lr': 0.0001946918408440117, 'epoch': 3.2}\n","{'loss': 1.5044, 'lr': 0.0001946354317588706, 'epoch': 3.22}\n","{'loss': 1.6168, 'lr': 0.00019457873278956185, 'epoch': 3.23}\n","{'loss': 1.4342, 'lr': 0.0001945217441097638, 'epoch': 3.25}\n","{'loss': 1.6012, 'lr': 0.00019446446589404243, 'epoch': 3.27}\n","{'loss': 1.5075, 'lr': 0.0001944068983178504, 'epoch': 3.28}\n","{'loss': 1.7545, 'lr': 0.00019434904155752692, 'epoch': 3.3}\n","{'loss': 1.4944, 'lr': 0.00019429089579029685, 'epoch': 3.32}\n","{'loss': 1.5717, 'grad_norm': 3.050128698348999, 'learning_rate': 0.00019423246119427043, 'epoch': 3.33}\n","{'loss': 1.8707, 'lr': 0.00019423246119427043, 'epoch': 3.33}\n","{'loss': 1.7525, 'lr': 0.00019417373794844265, 'epoch': 3.35}\n","{'loss': 1.4526, 'lr': 0.0001941147262326926, 'epoch': 3.37}\n","{'loss': 1.3118, 'lr': 0.00019405542622778302, 'epoch': 3.38}\n","{'loss': 1.6761, 'lr': 0.00019399583811535983, 'epoch': 3.4}\n","{'loss': 1.5482, 'lr': 0.00019393596207795136, 'epoch': 3.42}\n","{'loss': 1.4607, 'lr': 0.0001938757982989679, 'epoch': 3.43}\n","{'loss': 1.3046, 'lr': 0.00019381534696270118, 'epoch': 3.45}\n","{'loss': 1.5472, 'grad_norm': 2.026163101196289, 'learning_rate': 0.00019375460825432378, 'epoch': 3.47}\n","{'loss': 1.4289, 'lr': 0.00019375460825432378, 'epoch': 3.47}\n","{'loss': 1.5624, 'lr': 0.00019369358235988855, 'epoch': 3.48}\n","{'loss': 1.5818, 'lr': 0.000193632269466328, 'epoch': 3.5}\n","{'loss': 1.6546, 'lr': 0.00019357066976145374, 'epoch': 3.52}\n","{'loss': 1.4638, 'lr': 0.00019350878343395606, 'epoch': 3.53}\n","{'loss': 1.4877, 'lr': 0.0001934466106734031, 'epoch': 3.55}\n","{'loss': 1.5672, 'lr': 0.00019338415167024042, 'epoch': 3.57}\n","{'loss': 1.5593, 'lr': 0.00019332140661579042, 'epoch': 3.58}\n","{'loss': 1.5382, 'grad_norm': 5.018791675567627, 'learning_rate': 0.0001932583757022517, 'epoch': 3.6}\n","{'loss': 1.6764, 'lr': 0.0001932583757022517, 'epoch': 3.6}\n","{'loss': 1.7224, 'lr': 0.00019319505912269847, 'epoch': 3.62}\n","{'loss': 1.7788, 'lr': 0.0001931314570710801, 'epoch': 3.63}\n","{'loss': 1.5794, 'lr': 0.00019306756974222017, 'epoch': 3.65}\n","{'loss': 1.5331, 'lr': 0.00019300339733181642, 'epoch': 3.67}\n","{'loss': 1.6981, 'lr': 0.0001929389400364396, 'epoch': 3.68}\n","{'loss': 1.5538, 'lr': 0.00019287419805353318, 'epoch': 3.7}\n","{'loss': 1.6247, 'lr': 0.00019280917158141274, 'epoch': 3.72}\n","{'loss': 1.6459, 'grad_norm': 12.910163879394531, 'learning_rate': 0.00019274386081926517, 'epoch': 3.73}\n","{'loss': 1.5339, 'lr': 0.00019274386081926517, 'epoch': 3.73}\n","{'loss': 1.5993, 'lr': 0.0001926782659671484, 'epoch': 3.75}\n","{'loss': 1.8582, 'lr': 0.00019261238722599033, 'epoch': 3.77}\n","{'loss': 1.5821, 'lr': 0.00019254622479758862, 'epoch': 3.78}\n","{'loss': 1.2825, 'lr': 0.00019247977888460982, 'epoch': 3.8}\n","{'loss': 1.4372, 'lr': 0.00019241304969058894, 'epoch': 3.82}\n","{'loss': 1.5985, 'lr': 0.00019234603741992862, 'epoch': 3.83}\n","{'loss': 1.9827, 'lr': 0.0001922787422778987, 'epoch': 3.85}\n","{'loss': 1.6093, 'grad_norm': 4.937736988067627, 'learning_rate': 0.00019221116447063543, 'epoch': 3.87}\n","{'loss': 1.6018, 'lr': 0.00019221116447063543, 'epoch': 3.87}\n","{'loss': 1.7007, 'lr': 0.00019214330420514098, 'epoch': 3.88}\n","{'loss': 1.3309, 'lr': 0.00019207516168928267, 'epoch': 3.9}\n","{'loss': 1.3439, 'lr': 0.00019200673713179245, 'epoch': 3.92}\n","{'loss': 1.3648, 'lr': 0.00019193803074226619, 'epoch': 3.93}\n","{'loss': 1.6857, 'lr': 0.00019186904273116307, 'epoch': 3.95}\n","{'loss': 1.6177, 'lr': 0.00019179977330980487, 'epoch': 3.97}\n","{'loss': 1.6544, 'lr': 0.00019173022269037548, 'epoch': 3.98}\n","{'loss': 1.5375, 'grad_norm': 7.878296375274658, 'learning_rate': 0.0001916603910859201, 'epoch': 4.0}\n"," 13% 240/1800 [01:05<07:00,  3.71it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.3273, 'lr': 0.0001916603910859201, 'epoch': 4.0}\n","{'loss': 1.2874, 'lr': 0.00019159027871034452, 'epoch': 4.02}\n","{'loss': 1.2499, 'lr': 0.0001915198857784148, 'epoch': 4.03}\n","{'loss': 1.3185, 'lr': 0.00019144921250575619, 'epoch': 4.05}\n","{'loss': 1.2604, 'lr': 0.0001913782591088528, 'epoch': 4.07}\n","{'loss': 1.5811, 'lr': 0.00019130702580504676, 'epoch': 4.08}\n","{'loss': 1.5642, 'lr': 0.00019123551281253757, 'epoch': 4.1}\n","{'loss': 1.156, 'lr': 0.00019116372035038153, 'epoch': 4.12}\n","{'loss': 1.3431, 'grad_norm': 3.1012942790985107, 'learning_rate': 0.00019109164863849096, 'epoch': 4.13}\n","{'loss': 1.4707, 'lr': 0.00019109164863849096, 'epoch': 4.13}\n","{'loss': 1.4101, 'lr': 0.00019101929789763354, 'epoch': 4.15}\n","{'loss': 1.1423, 'lr': 0.00019094666834943179, 'epoch': 4.17}\n","{'loss': 1.3111, 'lr': 0.00019087376021636207, 'epoch': 4.18}\n","{'loss': 1.2605, 'lr': 0.00019080057372175424, 'epoch': 4.2}\n","{'loss': 1.4847, 'lr': 0.00019072710908979077, 'epoch': 4.22}\n","{'loss': 1.3093, 'lr': 0.00019065336654550618, 'epoch': 4.23}\n","{'loss': 1.3608, 'lr': 0.00019057934631478617, 'epoch': 4.25}\n","{'loss': 1.3437, 'grad_norm': 2.524831533432007, 'learning_rate': 0.00019050504862436709, 'epoch': 4.27}\n","{'loss': 1.2557, 'lr': 0.00019050504862436709, 'epoch': 4.27}\n","{'loss': 1.8494, 'lr': 0.0001904304737018352, 'epoch': 4.28}\n","{'loss': 1.3391, 'lr': 0.00019035562177562604, 'epoch': 4.3}\n","{'loss': 1.2638, 'lr': 0.00019028049307502352, 'epoch': 4.32}\n","{'loss': 1.4523, 'lr': 0.00019020508783015942, 'epoch': 4.33}\n","{'loss': 1.3125, 'lr': 0.0001901294062720127, 'epoch': 4.35}\n","{'loss': 1.3323, 'lr': 0.00019005344863240854, 'epoch': 4.37}\n","{'loss': 1.4554, 'lr': 0.000189977215144018, 'epoch': 4.38}\n","{'loss': 1.4076, 'grad_norm': 2.9977784156799316, 'learning_rate': 0.00018990070604035694, 'epoch': 4.4}\n","{'loss': 1.4018, 'lr': 0.00018990070604035694, 'epoch': 4.4}\n","{'loss': 1.3974, 'lr': 0.0001898239215557856, 'epoch': 4.42}\n","{'loss': 1.3162, 'lr': 0.00018974686192550771, 'epoch': 4.43}\n","{'loss': 1.4865, 'lr': 0.00018966952738556976, 'epoch': 4.45}\n","{'loss': 1.4791, 'lr': 0.00018959191817286044, 'epoch': 4.47}\n","{'loss': 1.5725, 'lr': 0.00018951403452510972, 'epoch': 4.48}\n","{'loss': 1.5102, 'lr': 0.00018943587668088832, 'epoch': 4.5}\n","{'loss': 1.6283, 'lr': 0.00018935744487960673, 'epoch': 4.52}\n","{'loss': 1.474, 'grad_norm': 2.951333522796631, 'learning_rate': 0.0001892787393615147, 'epoch': 4.53}\n","{'loss': 1.3086, 'lr': 0.0001892787393615147, 'epoch': 4.53}\n","{'loss': 1.4148, 'lr': 0.0001891997603677004, 'epoch': 4.55}\n","{'loss': 1.569, 'lr': 0.00018912050814008974, 'epoch': 4.57}\n","{'loss': 1.5093, 'lr': 0.00018904098292144554, 'epoch': 4.58}\n","{'loss': 1.4184, 'lr': 0.00018896118495536688, 'epoch': 4.6}\n","{'loss': 1.3843, 'lr': 0.00018888111448628822, 'epoch': 4.62}\n","{'loss': 1.3405, 'lr': 0.0001888007717594789, 'epoch': 4.63}\n","{'loss': 1.4368, 'lr': 0.00018872015702104205, 'epoch': 4.65}\n","{'loss': 1.4227, 'grad_norm': 2.086186408996582, 'learning_rate': 0.00018863927051791416, 'epoch': 4.67}\n","{'loss': 1.4336, 'lr': 0.00018863927051791416, 'epoch': 4.67}\n","{'loss': 1.3746, 'lr': 0.00018855811249786415, 'epoch': 4.68}\n","{'loss': 1.562, 'lr': 0.00018847668320949254, 'epoch': 4.7}\n","{'loss': 1.5499, 'lr': 0.00018839498290223095, 'epoch': 4.72}\n","{'loss': 1.344, 'lr': 0.00018831301182634105, 'epoch': 4.73}\n","{'loss': 1.4145, 'lr': 0.00018823077023291397, 'epoch': 4.75}\n","{'loss': 1.6579, 'lr': 0.0001881482583738695, 'epoch': 4.77}\n","{'loss': 1.5922, 'lr': 0.00018806547650195525, 'epoch': 4.78}\n","{'loss': 1.4911, 'grad_norm': 7.946610450744629, 'learning_rate': 0.00018798242487074598, 'epoch': 4.8}\n","{'loss': 1.3725, 'lr': 0.00018798242487074598, 'epoch': 4.8}\n","{'loss': 1.3428, 'lr': 0.00018789910373464267, 'epoch': 4.82}\n","{'loss': 1.2652, 'lr': 0.00018781551334887201, 'epoch': 4.83}\n","{'loss': 1.442, 'lr': 0.00018773165396948524, 'epoch': 4.85}\n","{'loss': 1.3375, 'lr': 0.00018764752585335778, 'epoch': 4.87}\n","{'loss': 1.3958, 'lr': 0.00018756312925818805, 'epoch': 4.88}\n","{'loss': 1.205, 'lr': 0.00018747846444249695, 'epoch': 4.9}\n","{'loss': 1.365, 'lr': 0.000187393531665627, 'epoch': 4.92}\n","{'loss': 1.3407, 'grad_norm': 1.8230375051498413, 'learning_rate': 0.00018730833118774153, 'epoch': 4.93}\n","{'loss': 1.7043, 'lr': 0.00018730833118774153, 'epoch': 4.93}\n","{'loss': 1.6391, 'lr': 0.00018722286326982386, 'epoch': 4.95}\n","{'loss': 1.2569, 'lr': 0.00018713712817367651, 'epoch': 4.97}\n","{'loss': 1.5562, 'lr': 0.00018705112616192046, 'epoch': 4.98}\n"," 17% 300/1800 [01:22<05:51,  4.27it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.2167, 'lr': 0.0001869648574979942, 'epoch': 5.0}\n","{'loss': 1.0571, 'lr': 0.0001868783224461532, 'epoch': 5.02}\n","{'loss': 1.2309, 'lr': 0.00018679152127146868, 'epoch': 5.03}\n","{'loss': 1.2137, 'lr': 0.00018670445423982725, 'epoch': 5.05}\n","{'loss': 1.3594, 'grad_norm': 2.430964946746826, 'learning_rate': 0.00018661712161792976, 'epoch': 5.07}\n","{'loss': 1.1773, 'lr': 0.00018661712161792976, 'epoch': 5.07}\n","{'loss': 1.4335, 'lr': 0.0001865295236732907, 'epoch': 5.08}\n","{'loss': 1.3315, 'lr': 0.0001864416606742372, 'epoch': 5.1}\n","{'loss': 1.16, 'lr': 0.00018635353288990828, 'epoch': 5.12}\n","{'loss': 1.3728, 'lr': 0.00018626514059025422, 'epoch': 5.13}\n","{'loss': 1.4908, 'lr': 0.00018617648404603532, 'epoch': 5.15}\n","{'loss': 1.2976, 'lr': 0.00018608756352882152, 'epoch': 5.17}\n","{'loss': 1.6083, 'lr': 0.00018599837931099108, 'epoch': 5.18}\n","{'loss': 1.359, 'grad_norm': 4.729401111602783, 'learning_rate': 0.00018590893166573036, 'epoch': 5.2}\n","{'loss': 1.2385, 'lr': 0.00018590893166573036, 'epoch': 5.2}\n","{'loss': 1.3525, 'lr': 0.00018581922086703242, 'epoch': 5.22}\n","{'loss': 1.3097, 'lr': 0.00018572924718969635, 'epoch': 5.23}\n","{'loss': 1.5976, 'lr': 0.00018563901090932672, 'epoch': 5.25}\n","{'loss': 1.4828, 'lr': 0.0001855485123023323, 'epoch': 5.27}\n","{'loss': 1.2272, 'lr': 0.0001854577516459255, 'epoch': 5.28}\n","{'loss': 1.2203, 'lr': 0.00018536672921812134, 'epoch': 5.3}\n","{'loss': 1.4386, 'lr': 0.0001852754452977368, 'epoch': 5.32}\n","{'loss': 1.3584, 'grad_norm': 3.696230411529541, 'learning_rate': 0.0001851839001643898, 'epoch': 5.33}\n","{'loss': 1.3412, 'lr': 0.0001851839001643898, 'epoch': 5.33}\n","{'loss': 1.4285, 'lr': 0.00018509209409849843, 'epoch': 5.35}\n","{'loss': 1.4345, 'lr': 0.00018500002738127997, 'epoch': 5.37}\n","{'loss': 1.1951, 'lr': 0.00018490770029475022, 'epoch': 5.38}\n","{'loss': 1.259, 'lr': 0.00018481511312172253, 'epoch': 5.4}\n","{'loss': 1.3166, 'lr': 0.0001847222661458069, 'epoch': 5.42}\n","{'loss': 1.5036, 'lr': 0.00018462915965140913, 'epoch': 5.43}\n","{'loss': 1.3077, 'lr': 0.00018453579392373, 'epoch': 5.45}\n","{'loss': 1.3483, 'grad_norm': 2.3871586322784424, 'learning_rate': 0.00018444216924876438, 'epoch': 5.47}\n","{'loss': 1.1018, 'lr': 0.00018444216924876438, 'epoch': 5.47}\n","{'loss': 1.3836, 'lr': 0.00018434828591330028, 'epoch': 5.48}\n","{'loss': 1.3011, 'lr': 0.00018425414420491815, 'epoch': 5.5}\n","{'loss': 1.077, 'lr': 0.00018415974441198972, 'epoch': 5.52}\n","{'loss': 1.2667, 'lr': 0.0001840650868236774, 'epoch': 5.53}\n","{'loss': 1.2904, 'lr': 0.00018397017172993321, 'epoch': 5.55}\n","{'loss': 1.1366, 'lr': 0.00018387499942149797, 'epoch': 5.57}\n","{'loss': 1.7488, 'lr': 0.0001837795701899004, 'epoch': 5.58}\n","{'loss': 1.2882, 'grad_norm': 8.051139831542969, 'learning_rate': 0.0001836838843274562, 'epoch': 5.6}\n","{'loss': 1.1203, 'lr': 0.0001836838843274562, 'epoch': 5.6}\n","{'loss': 1.2217, 'lr': 0.0001835879421272672, 'epoch': 5.62}\n","{'loss': 1.3336, 'lr': 0.0001834917438832204, 'epoch': 5.63}\n","{'loss': 1.2664, 'lr': 0.00018339528988998717, 'epoch': 5.65}\n","{'loss': 1.1639, 'lr': 0.00018329858044302213, 'epoch': 5.67}\n","{'loss': 1.5199, 'lr': 0.0001832016158385626, 'epoch': 5.68}\n","{'loss': 1.1828, 'lr': 0.00018310439637362735, 'epoch': 5.7}\n","{'loss': 1.3123, 'lr': 0.00018300692234601588, 'epoch': 5.72}\n","{'loss': 1.2651, 'grad_norm': 2.2093923091888428, 'learning_rate': 0.00018290919405430746, 'epoch': 5.73}\n","{'loss': 1.2795, 'lr': 0.00018290919405430746, 'epoch': 5.73}\n","{'loss': 1.4034, 'lr': 0.00018281121179786024, 'epoch': 5.75}\n","{'loss': 1.0881, 'lr': 0.0001827129758768102, 'epoch': 5.77}\n","{'loss': 1.1608, 'lr': 0.00018261448659207043, 'epoch': 5.78}\n","{'loss': 1.4558, 'lr': 0.00018251574424533017, 'epoch': 5.8}\n","{'loss': 1.3488, 'lr': 0.00018241674913905366, 'epoch': 5.82}\n","{'loss': 1.3617, 'lr': 0.0001823175015764795, 'epoch': 5.83}\n","{'loss': 1.5155, 'lr': 0.00018221800186161958, 'epoch': 5.85}\n","{'loss': 1.3267, 'grad_norm': 3.6161274909973145, 'learning_rate': 0.00018211825029925823, 'epoch': 5.87}\n","{'loss': 1.1906, 'lr': 0.00018211825029925823, 'epoch': 5.87}\n","{'loss': 1.3434, 'lr': 0.00018201824719495116, 'epoch': 5.88}\n","{'loss': 1.3007, 'lr': 0.0001819179928550246, 'epoch': 5.9}\n","{'loss': 1.2786, 'lr': 0.00018181748758657438, 'epoch': 5.92}\n","{'loss': 1.394, 'lr': 0.00018171673169746497, 'epoch': 5.93}\n","{'loss': 1.5107, 'lr': 0.00018161572549632853, 'epoch': 5.95}\n","{'loss': 1.3104, 'lr': 0.00018151446929256393, 'epoch': 5.97}\n","{'loss': 1.3355, 'lr': 0.00018141296339633592, 'epoch': 5.98}\n","{'loss': 1.333, 'grad_norm': 1.7620078325271606, 'learning_rate': 0.000181311208118574, 'epoch': 6.0}\n"," 20% 360/1800 [01:39<05:44,  4.18it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0554, 'lr': 0.000181311208118574, 'epoch': 6.0}\n","{'loss': 1.1693, 'lr': 0.0001812092037709716, 'epoch': 6.02}\n","{'loss': 1.1277, 'lr': 0.00018110695066598522, 'epoch': 6.03}\n","{'loss': 1.064, 'lr': 0.00018100444911683312, 'epoch': 6.05}\n","{'loss': 1.1007, 'lr': 0.00018090169943749476, 'epoch': 6.07}\n","{'loss': 1.2126, 'lr': 0.00018079870194270958, 'epoch': 6.08}\n","{'loss': 1.0691, 'lr': 0.00018069545694797615, 'epoch': 6.1}\n","{'loss': 1.158, 'lr': 0.0001805919647695512, 'epoch': 6.12}\n","{'loss': 1.1196, 'grad_norm': 1.7131507396697998, 'learning_rate': 0.00018048822572444854, 'epoch': 6.13}\n","{'loss': 1.1064, 'lr': 0.00018048822572444854, 'epoch': 6.13}\n","{'loss': 1.0328, 'lr': 0.0001803842401304383, 'epoch': 6.15}\n","{'loss': 1.1357, 'lr': 0.0001802800083060457, 'epoch': 6.17}\n","{'loss': 1.0987, 'lr': 0.0001801755305705503, 'epoch': 6.18}\n","{'loss': 1.2604, 'lr': 0.00018007080724398488, 'epoch': 6.2}\n","{'loss': 1.2033, 'lr': 0.0001799658386471345, 'epoch': 6.22}\n","{'loss': 1.1508, 'lr': 0.00017986062510153556, 'epoch': 6.23}\n","{'loss': 1.2655, 'lr': 0.00017975516692947475, 'epoch': 6.25}\n","{'loss': 1.1567, 'grad_norm': 2.730597972869873, 'learning_rate': 0.00017964946445398811, 'epoch': 6.27}\n","{'loss': 1.1264, 'lr': 0.00017964946445398811, 'epoch': 6.27}\n","{'loss': 1.1355, 'lr': 0.00017954351799886003, 'epoch': 6.28}\n","{'loss': 1.2273, 'lr': 0.00017943732788862224, 'epoch': 6.3}\n","{'loss': 1.1508, 'lr': 0.00017933089444855277, 'epoch': 6.32}\n","{'loss': 1.5392, 'lr': 0.00017922421800467512, 'epoch': 6.33}\n","{'loss': 1.2005, 'lr': 0.00017911729888375706, 'epoch': 6.35}\n","{'loss': 1.4077, 'lr': 0.0001790101374133098, 'epoch': 6.37}\n","{'loss': 1.1104, 'lr': 0.00017890273392158674, 'epoch': 6.38}\n","{'loss': 1.2372, 'grad_norm': 2.1163554191589355, 'learning_rate': 0.00017879508873758289, 'epoch': 6.4}\n","{'loss': 1.2118, 'lr': 0.00017879508873758289, 'epoch': 6.4}\n","{'loss': 1.2658, 'lr': 0.00017868720219103344, 'epoch': 6.42}\n","{'loss': 1.0594, 'lr': 0.00017857907461241285, 'epoch': 6.43}\n","{'loss': 1.0803, 'lr': 0.00017847070633293408, 'epoch': 6.45}\n","{'loss': 1.237, 'lr': 0.0001783620976845473, 'epoch': 6.47}\n","{'loss': 1.4106, 'lr': 0.00017825324899993893, 'epoch': 6.48}\n","{'loss': 1.5836, 'lr': 0.00017814416061253077, 'epoch': 6.5}\n","{'loss': 1.5554, 'lr': 0.00017803483285647878, 'epoch': 6.52}\n","{'loss': 1.3005, 'grad_norm': 12.413942337036133, 'learning_rate': 0.00017792526606667214, 'epoch': 6.53}\n","{'loss': 1.8112, 'lr': 0.00017792526606667214, 'epoch': 6.53}\n","{'loss': 1.3658, 'lr': 0.00017781546057873238, 'epoch': 6.55}\n","{'loss': 1.2773, 'lr': 0.00017770541672901193, 'epoch': 6.57}\n","{'loss': 1.392, 'lr': 0.00017759513485459367, 'epoch': 6.58}\n","{'loss': 1.2939, 'lr': 0.00017748461529328935, 'epoch': 6.6}\n","{'loss': 1.337, 'lr': 0.00017737385838363892, 'epoch': 6.62}\n","{'loss': 1.3641, 'lr': 0.0001772628644649093, 'epoch': 6.63}\n","{'loss': 1.3366, 'lr': 0.0001771516338770935, 'epoch': 6.65}\n","{'loss': 1.3972, 'grad_norm': 2.692465305328369, 'learning_rate': 0.00017704016696090937, 'epoch': 6.67}\n","{'loss': 1.0221, 'lr': 0.00017704016696090937, 'epoch': 6.67}\n","{'loss': 1.3676, 'lr': 0.00017692846405779875, 'epoch': 6.68}\n","{'loss': 1.2277, 'lr': 0.00017681652550992637, 'epoch': 6.7}\n","{'loss': 1.4147, 'lr': 0.00017670435166017867, 'epoch': 6.72}\n","{'loss': 1.2145, 'lr': 0.00017659194285216297, 'epoch': 6.73}\n","{'loss': 1.3136, 'lr': 0.00017647929943020625, 'epoch': 6.75}\n","{'loss': 1.1744, 'lr': 0.00017636642173935417, 'epoch': 6.77}\n","{'loss': 1.2737, 'lr': 0.00017625331012537, 'epoch': 6.78}\n","{'loss': 1.251, 'grad_norm': 1.8109492063522339, 'learning_rate': 0.00017613996493473356, 'epoch': 6.8}\n","{'loss': 1.2314, 'lr': 0.00017613996493473356, 'epoch': 6.8}\n","{'loss': 1.6102, 'lr': 0.00017602638651464006, 'epoch': 6.82}\n","{'loss': 1.1666, 'lr': 0.00017591257521299932, 'epoch': 6.83}\n","{'loss': 1.1905, 'lr': 0.00017579853137843435, 'epoch': 6.85}\n","{'loss': 1.212, 'lr': 0.00017568425536028048, 'epoch': 6.87}\n","{'loss': 1.4634, 'lr': 0.00017556974750858434, 'epoch': 6.88}\n","{'loss': 1.2074, 'lr': 0.0001754550081741026, 'epoch': 6.9}\n","{'loss': 1.3465, 'lr': 0.0001753400377083011, 'epoch': 6.92}\n","{'loss': 1.3035, 'grad_norm': 1.8800352811813354, 'learning_rate': 0.00017522483646335358, 'epoch': 6.93}\n","{'loss': 1.1795, 'lr': 0.00017522483646335358, 'epoch': 6.93}\n","{'loss': 1.307, 'lr': 0.0001751094047921407, 'epoch': 6.95}\n","{'loss': 1.1838, 'lr': 0.00017499374304824907, 'epoch': 6.97}\n","{'loss': 1.3372, 'lr': 0.0001748778515859699, 'epoch': 6.98}\n"," 23% 420/1800 [01:55<05:49,  3.95it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.989, 'lr': 0.0001747617307602982, 'epoch': 7.0}\n","{'loss': 1.1848, 'lr': 0.00017464538092693146, 'epoch': 7.02}\n","{'loss': 0.9768, 'lr': 0.0001745288024422687, 'epoch': 7.03}\n","{'loss': 1.0279, 'lr': 0.00017441199566340933, 'epoch': 7.05}\n","{'loss': 1.1483, 'grad_norm': 1.5100181102752686, 'learning_rate': 0.00017429496094815205, 'epoch': 7.07}\n","{'loss': 1.3686, 'lr': 0.00017429496094815205, 'epoch': 7.07}\n","{'loss': 0.9373, 'lr': 0.0001741776986549938, 'epoch': 7.08}\n","{'loss': 1.1603, 'lr': 0.00017406020914312865, 'epoch': 7.1}\n","{'loss': 1.1291, 'lr': 0.00017394249277244655, 'epoch': 7.12}\n","{'loss': 1.0016, 'lr': 0.00017382454990353253, 'epoch': 7.13}\n","{'loss': 1.0854, 'lr': 0.00017370638089766528, 'epoch': 7.15}\n","{'loss': 1.2588, 'lr': 0.0001735879861168163, 'epoch': 7.17}\n","{'loss': 1.198, 'lr': 0.0001734693659236486, 'epoch': 7.18}\n","{'loss': 1.1424, 'grad_norm': 6.9082417488098145, 'learning_rate': 0.00017335052068151566, 'epoch': 7.2}\n","{'loss': 1.513, 'lr': 0.00017335052068151566, 'epoch': 7.2}\n","{'loss': 1.038, 'lr': 0.00017323145075446035, 'epoch': 7.22}\n","{'loss': 1.2547, 'lr': 0.00017311215650721382, 'epoch': 7.23}\n","{'loss': 1.0543, 'lr': 0.0001729926383051943, 'epoch': 7.25}\n","{'loss': 1.1377, 'lr': 0.00017287289651450605, 'epoch': 7.27}\n","{'loss': 1.0629, 'lr': 0.0001727529315019382, 'epoch': 7.28}\n","{'loss': 1.1043, 'lr': 0.0001726327436349637, 'epoch': 7.3}\n","{'loss': 1.1763, 'lr': 0.00017251233328173808, 'epoch': 7.32}\n","{'loss': 1.1676, 'grad_norm': 5.713391304016113, 'learning_rate': 0.0001723917008110984, 'epoch': 7.33}\n","{'loss': 1.2794, 'lr': 0.0001723917008110984, 'epoch': 7.33}\n","{'loss': 1.0702, 'lr': 0.00017227084659256212, 'epoch': 7.35}\n","{'loss': 1.3273, 'lr': 0.00017214977099632591, 'epoch': 7.37}\n","{'loss': 1.0604, 'lr': 0.00017202847439326465, 'epoch': 7.38}\n","{'loss': 1.0377, 'lr': 0.0001719069571549301, 'epoch': 7.4}\n","{'loss': 1.2379, 'lr': 0.00017178521965354992, 'epoch': 7.42}\n","{'loss': 1.1078, 'lr': 0.00017166326226202643, 'epoch': 7.43}\n","{'loss': 1.0202, 'lr': 0.00017154108535393552, 'epoch': 7.45}\n","{'loss': 1.1426, 'grad_norm': 1.66573965549469, 'learning_rate': 0.00017141868930352556, 'epoch': 7.47}\n","{'loss': 1.1525, 'lr': 0.00017141868930352556, 'epoch': 7.47}\n","{'loss': 1.1568, 'lr': 0.0001712960744857161, 'epoch': 7.48}\n","{'loss': 1.2239, 'lr': 0.00017117324127609686, 'epoch': 7.5}\n","{'loss': 1.2297, 'lr': 0.0001710501900509265, 'epoch': 7.52}\n","{'loss': 1.4092, 'lr': 0.00017092692118713155, 'epoch': 7.53}\n","{'loss': 1.1367, 'lr': 0.00017080343506230522, 'epoch': 7.55}\n","{'loss': 1.3451, 'lr': 0.00017067973205470605, 'epoch': 7.57}\n","{'loss': 1.3036, 'lr': 0.00017055581254325715, 'epoch': 7.58}\n","{'loss': 1.2447, 'grad_norm': 1.8474104404449463, 'learning_rate': 0.00017043167690754471, 'epoch': 7.6}\n","{'loss': 1.2445, 'lr': 0.00017043167690754471, 'epoch': 7.6}\n","{'loss': 1.058, 'lr': 0.00017030732552781696, 'epoch': 7.62}\n","{'loss': 1.0493, 'lr': 0.000170182758784983, 'epoch': 7.63}\n","{'loss': 1.5629, 'lr': 0.00017005797706061158, 'epoch': 7.65}\n","{'loss': 1.2893, 'lr': 0.00016993298073693003, 'epoch': 7.67}\n","{'loss': 0.9585, 'lr': 0.000169807770196823, 'epoch': 7.68}\n","{'loss': 1.206, 'lr': 0.00016968234582383137, 'epoch': 7.7}\n","{'loss': 1.1598, 'lr': 0.00016955670800215093, 'epoch': 7.72}\n","{'loss': 1.191, 'grad_norm': 2.0353996753692627, 'learning_rate': 0.0001694308571166314, 'epoch': 7.73}\n","{'loss': 1.0618, 'lr': 0.0001694308571166314, 'epoch': 7.73}\n","{'loss': 1.2565, 'lr': 0.0001693047935527751, 'epoch': 7.75}\n","{'loss': 1.2114, 'lr': 0.00016917851769673582, 'epoch': 7.77}\n","{'loss': 1.1521, 'lr': 0.00016905202993531765, 'epoch': 7.78}\n","{'loss': 1.2034, 'lr': 0.00016892533065597377, 'epoch': 7.8}\n","{'loss': 1.3385, 'lr': 0.00016879842024680524, 'epoch': 7.82}\n","{'loss': 1.2206, 'lr': 0.00016867129909655998, 'epoch': 7.83}\n","{'loss': 1.1127, 'lr': 0.00016854396759463127, 'epoch': 7.85}\n","{'loss': 1.1946, 'grad_norm': 1.4589824676513672, 'learning_rate': 0.00016841642613105683, 'epoch': 7.87}\n","{'loss': 1.2042, 'lr': 0.00016841642613105683, 'epoch': 7.87}\n","{'loss': 1.3277, 'lr': 0.00016828867509651753, 'epoch': 7.88}\n","{'loss': 1.1621, 'lr': 0.00016816071488233618, 'epoch': 7.9}\n","{'loss': 1.1792, 'lr': 0.0001680325458804763, 'epoch': 7.92}\n","{'loss': 1.1184, 'lr': 0.000167904168483541, 'epoch': 7.93}\n","{'loss': 1.2318, 'lr': 0.00016777558308477176, 'epoch': 7.95}\n","{'loss': 1.2621, 'lr': 0.00016764679007804718, 'epoch': 7.97}\n","{'loss': 1.2173, 'lr': 0.00016751778985788177, 'epoch': 7.98}\n","{'loss': 1.2128, 'grad_norm': 2.101712942123413, 'learning_rate': 0.00016738858281942478, 'epoch': 8.0}\n"," 27% 480/1800 [02:11<05:23,  4.08it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9873, 'lr': 0.00016738858281942478, 'epoch': 8.0}\n","{'loss': 1.1276, 'lr': 0.00016725916935845907, 'epoch': 8.02}\n","{'loss': 1.1124, 'lr': 0.00016712954987139965, 'epoch': 8.03}\n","{'loss': 0.9485, 'lr': 0.00016699972475529279, 'epoch': 8.05}\n","{'loss': 1.6222, 'lr': 0.00016686969440781447, 'epoch': 8.07}\n","{'loss': 0.9912, 'lr': 0.00016673945922726944, 'epoch': 8.08}\n","{'loss': 1.3881, 'lr': 0.0001666090196125899, 'epoch': 8.1}\n","{'loss': 1.2499, 'lr': 0.00016647837596333413, 'epoch': 8.12}\n","{'loss': 1.1784, 'grad_norm': 3.137769937515259, 'learning_rate': 0.00016634752867968557, 'epoch': 8.13}\n","{'loss': 1.0596, 'lr': 0.00016634752867968557, 'epoch': 8.13}\n","{'loss': 1.1143, 'lr': 0.00016621647816245135, 'epoch': 8.15}\n","{'loss': 0.9467, 'lr': 0.00016608522481306108, 'epoch': 8.17}\n","{'loss': 1.0187, 'lr': 0.0001659537690335658, 'epoch': 8.18}\n","{'loss': 1.1757, 'lr': 0.00016582211122663653, 'epoch': 8.2}\n","{'loss': 1.1056, 'lr': 0.00016569025179556318, 'epoch': 8.22}\n","{'loss': 1.1625, 'lr': 0.0001655581911442533, 'epoch': 8.23}\n","{'loss': 1.0654, 'lr': 0.00016542592967723065, 'epoch': 8.25}\n","{'loss': 1.0811, 'grad_norm': 2.1297919750213623, 'learning_rate': 0.00016529346779963437, 'epoch': 8.27}\n","{'loss': 1.2156, 'lr': 0.00016529346779963437, 'epoch': 8.27}\n","{'loss': 1.0566, 'lr': 0.00016516080591721732, 'epoch': 8.28}\n","{'loss': 1.0909, 'lr': 0.00016502794443634504, 'epoch': 8.3}\n","{'loss': 1.1842, 'lr': 0.00016489488376399448, 'epoch': 8.32}\n","{'loss': 1.2181, 'lr': 0.00016476162430775277, 'epoch': 8.33}\n","{'loss': 1.0807, 'lr': 0.0001646281664758159, 'epoch': 8.35}\n","{'loss': 0.9673, 'lr': 0.0001644945106769876, 'epoch': 8.37}\n","{'loss': 1.1757, 'lr': 0.0001643606573206779, 'epoch': 8.38}\n","{'loss': 1.1236, 'grad_norm': 5.9115214347839355, 'learning_rate': 0.0001642266068169021, 'epoch': 8.4}\n","{'loss': 1.0313, 'lr': 0.0001642266068169021, 'epoch': 8.4}\n","{'loss': 1.2202, 'lr': 0.00016409235957627925, 'epoch': 8.42}\n","{'loss': 1.1007, 'lr': 0.00016395791601003112, 'epoch': 8.43}\n","{'loss': 1.2199, 'lr': 0.00016382327652998094, 'epoch': 8.45}\n","{'loss': 1.1149, 'lr': 0.00016368844154855192, 'epoch': 8.47}\n","{'loss': 1.1079, 'lr': 0.0001635534114787662, 'epoch': 8.48}\n","{'loss': 1.1721, 'lr': 0.00016341818673424344, 'epoch': 8.5}\n","{'loss': 1.3096, 'lr': 0.0001632827677291997, 'epoch': 8.52}\n","{'loss': 1.1596, 'grad_norm': 3.702639579772949, 'learning_rate': 0.00016314715487844605, 'epoch': 8.53}\n","{'loss': 1.0172, 'lr': 0.00016314715487844605, 'epoch': 8.53}\n","{'loss': 1.133, 'lr': 0.00016301134859738733, 'epoch': 8.55}\n","{'loss': 1.0887, 'lr': 0.00016287534930202097, 'epoch': 8.57}\n","{'loss': 1.0722, 'lr': 0.00016273915740893554, 'epoch': 8.58}\n","{'loss': 1.3978, 'lr': 0.0001626027733353096, 'epoch': 8.6}\n","{'loss': 1.1656, 'lr': 0.0001624661974989104, 'epoch': 8.62}\n","{'loss': 1.0296, 'lr': 0.00016232943031809258, 'epoch': 8.63}\n","{'loss': 1.1207, 'lr': 0.00016219247221179692, 'epoch': 8.65}\n","{'loss': 1.1281, 'grad_norm': 1.632574439048767, 'learning_rate': 0.00016205532359954902, 'epoch': 8.67}\n","{'loss': 1.4615, 'lr': 0.00016205532359954902, 'epoch': 8.67}\n","{'loss': 1.1995, 'lr': 0.00016191798490145805, 'epoch': 8.68}\n","{'loss': 1.1884, 'lr': 0.00016178045653821537, 'epoch': 8.7}\n","{'loss': 1.211, 'lr': 0.00016164273893109346, 'epoch': 8.72}\n","{'loss': 1.1167, 'lr': 0.00016150483250194434, 'epoch': 8.73}\n","{'loss': 1.1783, 'lr': 0.0001613667376731985, 'epoch': 8.75}\n","{'loss': 1.0964, 'lr': 0.00016122845486786353, 'epoch': 8.77}\n","{'loss': 1.7456, 'lr': 0.00016108998450952284, 'epoch': 8.78}\n","{'loss': 1.2747, 'grad_norm': 13.979640007019043, 'learning_rate': 0.00016095132702233423, 'epoch': 8.8}\n","{'loss': 1.0659, 'lr': 0.00016095132702233423, 'epoch': 8.8}\n","{'loss': 1.1187, 'lr': 0.00016081248283102885, 'epoch': 8.82}\n","{'loss': 0.9712, 'lr': 0.0001606734523609097, 'epoch': 8.83}\n","{'loss': 1.0673, 'lr': 0.0001605342360378504, 'epoch': 8.85}\n","{'loss': 1.2955, 'lr': 0.00016039483428829373, 'epoch': 8.87}\n","{'loss': 1.2876, 'lr': 0.0001602552475392507, 'epoch': 8.88}\n","{'loss': 1.0542, 'lr': 0.00016011547621829888, 'epoch': 8.9}\n","{'loss': 1.0934, 'lr': 0.0001599755207535812, 'epoch': 8.92}\n","{'loss': 1.1192, 'grad_norm': 1.3624612092971802, 'learning_rate': 0.00015983538157380463, 'epoch': 8.93}\n","{'loss': 1.1549, 'lr': 0.00015983538157380463, 'epoch': 8.93}\n","{'loss': 1.0144, 'lr': 0.00015969505910823904, 'epoch': 8.95}\n","{'loss': 1.1134, 'lr': 0.0001595545537867155, 'epoch': 8.97}\n","{'loss': 1.1177, 'lr': 0.00015941386603962542, 'epoch': 8.98}\n"," 30% 540/1800 [02:28<05:00,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8921, 'lr': 0.0001592729962979189, 'epoch': 9.0}\n","{'loss': 0.998, 'lr': 0.0001591319449931035, 'epoch': 9.02}\n","{'loss': 1.0669, 'lr': 0.00015899071255724305, 'epoch': 9.03}\n","{'loss': 1.1716, 'lr': 0.00015884929942295604, 'epoch': 9.05}\n","{'loss': 1.0661, 'grad_norm': 2.185558557510376, 'learning_rate': 0.00015870770602341468, 'epoch': 9.07}\n","{'loss': 0.981, 'lr': 0.00015870770602341468, 'epoch': 9.07}\n","{'loss': 0.9675, 'lr': 0.00015856593279234317, 'epoch': 9.08}\n","{'loss': 1.0234, 'lr': 0.00015842398016401664, 'epoch': 9.1}\n","{'loss': 1.1255, 'lr': 0.00015828184857325986, 'epoch': 9.12}\n","{'loss': 1.0745, 'lr': 0.00015813953845544555, 'epoch': 9.13}\n","{'loss': 0.9168, 'lr': 0.0001579970502464935, 'epoch': 9.15}\n","{'loss': 1.0306, 'lr': 0.00015785438438286893, 'epoch': 9.17}\n","{'loss': 1.1107, 'lr': 0.00015771154130158124, 'epoch': 9.18}\n","{'loss': 1.0288, 'grad_norm': 1.6792957782745361, 'learning_rate': 0.00015756852144018272, 'epoch': 9.2}\n","{'loss': 1.3232, 'lr': 0.00015756852144018272, 'epoch': 9.2}\n","{'loss': 1.1356, 'lr': 0.00015742532523676714, 'epoch': 9.22}\n","{'loss': 1.2477, 'lr': 0.00015728195312996842, 'epoch': 9.23}\n","{'loss': 1.0883, 'lr': 0.00015713840555895935, 'epoch': 9.25}\n","{'loss': 1.2129, 'lr': 0.00015699468296345015, 'epoch': 9.27}\n","{'loss': 1.0639, 'lr': 0.00015685078578368716, 'epoch': 9.28}\n","{'loss': 1.2243, 'lr': 0.00015670671446045158, 'epoch': 9.3}\n","{'loss': 1.0592, 'lr': 0.000156562469435058, 'epoch': 9.32}\n","{'loss': 1.1694, 'grad_norm': 1.3005889654159546, 'learning_rate': 0.00015641805114935297, 'epoch': 9.33}\n","{'loss': 1.1237, 'lr': 0.00015641805114935297, 'epoch': 9.33}\n","{'loss': 0.9867, 'lr': 0.000156273460045714, 'epoch': 9.35}\n","{'loss': 1.1819, 'lr': 0.00015612869656704773, 'epoch': 9.37}\n","{'loss': 1.0475, 'lr': 0.00015598376115678902, 'epoch': 9.38}\n","{'loss': 1.0625, 'lr': 0.00015583865425889925, 'epoch': 9.4}\n","{'loss': 1.0038, 'lr': 0.00015569337631786512, 'epoch': 9.42}\n","{'loss': 0.9847, 'lr': 0.0001555479277786973, 'epoch': 9.43}\n","{'loss': 1.4942, 'lr': 0.00015540230908692903, 'epoch': 9.45}\n","{'loss': 1.1106, 'grad_norm': 2.6549935340881348, 'learning_rate': 0.0001552565206886147, 'epoch': 9.47}\n","{'loss': 0.9722, 'lr': 0.0001552565206886147, 'epoch': 9.47}\n","{'loss': 1.1615, 'lr': 0.0001551105630303286, 'epoch': 9.48}\n","{'loss': 1.2514, 'lr': 0.00015496443655916347, 'epoch': 9.5}\n","{'loss': 1.0969, 'lr': 0.00015481814172272915, 'epoch': 9.52}\n","{'loss': 1.1765, 'lr': 0.00015467167896915116, 'epoch': 9.53}\n","{'loss': 1.0038, 'lr': 0.00015452504874706948, 'epoch': 9.55}\n","{'loss': 1.0777, 'lr': 0.00015437825150563703, 'epoch': 9.57}\n","{'loss': 1.1552, 'lr': 0.0001542312876945183, 'epoch': 9.58}\n","{'loss': 1.1119, 'grad_norm': 1.8088428974151611, 'learning_rate': 0.00015408415776388808, 'epoch': 9.6}\n","{'loss': 1.1177, 'lr': 0.00015408415776388808, 'epoch': 9.6}\n","{'loss': 1.0347, 'lr': 0.00015393686216442993, 'epoch': 9.62}\n","{'loss': 0.9581, 'lr': 0.00015378940134733497, 'epoch': 9.63}\n","{'loss': 1.1364, 'lr': 0.00015364177576430036, 'epoch': 9.65}\n","{'loss': 1.1552, 'lr': 0.00015349398586752793, 'epoch': 9.67}\n","{'loss': 1.1094, 'lr': 0.00015334603210972287, 'epoch': 9.68}\n","{'loss': 1.0785, 'lr': 0.00015319791494409235, 'epoch': 9.7}\n","{'loss': 1.0526, 'lr': 0.00015304963482434402, 'epoch': 9.72}\n","{'loss': 1.0803, 'grad_norm': 1.707115650177002, 'learning_rate': 0.0001529011922046847, 'epoch': 9.73}\n","{'loss': 1.1651, 'lr': 0.0001529011922046847, 'epoch': 9.73}\n","{'loss': 1.1129, 'lr': 0.000152752587539819, 'epoch': 9.75}\n","{'loss': 0.9793, 'lr': 0.0001526038212849479, 'epoch': 9.77}\n","{'loss': 1.1191, 'lr': 0.00015245489389576725, 'epoch': 9.78}\n","{'loss': 1.0356, 'lr': 0.00015230580582846663, 'epoch': 9.8}\n","{'loss': 1.2564, 'lr': 0.00015215655753972776, 'epoch': 9.82}\n","{'loss': 1.1982, 'lr': 0.0001520071494867231, 'epoch': 9.83}\n","{'loss': 1.0807, 'lr': 0.0001518575821271146, 'epoch': 9.85}\n","{'loss': 1.1184, 'grad_norm': 1.6505491733551025, 'learning_rate': 0.000151707855919052, 'epoch': 9.87}\n","{'loss': 0.8871, 'lr': 0.000151707855919052, 'epoch': 9.87}\n","{'loss': 1.0699, 'lr': 0.0001515579713211718, 'epoch': 9.88}\n","{'loss': 1.0382, 'lr': 0.00015140792879259562, 'epoch': 9.9}\n","{'loss': 1.2492, 'lr': 0.00015125772879292878, 'epoch': 9.92}\n","{'loss': 0.952, 'lr': 0.0001511073717822591, 'epoch': 9.93}\n","{'loss': 1.099, 'lr': 0.00015095685822115522, 'epoch': 9.95}\n","{'loss': 1.0406, 'lr': 0.0001508061885706654, 'epoch': 9.97}\n","{'loss': 1.318, 'lr': 0.000150655363292316, 'epoch': 9.98}\n","{'loss': 1.0817, 'grad_norm': 3.3598713874816895, 'learning_rate': 0.00015050438284811002, 'epoch': 10.0}\n"," 33% 600/1800 [02:45<04:42,  4.24it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 1.0613, 'lr': 0.00015050438284811002, 'epoch': 10.0}\n","{'loss': 1.0226, 'lr': 0.0001503532477005259, 'epoch': 10.02}\n","{'loss': 1.237, 'lr': 0.00015020195831251588, 'epoch': 10.03}\n","{'loss': 0.9749, 'lr': 0.0001500505151475047, 'epoch': 10.05}\n","{'loss': 0.941, 'lr': 0.00014989891866938807, 'epoch': 10.07}\n","{'loss': 1.0815, 'lr': 0.00014974716934253147, 'epoch': 10.08}\n","{'loss': 0.9974, 'lr': 0.00014959526763176837, 'epoch': 10.1}\n","{'loss': 1.1309, 'lr': 0.00014944321400239921, 'epoch': 10.12}\n","{'loss': 1.0558, 'grad_norm': 12.162103652954102, 'learning_rate': 0.00014929100892018972, 'epoch': 10.13}\n","{'loss': 1.26, 'lr': 0.00014929100892018972, 'epoch': 10.13}\n","{'loss': 1.1977, 'lr': 0.00014913865285136948, 'epoch': 10.15}\n","{'loss': 1.1095, 'lr': 0.00014898614626263066, 'epoch': 10.17}\n","{'loss': 0.9694, 'lr': 0.0001488334896211265, 'epoch': 10.18}\n","{'loss': 1.003, 'lr': 0.0001486806833944698, 'epoch': 10.2}\n","{'loss': 1.0299, 'lr': 0.00014852772805073162, 'epoch': 10.22}\n","{'loss': 1.0023, 'lr': 0.0001483746240584398, 'epoch': 10.23}\n","{'loss': 0.9658, 'lr': 0.00014822137188657752, 'epoch': 10.25}\n","{'loss': 1.0672, 'grad_norm': 1.6244391202926636, 'learning_rate': 0.00014806797200458174, 'epoch': 10.27}\n","{'loss': 1.1124, 'lr': 0.00014806797200458174, 'epoch': 10.27}\n","{'loss': 1.1045, 'lr': 0.00014791442488234203, 'epoch': 10.28}\n","{'loss': 1.415, 'lr': 0.00014776073099019894, 'epoch': 10.3}\n","{'loss': 0.9841, 'lr': 0.00014760689079894258, 'epoch': 10.32}\n","{'loss': 1.0033, 'lr': 0.0001474529047798112, 'epoch': 10.33}\n","{'loss': 1.2006, 'lr': 0.00014729877340448975, 'epoch': 10.35}\n","{'loss': 1.2215, 'lr': 0.0001471444971451084, 'epoch': 10.37}\n","{'loss': 0.9718, 'lr': 0.00014699007647424125, 'epoch': 10.38}\n","{'loss': 1.1266, 'grad_norm': 1.556607723236084, 'learning_rate': 0.00014683551186490455, 'epoch': 10.4}\n","{'loss': 1.0584, 'lr': 0.00014683551186490455, 'epoch': 10.4}\n","{'loss': 1.0086, 'lr': 0.00014668080379055562, 'epoch': 10.42}\n","{'loss': 1.1201, 'lr': 0.0001465259527250912, 'epoch': 10.43}\n","{'loss': 1.0598, 'lr': 0.000146370959142846, 'epoch': 10.45}\n","{'loss': 1.2076, 'lr': 0.00014621582351859137, 'epoch': 10.47}\n","{'loss': 1.2795, 'lr': 0.00014606054632753355, 'epoch': 10.48}\n","{'loss': 1.0641, 'lr': 0.0001459051280453127, 'epoch': 10.5}\n","{'loss': 0.9808, 'lr': 0.000145749569148001, 'epoch': 10.52}\n","{'loss': 1.0974, 'grad_norm': 1.49931800365448, 'learning_rate': 0.00014559387011210136, 'epoch': 10.53}\n","{'loss': 1.08, 'lr': 0.00014559387011210136, 'epoch': 10.53}\n","{'loss': 1.0383, 'lr': 0.000145438031414546, 'epoch': 10.55}\n","{'loss': 0.961, 'lr': 0.00014528205353269493, 'epoch': 10.57}\n","{'loss': 1.2033, 'lr': 0.00014512593694433453, 'epoch': 10.58}\n","{'loss': 1.0378, 'lr': 0.00014496968212767607, 'epoch': 10.6}\n","{'loss': 1.0335, 'lr': 0.00014481328956135413, 'epoch': 10.62}\n","{'loss': 1.0671, 'lr': 0.00014465675972442533, 'epoch': 10.63}\n","{'loss': 1.1381, 'lr': 0.00014450009309636684, 'epoch': 10.65}\n","{'loss': 1.0699, 'grad_norm': 1.9935479164123535, 'learning_rate': 0.00014434329015707467, 'epoch': 10.67}\n","{'loss': 0.9536, 'lr': 0.00014434329015707467, 'epoch': 10.67}\n","{'loss': 0.9625, 'lr': 0.00014418635138686254, 'epoch': 10.68}\n","{'loss': 1.0888, 'lr': 0.00014402927726646014, 'epoch': 10.7}\n","{'loss': 1.2338, 'lr': 0.0001438720682770118, 'epoch': 10.72}\n","{'loss': 1.0901, 'lr': 0.00014371472490007497, 'epoch': 10.73}\n","{'loss': 1.1025, 'lr': 0.0001435572476176187, 'epoch': 10.75}\n","{'loss': 1.2157, 'lr': 0.00014339963691202234, 'epoch': 10.77}\n","{'loss': 0.9586, 'lr': 0.00014324189326607385, 'epoch': 10.78}\n","{'loss': 1.0757, 'grad_norm': 0.8468492031097412, 'learning_rate': 0.00014308401716296837, 'epoch': 10.8}\n","{'loss': 1.1101, 'lr': 0.00014308401716296837, 'epoch': 10.8}\n","{'loss': 1.0639, 'lr': 0.00014292600908630685, 'epoch': 10.82}\n","{'loss': 1.1065, 'lr': 0.00014276786952009451, 'epoch': 10.83}\n","{'loss': 1.0377, 'lr': 0.00014260959894873932, 'epoch': 10.85}\n","{'loss': 1.1074, 'lr': 0.00014245119785705044, 'epoch': 10.87}\n","{'loss': 1.035, 'lr': 0.00014229266673023702, 'epoch': 10.88}\n","{'loss': 1.2913, 'lr': 0.00014213400605390638, 'epoch': 10.9}\n","{'loss': 1.0466, 'lr': 0.0001419752163140628, 'epoch': 10.92}\n","{'loss': 1.0998, 'grad_norm': 1.7085325717926025, 'learning_rate': 0.00014181629799710574, 'epoch': 10.93}\n","{'loss': 1.0325, 'lr': 0.00014181629799710574, 'epoch': 10.93}\n","{'loss': 0.9875, 'lr': 0.00014165725158982867, 'epoch': 10.95}\n","{'loss': 1.207, 'lr': 0.00014149807757941733, 'epoch': 10.97}\n","{'loss': 1.2105, 'lr': 0.00014133877645344835, 'epoch': 10.98}\n"," 37% 660/1800 [03:01<04:34,  4.15it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9748, 'lr': 0.00014117934869988777, 'epoch': 11.0}\n","{'loss': 0.926, 'lr': 0.00014101979480708943, 'epoch': 11.02}\n","{'loss': 1.0678, 'lr': 0.0001408601152637937, 'epoch': 11.03}\n","{'loss': 0.9941, 'lr': 0.00014070031055912568, 'epoch': 11.05}\n","{'loss': 1.05, 'grad_norm': 1.5123634338378906, 'learning_rate': 0.00014054038118259396, 'epoch': 11.07}\n","{'loss': 1.2843, 'lr': 0.00014054038118259396, 'epoch': 11.07}\n","{'loss': 0.9145, 'lr': 0.00014038032762408897, 'epoch': 11.08}\n","{'loss': 1.0186, 'lr': 0.00014022015037388154, 'epoch': 11.1}\n","{'loss': 0.953, 'lr': 0.00014005984992262143, 'epoch': 11.12}\n","{'loss': 1.059, 'lr': 0.0001398994267613357, 'epoch': 11.13}\n","{'loss': 0.9276, 'lr': 0.00013973888138142743, 'epoch': 11.15}\n","{'loss': 1.2228, 'lr': 0.00013957821427467392, 'epoch': 11.17}\n","{'loss': 0.9943, 'lr': 0.00013941742593322545, 'epoch': 11.18}\n","{'loss': 1.0467, 'grad_norm': 6.051181793212891, 'learning_rate': 0.0001392565168496036, 'epoch': 11.2}\n","{'loss': 0.9054, 'lr': 0.0001392565168496036, 'epoch': 11.2}\n","{'loss': 1.1243, 'lr': 0.00013909548751669983, 'epoch': 11.22}\n","{'loss': 0.96, 'lr': 0.00013893433842777393, 'epoch': 11.23}\n","{'loss': 1.0204, 'lr': 0.00013877307007645256, 'epoch': 11.25}\n","{'loss': 1.071, 'lr': 0.00013861168295672768, 'epoch': 11.27}\n","{'loss': 1.056, 'lr': 0.00013845017756295498, 'epoch': 11.28}\n","{'loss': 0.9114, 'lr': 0.00013828855438985258, 'epoch': 11.3}\n","{'loss': 0.9915, 'lr': 0.00013812681393249928, 'epoch': 11.32}\n","{'loss': 1.005, 'grad_norm': 1.1519094705581665, 'learning_rate': 0.00013796495668633326, 'epoch': 11.33}\n","{'loss': 0.893, 'lr': 0.00013796495668633326, 'epoch': 11.33}\n","{'loss': 0.9546, 'lr': 0.00013780298314715022, 'epoch': 11.35}\n","{'loss': 0.9916, 'lr': 0.0001376408938111023, 'epoch': 11.37}\n","{'loss': 1.0342, 'lr': 0.00013747868917469628, 'epoch': 11.38}\n","{'loss': 1.0939, 'lr': 0.00013731636973479208, 'epoch': 11.4}\n","{'loss': 0.9329, 'lr': 0.0001371539359886013, 'epoch': 11.42}\n","{'loss': 0.9957, 'lr': 0.0001369913884336857, 'epoch': 11.43}\n","{'loss': 0.9609, 'lr': 0.0001368287275679557, 'epoch': 11.45}\n","{'loss': 0.9821, 'grad_norm': 1.2085964679718018, 'learning_rate': 0.00013666595388966868, 'epoch': 11.47}\n","{'loss': 1.0645, 'lr': 0.00013666595388966868, 'epoch': 11.47}\n","{'loss': 0.9621, 'lr': 0.00013650306789742773, 'epoch': 11.48}\n","{'loss': 1.1182, 'lr': 0.00013634007009017985, 'epoch': 11.5}\n","{'loss': 1.2417, 'lr': 0.00013617696096721466, 'epoch': 11.52}\n","{'loss': 1.3646, 'lr': 0.0001360137410281627, 'epoch': 11.53}\n","{'loss': 0.9881, 'lr': 0.00013585041077299393, 'epoch': 11.55}\n","{'loss': 1.0525, 'lr': 0.00013568697070201626, 'epoch': 11.57}\n","{'loss': 1.1319, 'lr': 0.00013552342131587398, 'epoch': 11.58}\n","{'loss': 1.1155, 'grad_norm': 2.3826708793640137, 'learning_rate': 0.00013535976311554627, 'epoch': 11.6}\n","{'loss': 1.2485, 'lr': 0.00013535976311554627, 'epoch': 11.6}\n","{'loss': 1.0896, 'lr': 0.00013519599660234558, 'epoch': 11.62}\n","{'loss': 1.1308, 'lr': 0.00013503212227791616, 'epoch': 11.63}\n","{'loss': 1.0268, 'lr': 0.0001348681406442325, 'epoch': 11.65}\n","{'loss': 1.0523, 'lr': 0.00013470405220359773, 'epoch': 11.67}\n","{'loss': 1.1082, 'lr': 0.00013453985745864224, 'epoch': 11.68}\n","{'loss': 1.258, 'lr': 0.00013437555691232206, 'epoch': 11.7}\n","{'loss': 0.979, 'lr': 0.0001342111510679172, 'epoch': 11.72}\n","{'loss': 1.1116, 'grad_norm': 1.423439621925354, 'learning_rate': 0.00013404664042903033, 'epoch': 11.73}\n","{'loss': 1.1649, 'lr': 0.00013404664042903033, 'epoch': 11.73}\n","{'loss': 1.2121, 'lr': 0.00013388202549958507, 'epoch': 11.75}\n","{'loss': 1.1057, 'lr': 0.0001337173067838245, 'epoch': 11.77}\n","{'loss': 1.0237, 'lr': 0.00013355248478630956, 'epoch': 11.78}\n","{'loss': 1.1293, 'lr': 0.00013338756001191767, 'epoch': 11.8}\n","{'loss': 1.0712, 'lr': 0.000133222532965841, 'epoch': 11.82}\n","{'loss': 1.1834, 'lr': 0.00013305740415358504, 'epoch': 11.83}\n","{'loss': 0.9995, 'lr': 0.00013289217408096697, 'epoch': 11.85}\n","{'loss': 1.1112, 'grad_norm': 1.319822907447815, 'learning_rate': 0.00013272684325411416, 'epoch': 11.87}\n","{'loss': 1.0347, 'lr': 0.00013272684325411416, 'epoch': 11.87}\n","{'loss': 1.0157, 'lr': 0.00013256141217946263, 'epoch': 11.88}\n","{'loss': 1.0342, 'lr': 0.00013239588136375538, 'epoch': 11.9}\n","{'loss': 1.1901, 'lr': 0.00013223025131404106, 'epoch': 11.92}\n","{'loss': 1.1168, 'lr': 0.00013206452253767222, 'epoch': 11.93}\n","{'loss': 1.2507, 'lr': 0.00013189869554230382, 'epoch': 11.95}\n","{'loss': 1.0029, 'lr': 0.00013173277083589177, 'epoch': 11.97}\n","{'loss': 1.0149, 'lr': 0.00013156674892669115, 'epoch': 11.98}\n","{'loss': 1.0825, 'grad_norm': 1.1580524444580078, 'learning_rate': 0.0001314006303232549, 'epoch': 12.0}\n"," 40% 720/1800 [03:18<04:24,  4.08it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9173, 'lr': 0.0001314006303232549, 'epoch': 12.0}\n","{'loss': 0.9691, 'lr': 0.0001312344155344321, 'epoch': 12.02}\n","{'loss': 0.8791, 'lr': 0.00013106810506936646, 'epoch': 12.03}\n","{'loss': 0.8953, 'lr': 0.00013090169943749476, 'epoch': 12.05}\n","{'loss': 0.9256, 'lr': 0.0001307351991485453, 'epoch': 12.07}\n","{'loss': 1.0204, 'lr': 0.00013056860471253638, 'epoch': 12.08}\n","{'loss': 1.256, 'lr': 0.0001304019166397746, 'epoch': 12.1}\n","{'loss': 0.9165, 'lr': 0.00013023513544085344, 'epoch': 12.12}\n","{'loss': 0.9724, 'grad_norm': 0.7560802102088928, 'learning_rate': 0.00013006826162665163, 'epoch': 12.13}\n","{'loss': 0.9546, 'lr': 0.00013006826162665163, 'epoch': 12.13}\n","{'loss': 0.9572, 'lr': 0.0001299012957083316, 'epoch': 12.15}\n","{'loss': 0.8757, 'lr': 0.0001297342381973379, 'epoch': 12.17}\n","{'loss': 1.1756, 'lr': 0.00012956708960539564, 'epoch': 12.18}\n","{'loss': 0.9619, 'lr': 0.0001293998504445089, 'epoch': 12.2}\n","{'loss': 1.0528, 'lr': 0.0001292325212269593, 'epoch': 12.22}\n","{'loss': 1.1152, 'lr': 0.00012906510246530424, 'epoch': 12.23}\n","{'loss': 0.997, 'lr': 0.00012889759467237533, 'epoch': 12.25}\n","{'loss': 1.0113, 'grad_norm': 1.3632241487503052, 'learning_rate': 0.00012872999836127704, 'epoch': 12.27}\n","{'loss': 0.9961, 'lr': 0.00012872999836127704, 'epoch': 12.27}\n","{'loss': 1.0595, 'lr': 0.0001285623140453849, 'epoch': 12.28}\n","{'loss': 0.9589, 'lr': 0.000128394542238344, 'epoch': 12.3}\n","{'loss': 1.1339, 'lr': 0.00012822668345406754, 'epoch': 12.32}\n","{'loss': 0.9845, 'lr': 0.0001280587382067351, 'epoch': 12.33}\n","{'loss': 0.9819, 'lr': 0.00012789070701079095, 'epoch': 12.35}\n","{'loss': 0.9797, 'lr': 0.00012772259038094284, 'epoch': 12.37}\n","{'loss': 0.9996, 'lr': 0.0001275543888321602, 'epoch': 12.38}\n","{'loss': 1.0118, 'grad_norm': 1.5462462902069092, 'learning_rate': 0.00012738610287967245, 'epoch': 12.4}\n","{'loss': 1.0292, 'lr': 0.00012738610287967245, 'epoch': 12.4}\n","{'loss': 0.9202, 'lr': 0.00012721773303896763, 'epoch': 12.42}\n","{'loss': 0.9941, 'lr': 0.00012704927982579083, 'epoch': 12.43}\n","{'loss': 1.0112, 'lr': 0.00012688074375614235, 'epoch': 12.45}\n","{'loss': 1.0233, 'lr': 0.00012671212534627642, 'epoch': 12.47}\n","{'loss': 1.0224, 'lr': 0.00012654342511269942, 'epoch': 12.48}\n","{'loss': 1.0147, 'lr': 0.00012637464357216846, 'epoch': 12.5}\n","{'loss': 1.0151, 'lr': 0.00012620578124168961, 'epoch': 12.52}\n","{'loss': 1.0038, 'grad_norm': 1.5679069757461548, 'learning_rate': 0.00012603683863851643, 'epoch': 12.53}\n","{'loss': 0.8984, 'lr': 0.00012603683863851643, 'epoch': 12.53}\n","{'loss': 1.0827, 'lr': 0.00012586781628014842, 'epoch': 12.55}\n","{'loss': 0.9918, 'lr': 0.0001256987146843294, 'epoch': 12.57}\n","{'loss': 1.1182, 'lr': 0.00012552953436904577, 'epoch': 12.58}\n","{'loss': 0.9508, 'lr': 0.00012536027585252526, 'epoch': 12.6}\n","{'loss': 1.2865, 'lr': 0.00012519093965323493, 'epoch': 12.62}\n","{'loss': 1.1437, 'lr': 0.00012502152628987996, 'epoch': 12.63}\n","{'loss': 1.1541, 'lr': 0.00012485203628140182, 'epoch': 12.65}\n","{'loss': 1.0783, 'grad_norm': 3.14552640914917, 'learning_rate': 0.0001246824701469768, 'epoch': 12.67}\n","{'loss': 0.9648, 'lr': 0.0001246824701469768, 'epoch': 12.67}\n","{'loss': 1.1538, 'lr': 0.00012451282840601432, 'epoch': 12.68}\n","{'loss': 1.2133, 'lr': 0.00012434311157815545, 'epoch': 12.7}\n","{'loss': 1.0581, 'lr': 0.00012417332018327125, 'epoch': 12.72}\n","{'loss': 0.998, 'lr': 0.00012400345474146115, 'epoch': 12.73}\n","{'loss': 1.0219, 'lr': 0.00012383351577305147, 'epoch': 12.75}\n","{'loss': 1.038, 'lr': 0.00012366350379859365, 'epoch': 12.77}\n","{'loss': 1.0072, 'lr': 0.00012349341933886291, 'epoch': 12.78}\n","{'loss': 1.0569, 'grad_norm': 1.0036433935165405, 'learning_rate': 0.00012332326291485633, 'epoch': 12.8}\n","{'loss': 1.0411, 'lr': 0.00012332326291485633, 'epoch': 12.8}\n","{'loss': 1.0422, 'lr': 0.0001231530350477916, 'epoch': 12.82}\n","{'loss': 0.9129, 'lr': 0.0001229827362591051, 'epoch': 12.83}\n","{'loss': 0.9624, 'lr': 0.00012281236707045062, 'epoch': 12.85}\n","{'loss': 1.0282, 'lr': 0.0001226419280036974, 'epoch': 12.87}\n","{'loss': 1.1267, 'lr': 0.00012247141958092887, 'epoch': 12.88}\n","{'loss': 0.9558, 'lr': 0.00012230084232444085, 'epoch': 12.9}\n","{'loss': 1.1016, 'lr': 0.00012213019675674008, 'epoch': 12.92}\n","{'loss': 1.0214, 'grad_norm': 1.388375163078308, 'learning_rate': 0.00012195948340054246, 'epoch': 12.93}\n","{'loss': 1.0138, 'lr': 0.00012195948340054246, 'epoch': 12.93}\n","{'loss': 0.9842, 'lr': 0.00012178870277877155, 'epoch': 12.95}\n","{'loss': 0.937, 'lr': 0.00012161785541455702, 'epoch': 12.97}\n","{'loss': 1.0179, 'lr': 0.00012144694183123293, 'epoch': 12.98}\n"," 43% 780/1800 [03:35<04:13,  4.02it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.9391, 'lr': 0.00012127596255233622, 'epoch': 13.0}\n","{'loss': 0.8368, 'lr': 0.00012110491810160498, 'epoch': 13.02}\n","{'loss': 1.0591, 'lr': 0.00012093380900297701, 'epoch': 13.03}\n","{'loss': 0.8674, 'lr': 0.00012076263578058818, 'epoch': 13.05}\n","{'loss': 0.9569, 'grad_norm': 0.697371780872345, 'learning_rate': 0.00012059139895877067, 'epoch': 13.07}\n","{'loss': 0.8933, 'lr': 0.00012059139895877067, 'epoch': 13.07}\n","{'loss': 0.95, 'lr': 0.00012042009906205152, 'epoch': 13.08}\n","{'loss': 0.9052, 'lr': 0.00012024873661515101, 'epoch': 13.1}\n","{'loss': 0.9892, 'lr': 0.00012007731214298097, 'epoch': 13.12}\n","{'loss': 1.0013, 'lr': 0.0001199058261706433, 'epoch': 13.13}\n","{'loss': 0.885, 'lr': 0.00011973427922342817, 'epoch': 13.15}\n","{'loss': 1.0451, 'lr': 0.00011956267182681264, 'epoch': 13.17}\n","{'loss': 1.0526, 'lr': 0.00011939100450645888, 'epoch': 13.18}\n","{'loss': 0.9652, 'grad_norm': 1.6660305261611938, 'learning_rate': 0.00011921927778821266, 'epoch': 13.2}\n","{'loss': 0.9324, 'lr': 0.00011921927778821266, 'epoch': 13.2}\n","{'loss': 0.8782, 'lr': 0.00011904749219810161, 'epoch': 13.22}\n","{'loss': 0.9619, 'lr': 0.00011887564826233372, 'epoch': 13.23}\n","{'loss': 1.1318, 'lr': 0.0001187037465072958, 'epoch': 13.25}\n","{'loss': 0.9776, 'lr': 0.00011853178745955169, 'epoch': 13.27}\n","{'loss': 0.898, 'lr': 0.00011835977164584069, 'epoch': 13.28}\n","{'loss': 0.9366, 'lr': 0.00011818769959307607, 'epoch': 13.3}\n","{'loss': 0.9004, 'lr': 0.0001180155718283433, 'epoch': 13.32}\n","{'loss': 0.9521, 'grad_norm': 2.3354101181030273, 'learning_rate': 0.00011784338887889858, 'epoch': 13.33}\n","{'loss': 1.0141, 'lr': 0.00011784338887889858, 'epoch': 13.33}\n","{'loss': 1.025, 'lr': 0.00011767115127216708, 'epoch': 13.35}\n","{'loss': 0.9806, 'lr': 0.00011749885953574138, 'epoch': 13.37}\n","{'loss': 0.9602, 'lr': 0.00011732651419737993, 'epoch': 13.38}\n","{'loss': 1.1038, 'lr': 0.00011715411578500539, 'epoch': 13.4}\n","{'loss': 1.0333, 'lr': 0.00011698166482670292, 'epoch': 13.42}\n","{'loss': 1.2047, 'lr': 0.00011680916185071866, 'epoch': 13.43}\n","{'loss': 0.9484, 'lr': 0.00011663660738545812, 'epoch': 13.45}\n","{'loss': 1.0338, 'grad_norm': 1.3699685335159302, 'learning_rate': 0.0001164640019594845, 'epoch': 13.47}\n","{'loss': 0.885, 'lr': 0.0001164640019594845, 'epoch': 13.47}\n","{'loss': 0.8629, 'lr': 0.0001162913461015171, 'epoch': 13.48}\n","{'loss': 0.9957, 'lr': 0.00011611864034042972, 'epoch': 13.5}\n","{'loss': 0.893, 'lr': 0.00011594588520524903, 'epoch': 13.52}\n","{'loss': 1.0152, 'lr': 0.00011577308122515294, 'epoch': 13.53}\n","{'loss': 1.0494, 'lr': 0.0001156002289294689, 'epoch': 13.55}\n","{'loss': 0.9602, 'lr': 0.00011542732884767249, 'epoch': 13.57}\n","{'loss': 1.0421, 'lr': 0.00011525438150938554, 'epoch': 13.58}\n","{'loss': 0.9629, 'grad_norm': 1.7646279335021973, 'learning_rate': 0.00011508138744437475, 'epoch': 13.6}\n","{'loss': 0.9955, 'lr': 0.00011508138744437475, 'epoch': 13.6}\n","{'loss': 0.984, 'lr': 0.00011490834718254989, 'epoch': 13.62}\n","{'loss': 1.0898, 'lr': 0.0001147352612539622, 'epoch': 13.63}\n","{'loss': 1.0287, 'lr': 0.00011456213018880289, 'epoch': 13.65}\n","{'loss': 0.9308, 'lr': 0.00011438895451740142, 'epoch': 13.67}\n","{'loss': 0.92, 'lr': 0.00011421573477022378, 'epoch': 13.68}\n","{'loss': 1.0245, 'lr': 0.0001140424714778711, 'epoch': 13.7}\n","{'loss': 1.0651, 'lr': 0.00011386916517107784, 'epoch': 13.72}\n","{'loss': 1.0048, 'grad_norm': 11.19847583770752, 'learning_rate': 0.00011369581638071021, 'epoch': 13.73}\n","{'loss': 1.0951, 'lr': 0.00011369581638071021, 'epoch': 13.73}\n","{'loss': 0.9681, 'lr': 0.0001135224256377646, 'epoch': 13.75}\n","{'loss': 1.0274, 'lr': 0.00011334899347336586, 'epoch': 13.77}\n","{'loss': 0.9068, 'lr': 0.00011317552041876571, 'epoch': 13.78}\n","{'loss': 0.8989, 'lr': 0.0001130020070053412, 'epoch': 13.8}\n","{'loss': 1.0351, 'lr': 0.00011282845376459297, 'epoch': 13.82}\n","{'loss': 1.1535, 'lr': 0.00011265486122814359, 'epoch': 13.83}\n","{'loss': 1.0655, 'lr': 0.00011248122992773608, 'epoch': 13.85}\n","{'loss': 1.0188, 'grad_norm': 6.249799728393555, 'learning_rate': 0.00011230756039523215, 'epoch': 13.87}\n","{'loss': 1.0124, 'lr': 0.00011230756039523215, 'epoch': 13.87}\n","{'loss': 0.9898, 'lr': 0.00011213385316261071, 'epoch': 13.88}\n","{'loss': 1.1035, 'lr': 0.00011196010876196606, 'epoch': 13.9}\n","{'loss': 0.8999, 'lr': 0.00011178632772550635, 'epoch': 13.92}\n","{'loss': 1.0333, 'lr': 0.00011161251058555202, 'epoch': 13.93}\n","{'loss': 0.97, 'lr': 0.00011143865787453405, 'epoch': 13.95}\n","{'loss': 1.1696, 'lr': 0.0001112647701249924, 'epoch': 13.97}\n","{'loss': 1.0109, 'lr': 0.00011109084786957428, 'epoch': 13.98}\n","{'loss': 1.0237, 'grad_norm': 1.4429227113723755, 'learning_rate': 0.00011091689164103281, 'epoch': 14.0}\n"," 47% 840/1800 [03:52<03:56,  4.06it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8613, 'lr': 0.00011091689164103281, 'epoch': 14.0}\n","{'loss': 0.8922, 'lr': 0.00011074290197222494, 'epoch': 14.02}\n","{'loss': 0.9013, 'lr': 0.00011056887939611016, 'epoch': 14.03}\n","{'loss': 0.8978, 'lr': 0.00011039482444574876, 'epoch': 14.05}\n","{'loss': 0.9033, 'lr': 0.00011022073765430019, 'epoch': 14.07}\n","{'loss': 0.8557, 'lr': 0.00011004661955502142, 'epoch': 14.08}\n","{'loss': 0.9146, 'lr': 0.00010987247068126532, 'epoch': 14.1}\n","{'loss': 0.8216, 'lr': 0.00010969829156647908, 'epoch': 14.12}\n","{'loss': 0.881, 'grad_norm': 1.3732514381408691, 'learning_rate': 0.00010952408274420244, 'epoch': 14.13}\n","{'loss': 0.9227, 'lr': 0.00010952408274420244, 'epoch': 14.13}\n","{'loss': 0.8259, 'lr': 0.00010934984474806624, 'epoch': 14.15}\n","{'loss': 0.9033, 'lr': 0.00010917557811179056, 'epoch': 14.17}\n","{'loss': 0.8809, 'lr': 0.0001090012833691833, 'epoch': 14.18}\n","{'loss': 1.0138, 'lr': 0.00010882696105413843, 'epoch': 14.2}\n","{'loss': 0.9487, 'lr': 0.00010865261170063439, 'epoch': 14.22}\n","{'loss': 1.0058, 'lr': 0.00010847823584273243, 'epoch': 14.23}\n","{'loss': 0.9097, 'lr': 0.00010830383401457498, 'epoch': 14.25}\n","{'loss': 0.9264, 'grad_norm': 1.2299445867538452, 'learning_rate': 0.00010812940675038403, 'epoch': 14.27}\n","{'loss': 0.9158, 'lr': 0.00010812940675038403, 'epoch': 14.27}\n","{'loss': 0.856, 'lr': 0.00010795495458445952, 'epoch': 14.28}\n","{'loss': 0.9829, 'lr': 0.0001077804780511776, 'epoch': 14.3}\n","{'loss': 0.8963, 'lr': 0.00010760597768498908, 'epoch': 14.32}\n","{'loss': 0.9212, 'lr': 0.00010743145402041781, 'epoch': 14.33}\n","{'loss': 1.085, 'lr': 0.000107256907592059, 'epoch': 14.35}\n","{'loss': 1.0023, 'lr': 0.00010708233893457758, 'epoch': 14.37}\n","{'loss': 0.9844, 'lr': 0.00010690774858270654, 'epoch': 14.38}\n","{'loss': 0.9555, 'grad_norm': 1.7074368000030518, 'learning_rate': 0.00010673313707124535, 'epoch': 14.4}\n","{'loss': 0.896, 'lr': 0.00010673313707124535, 'epoch': 14.4}\n","{'loss': 0.962, 'lr': 0.00010655850493505834, 'epoch': 14.42}\n","{'loss': 1.0441, 'lr': 0.00010638385270907292, 'epoch': 14.43}\n","{'loss': 0.9692, 'lr': 0.00010620918092827812, 'epoch': 14.45}\n","{'loss': 0.8767, 'lr': 0.00010603449012772281, 'epoch': 14.47}\n","{'loss': 0.8873, 'lr': 0.00010585978084251423, 'epoch': 14.48}\n","{'loss': 0.809, 'lr': 0.00010568505360781606, 'epoch': 14.5}\n","{'loss': 1.0951, 'lr': 0.00010551030895884715, 'epoch': 14.52}\n","{'loss': 0.9424, 'grad_norm': 1.8320363759994507, 'learning_rate': 0.00010533554743087956, 'epoch': 14.53}\n","{'loss': 1.0881, 'lr': 0.00010533554743087956, 'epoch': 14.53}\n","{'loss': 0.9313, 'lr': 0.00010516076955923715, 'epoch': 14.55}\n","{'loss': 0.9648, 'lr': 0.00010498597587929373, 'epoch': 14.57}\n","{'loss': 1.0722, 'lr': 0.00010481116692647164, 'epoch': 14.58}\n","{'loss': 0.9168, 'lr': 0.00010463634323623995, 'epoch': 14.6}\n","{'loss': 1.1563, 'lr': 0.00010446150534411292, 'epoch': 14.62}\n","{'loss': 0.9587, 'lr': 0.00010428665378564824, 'epoch': 14.63}\n","{'loss': 1.0162, 'lr': 0.00010411178909644547, 'epoch': 14.65}\n","{'loss': 1.0131, 'grad_norm': 1.1344878673553467, 'learning_rate': 0.0001039369118121445, 'epoch': 14.67}\n","{'loss': 0.9979, 'lr': 0.0001039369118121445, 'epoch': 14.67}\n","{'loss': 0.9455, 'lr': 0.00010376202246842366, 'epoch': 14.68}\n","{'loss': 1.0013, 'lr': 0.00010358712160099832, 'epoch': 14.7}\n","{'loss': 0.9022, 'lr': 0.0001034122097456191, 'epoch': 14.72}\n","{'loss': 0.997, 'lr': 0.00010323728743807028, 'epoch': 14.73}\n","{'loss': 1.0248, 'lr': 0.0001030623552141682, 'epoch': 14.75}\n","{'loss': 1.1198, 'lr': 0.00010288741360975949, 'epoch': 14.77}\n","{'loss': 1.0775, 'lr': 0.0001027124631607196, 'epoch': 14.78}\n","{'loss': 1.0082, 'grad_norm': 1.6324176788330078, 'learning_rate': 0.00010253750440295105, 'epoch': 14.8}\n","{'loss': 0.9333, 'lr': 0.00010253750440295105, 'epoch': 14.8}\n","{'loss': 1.0553, 'lr': 0.00010236253787238177, 'epoch': 14.82}\n","{'loss': 0.8925, 'lr': 0.00010218756410496354, 'epoch': 14.83}\n","{'loss': 1.0439, 'lr': 0.0001020125836366703, 'epoch': 14.85}\n","{'loss': 1.0401, 'lr': 0.0001018375970034965, 'epoch': 14.87}\n","{'loss': 0.9191, 'lr': 0.00010166260474145552, 'epoch': 14.88}\n","{'loss': 1.0709, 'lr': 0.00010148760738657795, 'epoch': 14.9}\n","{'loss': 1.0221, 'lr': 0.00010131260547490991, 'epoch': 14.92}\n","{'loss': 0.9972, 'grad_norm': 3.309616804122925, 'learning_rate': 0.00010113759954251159, 'epoch': 14.93}\n","{'loss': 1.1195, 'lr': 0.00010113759954251159, 'epoch': 14.93}\n","{'loss': 0.9853, 'lr': 0.0001009625901254555, 'epoch': 14.95}\n","{'loss': 0.9361, 'lr': 0.0001007875777598247, 'epoch': 14.97}\n","{'loss': 0.9892, 'lr': 0.00010061256298171143, 'epoch': 14.98}\n"," 50% 900/1800 [04:08<03:44,  4.01it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.917, 'lr': 0.00010043754632721518, 'epoch': 15.0}\n","{'loss': 0.8298, 'lr': 0.00010026252833244134, 'epoch': 15.02}\n","{'loss': 0.932, 'lr': 0.00010008750953349923, 'epoch': 15.03}\n","{'loss': 1.0581, 'lr': 9.99124904665008e-05, 'epoch': 15.05}\n","{'loss': 0.9709, 'grad_norm': 1.3297207355499268, 'learning_rate': 9.97374716675587e-05, 'epoch': 15.07}\n","{'loss': 1.0504, 'lr': 9.97374716675587e-05, 'epoch': 15.07}\n","{'loss': 0.8557, 'lr': 9.956245367278482e-05, 'epoch': 15.08}\n","{'loss': 0.957, 'lr': 9.93874370182886e-05, 'epoch': 15.1}\n","{'loss': 0.7822, 'lr': 9.92124222401753e-05, 'epoch': 15.12}\n","{'loss': 0.8867, 'lr': 9.903740987454452e-05, 'epoch': 15.13}\n","{'loss': 0.9211, 'lr': 9.886240045748841e-05, 'epoch': 15.15}\n","{'loss': 1.034, 'lr': 9.868739452509011e-05, 'epoch': 15.17}\n","{'loss': 1.0284, 'lr': 9.851239261342211e-05, 'epoch': 15.18}\n","{'loss': 0.9394, 'grad_norm': 1.3882735967636108, 'learning_rate': 9.833739525854451e-05, 'epoch': 15.2}\n","{'loss': 0.8702, 'lr': 9.833739525854451e-05, 'epoch': 15.2}\n","{'loss': 0.8296, 'lr': 9.816240299650352e-05, 'epoch': 15.22}\n","{'loss': 0.9299, 'lr': 9.798741636332973e-05, 'epoch': 15.23}\n","{'loss': 0.9933, 'lr': 9.781243589503649e-05, 'epoch': 15.25}\n","{'loss': 0.8115, 'lr': 9.763746212761826e-05, 'epoch': 15.27}\n","{'loss': 1.0562, 'lr': 9.746249559704898e-05, 'epoch': 15.28}\n","{'loss': 0.8897, 'lr': 9.728753683928042e-05, 'epoch': 15.3}\n","{'loss': 0.8281, 'lr': 9.711258639024055e-05, 'epoch': 15.32}\n","{'loss': 0.9011, 'grad_norm': 4.6393609046936035, 'learning_rate': 9.693764478583185e-05, 'epoch': 15.33}\n","{'loss': 0.9895, 'lr': 9.693764478583185e-05, 'epoch': 15.33}\n","{'loss': 0.8283, 'lr': 9.676271256192974e-05, 'epoch': 15.35}\n","{'loss': 1.1338, 'lr': 9.658779025438092e-05, 'epoch': 15.37}\n","{'loss': 0.9782, 'lr': 9.64128783990017e-05, 'epoch': 15.38}\n","{'loss': 0.8833, 'lr': 9.623797753157636e-05, 'epoch': 15.4}\n","{'loss': 0.8186, 'lr': 9.606308818785551e-05, 'epoch': 15.42}\n","{'loss': 0.9181, 'lr': 9.588821090355452e-05, 'epoch': 15.43}\n","{'loss': 1.1066, 'lr': 9.571334621435182e-05, 'epoch': 15.45}\n","{'loss': 0.9571, 'grad_norm': 8.488218307495117, 'learning_rate': 9.553849465588713e-05, 'epoch': 15.47}\n","{'loss': 0.9566, 'lr': 9.553849465588713e-05, 'epoch': 15.47}\n","{'loss': 0.9002, 'lr': 9.536365676376006e-05, 'epoch': 15.48}\n","{'loss': 0.9874, 'lr': 9.518883307352839e-05, 'epoch': 15.5}\n","{'loss': 0.9842, 'lr': 9.50140241207063e-05, 'epoch': 15.52}\n","{'loss': 1.0845, 'lr': 9.483923044076286e-05, 'epoch': 15.53}\n","{'loss': 0.9226, 'lr': 9.466445256912043e-05, 'epoch': 15.55}\n","{'loss': 0.9195, 'lr': 9.448969104115284e-05, 'epoch': 15.57}\n","{'loss': 0.786, 'lr': 9.431494639218397e-05, 'epoch': 15.58}\n","{'loss': 0.9426, 'grad_norm': 0.9471202492713928, 'learning_rate': 9.414021915748582e-05, 'epoch': 15.6}\n","{'loss': 0.9329, 'lr': 9.414021915748582e-05, 'epoch': 15.6}\n","{'loss': 0.8516, 'lr': 9.396550987227721e-05, 'epoch': 15.62}\n","{'loss': 0.9877, 'lr': 9.37908190717219e-05, 'epoch': 15.63}\n","{'loss': 0.9504, 'lr': 9.36161472909271e-05, 'epoch': 15.65}\n","{'loss': 0.9123, 'lr': 9.344149506494168e-05, 'epoch': 15.67}\n","{'loss': 0.948, 'lr': 9.326686292875465e-05, 'epoch': 15.68}\n","{'loss': 0.9596, 'lr': 9.309225141729347e-05, 'epoch': 15.7}\n","{'loss': 1.0333, 'lr': 9.291766106542247e-05, 'epoch': 15.72}\n","{'loss': 0.947, 'grad_norm': 1.7077583074569702, 'learning_rate': 9.274309240794103e-05, 'epoch': 15.73}\n","{'loss': 0.8441, 'lr': 9.274309240794103e-05, 'epoch': 15.73}\n","{'loss': 0.9088, 'lr': 9.256854597958221e-05, 'epoch': 15.75}\n","{'loss': 1.0234, 'lr': 9.239402231501095e-05, 'epoch': 15.77}\n","{'loss': 0.8937, 'lr': 9.221952194882244e-05, 'epoch': 15.78}\n","{'loss': 1.0688, 'lr': 9.204504541554051e-05, 'epoch': 15.8}\n","{'loss': 1.0067, 'lr': 9.187059324961598e-05, 'epoch': 15.82}\n","{'loss': 1.0771, 'lr': 9.169616598542503e-05, 'epoch': 15.83}\n","{'loss': 0.9328, 'lr': 9.152176415726762e-05, 'epoch': 15.85}\n","{'loss': 0.9694, 'grad_norm': 1.077552318572998, 'learning_rate': 9.134738829936563e-05, 'epoch': 15.87}\n","{'loss': 0.8569, 'lr': 9.134738829936563e-05, 'epoch': 15.87}\n","{'loss': 0.8643, 'lr': 9.11730389458616e-05, 'epoch': 15.88}\n","{'loss': 1.0888, 'lr': 9.099871663081673e-05, 'epoch': 15.9}\n","{'loss': 0.8791, 'lr': 9.082442188820946e-05, 'epoch': 15.92}\n","{'loss': 1.02, 'lr': 9.06501552519338e-05, 'epoch': 15.93}\n","{'loss': 1.0533, 'lr': 9.047591725579755e-05, 'epoch': 15.95}\n","{'loss': 0.9285, 'lr': 9.030170843352093e-05, 'epoch': 15.97}\n","{'loss': 0.9683, 'lr': 9.012752931873469e-05, 'epoch': 15.98}\n","{'loss': 0.9574, 'grad_norm': 1.1278777122497559, 'learning_rate': 8.99533804449786e-05, 'epoch': 16.0}\n"," 53% 960/1800 [04:24<03:26,  4.07it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8919, 'lr': 8.99533804449786e-05, 'epoch': 16.0}\n","{'loss': 0.8795, 'lr': 8.977926234569983e-05, 'epoch': 16.02}\n","{'loss': 0.8603, 'lr': 8.960517555425125e-05, 'epoch': 16.03}\n","{'loss': 1.2318, 'lr': 8.943112060388985e-05, 'epoch': 16.05}\n","{'loss': 0.8941, 'lr': 8.925709802777508e-05, 'epoch': 16.07}\n","{'loss': 1.0353, 'lr': 8.90831083589672e-05, 'epoch': 16.08}\n","{'loss': 0.8855, 'lr': 8.89091521304257e-05, 'epoch': 16.1}\n","{'loss': 0.8967, 'lr': 8.873522987500766e-05, 'epoch': 16.12}\n","{'loss': 0.9469, 'grad_norm': 1.3413161039352417, 'learning_rate': 8.856134212546599e-05, 'epoch': 16.13}\n","{'loss': 0.8285, 'lr': 8.856134212546599e-05, 'epoch': 16.13}\n","{'loss': 0.8802, 'lr': 8.838748941444801e-05, 'epoch': 16.15}\n","{'loss': 0.9405, 'lr': 8.821367227449367e-05, 'epoch': 16.17}\n","{'loss': 0.9193, 'lr': 8.803989123803396e-05, 'epoch': 16.18}\n","{'loss': 0.9586, 'lr': 8.78661468373893e-05, 'epoch': 16.2}\n","{'loss': 0.8086, 'lr': 8.769243960476786e-05, 'epoch': 16.22}\n","{'loss': 0.8615, 'lr': 8.751877007226394e-05, 'epoch': 16.23}\n","{'loss': 0.9356, 'lr': 8.734513877185644e-05, 'epoch': 16.25}\n","{'loss': 0.8916, 'grad_norm': 1.4053858518600464, 'learning_rate': 8.717154623540707e-05, 'epoch': 16.27}\n","{'loss': 0.8375, 'lr': 8.717154623540707e-05, 'epoch': 16.27}\n","{'loss': 0.8557, 'lr': 8.699799299465881e-05, 'epoch': 16.28}\n","{'loss': 1.0072, 'lr': 8.68244795812343e-05, 'epoch': 16.3}\n","{'loss': 0.8943, 'lr': 8.665100652663417e-05, 'epoch': 16.32}\n","{'loss': 0.869, 'lr': 8.647757436223543e-05, 'epoch': 16.33}\n","{'loss': 0.967, 'lr': 8.63041836192898e-05, 'epoch': 16.35}\n","{'loss': 0.891, 'lr': 8.613083482892218e-05, 'epoch': 16.37}\n","{'loss': 0.9101, 'lr': 8.595752852212894e-05, 'epoch': 16.38}\n","{'loss': 0.904, 'grad_norm': 1.5166492462158203, 'learning_rate': 8.578426522977626e-05, 'epoch': 16.4}\n","{'loss': 0.9635, 'lr': 8.578426522977626e-05, 'epoch': 16.4}\n","{'loss': 0.93, 'lr': 8.561104548259863e-05, 'epoch': 16.42}\n","{'loss': 0.9225, 'lr': 8.543786981119712e-05, 'epoch': 16.43}\n","{'loss': 0.9713, 'lr': 8.526473874603782e-05, 'epoch': 16.45}\n","{'loss': 0.8363, 'lr': 8.509165281745015e-05, 'epoch': 16.47}\n","{'loss': 0.856, 'lr': 8.491861255562527e-05, 'epoch': 16.48}\n","{'loss': 0.8617, 'lr': 8.474561849061445e-05, 'epoch': 16.5}\n","{'loss': 0.9058, 'lr': 8.457267115232752e-05, 'epoch': 16.52}\n","{'loss': 0.9059, 'grad_norm': 2.0058608055114746, 'learning_rate': 8.439977107053114e-05, 'epoch': 16.53}\n","{'loss': 0.8604, 'lr': 8.439977107053114e-05, 'epoch': 16.53}\n","{'loss': 0.8664, 'lr': 8.422691877484711e-05, 'epoch': 16.55}\n","{'loss': 0.8574, 'lr': 8.4054114794751e-05, 'epoch': 16.57}\n","{'loss': 0.8826, 'lr': 8.38813596595703e-05, 'epoch': 16.58}\n","{'loss': 0.937, 'lr': 8.370865389848292e-05, 'epoch': 16.6}\n","{'loss': 0.9281, 'lr': 8.353599804051551e-05, 'epoch': 16.62}\n","{'loss': 0.8981, 'lr': 8.336339261454189e-05, 'epoch': 16.63}\n","{'loss': 0.9257, 'lr': 8.319083814928135e-05, 'epoch': 16.65}\n","{'loss': 0.8945, 'grad_norm': 1.406947135925293, 'learning_rate': 8.301833517329714e-05, 'epoch': 16.67}\n","{'loss': 0.8225, 'lr': 8.301833517329714e-05, 'epoch': 16.67}\n","{'loss': 0.9482, 'lr': 8.284588421499465e-05, 'epoch': 16.68}\n","{'loss': 0.9938, 'lr': 8.26734858026201e-05, 'epoch': 16.7}\n","{'loss': 0.9114, 'lr': 8.250114046425865e-05, 'epoch': 16.72}\n","{'loss': 0.9307, 'lr': 8.232884872783296e-05, 'epoch': 16.73}\n","{'loss': 0.9024, 'lr': 8.215661112110143e-05, 'epoch': 16.75}\n","{'loss': 0.9159, 'lr': 8.198442817165668e-05, 'epoch': 16.77}\n","{'loss': 1.0012, 'lr': 8.181230040692393e-05, 'epoch': 16.78}\n","{'loss': 0.9282, 'grad_norm': 1.4379677772521973, 'learning_rate': 8.164022835415934e-05, 'epoch': 16.8}\n","{'loss': 0.7887, 'lr': 8.164022835415934e-05, 'epoch': 16.8}\n","{'loss': 1.0063, 'lr': 8.146821254044834e-05, 'epoch': 16.82}\n","{'loss': 0.9865, 'lr': 8.12962534927042e-05, 'epoch': 16.83}\n","{'loss': 0.9498, 'lr': 8.112435173766629e-05, 'epoch': 16.85}\n","{'loss': 0.8995, 'lr': 8.095250780189842e-05, 'epoch': 16.87}\n","{'loss': 1.0659, 'lr': 8.078072221178736e-05, 'epoch': 16.88}\n","{'loss': 0.9009, 'lr': 8.06089954935411e-05, 'epoch': 16.9}\n","{'loss': 0.9054, 'lr': 8.043732817318736e-05, 'epoch': 16.92}\n","{'loss': 0.9379, 'grad_norm': 2.365766763687134, 'learning_rate': 8.026572077657186e-05, 'epoch': 16.93}\n","{'loss': 0.9487, 'lr': 8.026572077657186e-05, 'epoch': 16.93}\n","{'loss': 0.8552, 'lr': 8.009417382935673e-05, 'epoch': 16.95}\n","{'loss': 0.8377, 'lr': 7.992268785701904e-05, 'epoch': 16.97}\n","{'loss': 1.0549, 'lr': 7.975126338484902e-05, 'epoch': 16.98}\n"," 57% 1020/1800 [04:41<03:15,  3.99it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8544, 'lr': 7.957990093794849e-05, 'epoch': 17.0}\n","{'loss': 0.8205, 'lr': 7.940860104122934e-05, 'epoch': 17.02}\n","{'loss': 0.8284, 'lr': 7.923736421941182e-05, 'epoch': 17.03}\n","{'loss': 0.9514, 'lr': 7.906619099702297e-05, 'epoch': 17.05}\n","{'loss': 0.8939, 'grad_norm': 1.3538838624954224, 'learning_rate': 7.889508189839506e-05, 'epoch': 17.07}\n","{'loss': 0.8612, 'lr': 7.889508189839506e-05, 'epoch': 17.07}\n","{'loss': 0.7579, 'lr': 7.872403744766383e-05, 'epoch': 17.08}\n","{'loss': 0.788, 'lr': 7.855305816876708e-05, 'epoch': 17.1}\n","{'loss': 0.9399, 'lr': 7.838214458544299e-05, 'epoch': 17.12}\n","{'loss': 0.9192, 'lr': 7.821129722122846e-05, 'epoch': 17.13}\n","{'loss': 0.9125, 'lr': 7.804051659945757e-05, 'epoch': 17.15}\n","{'loss': 0.8919, 'lr': 7.786980324325995e-05, 'epoch': 17.17}\n","{'loss': 0.8784, 'lr': 7.769915767555915e-05, 'epoch': 17.18}\n","{'loss': 0.8686, 'grad_norm': 1.765419840812683, 'learning_rate': 7.752858041907116e-05, 'epoch': 17.2}\n","{'loss': 0.9366, 'lr': 7.752858041907116e-05, 'epoch': 17.2}\n","{'loss': 0.9196, 'lr': 7.735807199630264e-05, 'epoch': 17.22}\n","{'loss': 0.8731, 'lr': 7.718763292954942e-05, 'epoch': 17.23}\n","{'loss': 0.8277, 'lr': 7.70172637408949e-05, 'epoch': 17.25}\n","{'loss': 0.8502, 'lr': 7.684696495220842e-05, 'epoch': 17.27}\n","{'loss': 0.8918, 'lr': 7.667673708514366e-05, 'epoch': 17.28}\n","{'loss': 0.8178, 'lr': 7.650658066113712e-05, 'epoch': 17.3}\n","{'loss': 0.825, 'lr': 7.633649620140636e-05, 'epoch': 17.32}\n","{'loss': 0.8677, 'grad_norm': 3.17989182472229, 'learning_rate': 7.616648422694858e-05, 'epoch': 17.33}\n","{'loss': 0.9829, 'lr': 7.616648422694858e-05, 'epoch': 17.33}\n","{'loss': 0.8767, 'lr': 7.599654525853888e-05, 'epoch': 17.35}\n","{'loss': 0.8035, 'lr': 7.582667981672878e-05, 'epoch': 17.37}\n","{'loss': 0.9358, 'lr': 7.565688842184456e-05, 'epoch': 17.38}\n","{'loss': 0.908, 'lr': 7.54871715939857e-05, 'epoch': 17.4}\n","{'loss': 0.8592, 'lr': 7.531752985302323e-05, 'epoch': 17.42}\n","{'loss': 1.0246, 'lr': 7.514796371859819e-05, 'epoch': 17.43}\n","{'loss': 0.8144, 'lr': 7.497847371012004e-05, 'epoch': 17.45}\n","{'loss': 0.9007, 'grad_norm': 0.9117438793182373, 'learning_rate': 7.480906034676511e-05, 'epoch': 17.47}\n","{'loss': 0.8288, 'lr': 7.480906034676511e-05, 'epoch': 17.47}\n","{'loss': 0.9573, 'lr': 7.46397241474748e-05, 'epoch': 17.48}\n","{'loss': 0.8716, 'lr': 7.447046563095424e-05, 'epoch': 17.5}\n","{'loss': 0.8249, 'lr': 7.430128531567063e-05, 'epoch': 17.52}\n","{'loss': 1.0236, 'lr': 7.413218371985158e-05, 'epoch': 17.53}\n","{'loss': 0.8506, 'lr': 7.396316136148359e-05, 'epoch': 17.55}\n","{'loss': 0.863, 'lr': 7.379421875831041e-05, 'epoch': 17.57}\n","{'loss': 0.9357, 'lr': 7.362535642783155e-05, 'epoch': 17.58}\n","{'loss': 0.8944, 'grad_norm': 1.2903823852539062, 'learning_rate': 7.345657488730062e-05, 'epoch': 17.6}\n","{'loss': 0.8387, 'lr': 7.345657488730062e-05, 'epoch': 17.6}\n","{'loss': 0.8203, 'lr': 7.328787465372363e-05, 'epoch': 17.62}\n","{'loss': 0.8928, 'lr': 7.311925624385769e-05, 'epoch': 17.63}\n","{'loss': 0.8621, 'lr': 7.29507201742092e-05, 'epoch': 17.65}\n","{'loss': 0.8278, 'lr': 7.278226696103239e-05, 'epoch': 17.67}\n","{'loss': 0.85, 'lr': 7.261389712032756e-05, 'epoch': 17.68}\n","{'loss': 0.9426, 'lr': 7.244561116783981e-05, 'epoch': 17.7}\n","{'loss': 0.9624, 'lr': 7.227740961905715e-05, 'epoch': 17.72}\n","{'loss': 0.8746, 'grad_norm': 3.0889549255371094, 'learning_rate': 7.21092929892091e-05, 'epoch': 17.73}\n","{'loss': 0.9571, 'lr': 7.21092929892091e-05, 'epoch': 17.73}\n","{'loss': 0.8526, 'lr': 7.194126179326497e-05, 'epoch': 17.75}\n","{'loss': 0.872, 'lr': 7.177331654593247e-05, 'epoch': 17.77}\n","{'loss': 0.907, 'lr': 7.160545776165599e-05, 'epoch': 17.78}\n","{'loss': 0.8991, 'lr': 7.143768595461513e-05, 'epoch': 17.8}\n","{'loss': 0.9388, 'lr': 7.127000163872298e-05, 'epoch': 17.82}\n","{'loss': 0.9259, 'lr': 7.110240532762469e-05, 'epoch': 17.83}\n","{'loss': 1.3662, 'lr': 7.093489753469577e-05, 'epoch': 17.85}\n","{'loss': 0.9648, 'grad_norm': 18.437467575073242, 'learning_rate': 7.076747877304073e-05, 'epoch': 17.87}\n","{'loss': 0.8677, 'lr': 7.076747877304073e-05, 'epoch': 17.87}\n","{'loss': 0.8941, 'lr': 7.060014955549113e-05, 'epoch': 17.88}\n","{'loss': 0.8513, 'lr': 7.04329103946044e-05, 'epoch': 17.9}\n","{'loss': 0.9391, 'lr': 7.026576180266214e-05, 'epoch': 17.92}\n","{'loss': 0.8624, 'lr': 7.009870429166842e-05, 'epoch': 17.93}\n","{'loss': 0.91, 'lr': 6.993173837334838e-05, 'epoch': 17.95}\n","{'loss': 0.8646, 'lr': 6.976486455914657e-05, 'epoch': 17.97}\n","{'loss': 0.8, 'lr': 6.959808336022541e-05, 'epoch': 17.98}\n","{'loss': 0.8736, 'grad_norm': 1.076189637184143, 'learning_rate': 6.943139528746366e-05, 'epoch': 18.0}\n"," 60% 1080/1800 [04:57<02:57,  4.05it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8007, 'lr': 6.943139528746366e-05, 'epoch': 18.0}\n","{'loss': 0.7803, 'lr': 6.926480085145471e-05, 'epoch': 18.02}\n","{'loss': 0.8319, 'lr': 6.909830056250527e-05, 'epoch': 18.03}\n","{'loss': 0.7469, 'lr': 6.893189493063357e-05, 'epoch': 18.05}\n","{'loss': 0.8109, 'lr': 6.876558446556791e-05, 'epoch': 18.07}\n","{'loss': 0.8907, 'lr': 6.859936967674509e-05, 'epoch': 18.08}\n","{'loss': 0.8446, 'lr': 6.843325107330885e-05, 'epoch': 18.1}\n","{'loss': 0.7942, 'lr': 6.826722916410824e-05, 'epoch': 18.12}\n","{'loss': 0.8125, 'grad_norm': 0.874358057975769, 'learning_rate': 6.810130445769619e-05, 'epoch': 18.13}\n","{'loss': 0.8983, 'lr': 6.810130445769619e-05, 'epoch': 18.13}\n","{'loss': 0.8096, 'lr': 6.793547746232782e-05, 'epoch': 18.15}\n","{'loss': 0.8124, 'lr': 6.776974868595898e-05, 'epoch': 18.17}\n","{'loss': 0.8787, 'lr': 6.760411863624465e-05, 'epoch': 18.18}\n","{'loss': 0.88, 'lr': 6.74385878205374e-05, 'epoch': 18.2}\n","{'loss': 0.8283, 'lr': 6.727315674588583e-05, 'epoch': 18.22}\n","{'loss': 0.7787, 'lr': 6.710782591903301e-05, 'epoch': 18.23}\n","{'loss': 0.9596, 'lr': 6.694259584641496e-05, 'epoch': 18.25}\n","{'loss': 0.8557, 'grad_norm': 1.2986419200897217, 'learning_rate': 6.677746703415901e-05, 'epoch': 18.27}\n","{'loss': 0.8008, 'lr': 6.677746703415901e-05, 'epoch': 18.27}\n","{'loss': 0.9388, 'lr': 6.661243998808235e-05, 'epoch': 18.28}\n","{'loss': 0.9318, 'lr': 6.644751521369049e-05, 'epoch': 18.3}\n","{'loss': 0.8536, 'lr': 6.628269321617556e-05, 'epoch': 18.32}\n","{'loss': 0.927, 'lr': 6.611797450041495e-05, 'epoch': 18.33}\n","{'loss': 0.8041, 'lr': 6.595335957096969e-05, 'epoch': 18.35}\n","{'loss': 0.7519, 'lr': 6.578884893208281e-05, 'epoch': 18.37}\n","{'loss': 0.809, 'lr': 6.562444308767797e-05, 'epoch': 18.38}\n","{'loss': 0.8521, 'grad_norm': 0.9188308715820312, 'learning_rate': 6.54601425413578e-05, 'epoch': 18.4}\n","{'loss': 0.9472, 'lr': 6.54601425413578e-05, 'epoch': 18.4}\n","{'loss': 0.8955, 'lr': 6.52959477964023e-05, 'epoch': 18.42}\n","{'loss': 0.8052, 'lr': 6.513185935576756e-05, 'epoch': 18.43}\n","{'loss': 0.7774, 'lr': 6.496787772208386e-05, 'epoch': 18.45}\n","{'loss': 0.8768, 'lr': 6.480400339765442e-05, 'epoch': 18.47}\n","{'loss': 0.8141, 'lr': 6.464023688445373e-05, 'epoch': 18.48}\n","{'loss': 0.8396, 'lr': 6.447657868412602e-05, 'epoch': 18.5}\n","{'loss': 0.9558, 'lr': 6.431302929798376e-05, 'epoch': 18.52}\n","{'loss': 0.8639, 'grad_norm': 1.4654484987258911, 'learning_rate': 6.414958922700611e-05, 'epoch': 18.53}\n","{'loss': 0.8176, 'lr': 6.414958922700611e-05, 'epoch': 18.53}\n","{'loss': 0.8674, 'lr': 6.398625897183735e-05, 'epoch': 18.55}\n","{'loss': 0.7803, 'lr': 6.382303903278535e-05, 'epoch': 18.57}\n","{'loss': 0.7825, 'lr': 6.365992990982015e-05, 'epoch': 18.58}\n","{'loss': 0.824, 'lr': 6.34969321025723e-05, 'epoch': 18.6}\n","{'loss': 0.9223, 'lr': 6.333404611033133e-05, 'epoch': 18.62}\n","{'loss': 0.9233, 'lr': 6.317127243204431e-05, 'epoch': 18.63}\n","{'loss': 0.8489, 'lr': 6.300861156631429e-05, 'epoch': 18.65}\n","{'loss': 0.8458, 'grad_norm': 1.1680192947387695, 'learning_rate': 6.284606401139875e-05, 'epoch': 18.67}\n","{'loss': 0.7481, 'lr': 6.284606401139875e-05, 'epoch': 18.67}\n","{'loss': 0.8249, 'lr': 6.268363026520798e-05, 'epoch': 18.68}\n","{'loss': 0.9002, 'lr': 6.252131082530378e-05, 'epoch': 18.7}\n","{'loss': 0.8532, 'lr': 6.235910618889773e-05, 'epoch': 18.72}\n","{'loss': 0.7571, 'lr': 6.21970168528498e-05, 'epoch': 18.73}\n","{'loss': 0.9442, 'lr': 6.203504331366677e-05, 'epoch': 18.75}\n","{'loss': 0.7957, 'lr': 6.18731860675007e-05, 'epoch': 18.77}\n","{'loss': 0.9143, 'lr': 6.171144561014742e-05, 'epoch': 18.78}\n","{'loss': 0.8422, 'grad_norm': 1.3997694253921509, 'learning_rate': 6.154982243704507e-05, 'epoch': 18.8}\n","{'loss': 0.7945, 'lr': 6.154982243704507e-05, 'epoch': 18.8}\n","{'loss': 0.7947, 'lr': 6.138831704327239e-05, 'epoch': 18.82}\n","{'loss': 0.7486, 'lr': 6.122692992354748e-05, 'epoch': 18.83}\n","{'loss': 0.8372, 'lr': 6.106566157222609e-05, 'epoch': 18.85}\n","{'loss': 0.8952, 'lr': 6.09045124833002e-05, 'epoch': 18.87}\n","{'loss': 0.8517, 'lr': 6.074348315039642e-05, 'epoch': 18.88}\n","{'loss': 0.8669, 'lr': 6.058257406677457e-05, 'epoch': 18.9}\n","{'loss': 0.7737, 'lr': 6.042178572532609e-05, 'epoch': 18.92}\n","{'loss': 0.8203, 'grad_norm': 1.1005667448043823, 'learning_rate': 6.026111861857262e-05, 'epoch': 18.93}\n","{'loss': 0.7419, 'lr': 6.026111861857262e-05, 'epoch': 18.93}\n","{'loss': 1.0302, 'lr': 6.0100573238664324e-05, 'epoch': 18.95}\n","{'loss': 0.9392, 'lr': 5.99401500773786e-05, 'epoch': 18.97}\n","{'loss': 0.9009, 'lr': 5.977984962611848e-05, 'epoch': 18.98}\n"," 63% 1140/1800 [05:14<02:38,  4.16it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.769, 'lr': 5.9619672375911065e-05, 'epoch': 19.0}\n","{'loss': 0.9047, 'lr': 5.945961881740606e-05, 'epoch': 19.02}\n","{'loss': 0.7983, 'lr': 5.929968944087432e-05, 'epoch': 19.03}\n","{'loss': 0.765, 'lr': 5.9139884736206305e-05, 'epoch': 19.05}\n","{'loss': 0.8561, 'grad_norm': 1.2506128549575806, 'learning_rate': 5.898020519291059e-05, 'epoch': 19.07}\n","{'loss': 0.7376, 'lr': 5.898020519291059e-05, 'epoch': 19.07}\n","{'loss': 0.7834, 'lr': 5.882065130011226e-05, 'epoch': 19.08}\n","{'loss': 0.8055, 'lr': 5.866122354655168e-05, 'epoch': 19.1}\n","{'loss': 0.8946, 'lr': 5.85019224205827e-05, 'epoch': 19.12}\n","{'loss': 0.8739, 'lr': 5.834274841017138e-05, 'epoch': 19.13}\n","{'loss': 0.8475, 'lr': 5.8183702002894266e-05, 'epoch': 19.15}\n","{'loss': 0.8633, 'lr': 5.80247836859372e-05, 'epoch': 19.17}\n","{'loss': 0.8658, 'lr': 5.786599394609362e-05, 'epoch': 19.18}\n","{'loss': 0.834, 'grad_norm': 1.8022606372833252, 'learning_rate': 5.7707333269762964e-05, 'epoch': 19.2}\n","{'loss': 0.8124, 'lr': 5.7707333269762964e-05, 'epoch': 19.2}\n","{'loss': 0.8185, 'lr': 5.7548802142949596e-05, 'epoch': 19.22}\n","{'loss': 0.7894, 'lr': 5.739040105126072e-05, 'epoch': 19.23}\n","{'loss': 0.7606, 'lr': 5.723213047990552e-05, 'epoch': 19.25}\n","{'loss': 0.7413, 'lr': 5.7073990913693155e-05, 'epoch': 19.27}\n","{'loss': 0.7636, 'lr': 5.6915982837031636e-05, 'epoch': 19.28}\n","{'loss': 0.7768, 'lr': 5.675810673392619e-05, 'epoch': 19.3}\n","{'loss': 0.8335, 'lr': 5.6600363087977646e-05, 'epoch': 19.32}\n","{'loss': 0.787, 'grad_norm': 1.297608733177185, 'learning_rate': 5.6442752382381304e-05, 'epoch': 19.33}\n","{'loss': 0.7994, 'lr': 5.6442752382381304e-05, 'epoch': 19.33}\n","{'loss': 0.8053, 'lr': 5.6285275099925086e-05, 'epoch': 19.35}\n","{'loss': 0.7593, 'lr': 5.6127931722988245e-05, 'epoch': 19.37}\n","{'loss': 0.8052, 'lr': 5.597072273353988e-05, 'epoch': 19.38}\n","{'loss': 0.735, 'lr': 5.581364861313746e-05, 'epoch': 19.4}\n","{'loss': 0.8299, 'lr': 5.5656709842925335e-05, 'epoch': 19.42}\n","{'loss': 0.9095, 'lr': 5.549990690363317e-05, 'epoch': 19.43}\n","{'loss': 0.7813, 'lr': 5.534324027557467e-05, 'epoch': 19.45}\n","{'loss': 0.8031, 'grad_norm': 0.9383938312530518, 'learning_rate': 5.51867104386459e-05, 'epoch': 19.47}\n","{'loss': 0.8619, 'lr': 5.51867104386459e-05, 'epoch': 19.47}\n","{'loss': 0.8352, 'lr': 5.503031787232401e-05, 'epoch': 19.48}\n","{'loss': 0.7537, 'lr': 5.487406305566549e-05, 'epoch': 19.5}\n","{'loss': 0.8628, 'lr': 5.471794646730508e-05, 'epoch': 19.52}\n","{'loss': 0.7456, 'lr': 5.456196858545404e-05, 'epoch': 19.53}\n","{'loss': 0.7796, 'lr': 5.440612988789866e-05, 'epoch': 19.55}\n","{'loss': 0.8167, 'lr': 5.4250430851999034e-05, 'epoch': 19.57}\n","{'loss': 0.8894, 'lr': 5.40948719546873e-05, 'epoch': 19.58}\n","{'loss': 0.8181, 'grad_norm': 8.96509838104248, 'learning_rate': 5.3939453672466456e-05, 'epoch': 19.6}\n","{'loss': 1.0327, 'lr': 5.3939453672466456e-05, 'epoch': 19.6}\n","{'loss': 0.8315, 'lr': 5.37841764814087e-05, 'epoch': 19.62}\n","{'loss': 0.816, 'lr': 5.362904085715399e-05, 'epoch': 19.63}\n","{'loss': 0.8423, 'lr': 5.347404727490881e-05, 'epoch': 19.65}\n","{'loss': 0.8064, 'lr': 5.331919620944438e-05, 'epoch': 19.67}\n","{'loss': 0.8141, 'lr': 5.316448813509547e-05, 'epoch': 19.68}\n","{'loss': 0.8739, 'lr': 5.3009923525758776e-05, 'epoch': 19.7}\n","{'loss': 0.7797, 'lr': 5.2855502854891605e-05, 'epoch': 19.72}\n","{'loss': 0.8496, 'grad_norm': 1.4175206422805786, 'learning_rate': 5.2701226595510255e-05, 'epoch': 19.73}\n","{'loss': 0.7854, 'lr': 5.2701226595510255e-05, 'epoch': 19.73}\n","{'loss': 0.868, 'lr': 5.2547095220188813e-05, 'epoch': 19.75}\n","{'loss': 0.6955, 'lr': 5.2393109201057456e-05, 'epoch': 19.77}\n","{'loss': 0.9314, 'lr': 5.2239269009801074e-05, 'epoch': 19.78}\n","{'loss': 0.9027, 'lr': 5.208557511765799e-05, 'epoch': 19.8}\n","{'loss': 0.749, 'lr': 5.193202799541828e-05, 'epoch': 19.82}\n","{'loss': 0.8118, 'lr': 5.177862811342253e-05, 'epoch': 19.83}\n","{'loss': 0.8136, 'lr': 5.1625375941560185e-05, 'epoch': 19.85}\n","{'loss': 0.8197, 'grad_norm': 1.429825782775879, 'learning_rate': 5.147227194926838e-05, 'epoch': 19.87}\n","{'loss': 0.7304, 'lr': 5.147227194926838e-05, 'epoch': 19.87}\n","{'loss': 0.772, 'lr': 5.1319316605530246e-05, 'epoch': 19.88}\n","{'loss': 0.6909, 'lr': 5.116651037887352e-05, 'epoch': 19.9}\n","{'loss': 0.8016, 'lr': 5.101385373736937e-05, 'epoch': 19.92}\n","{'loss': 0.9174, 'lr': 5.0861347148630534e-05, 'epoch': 19.93}\n","{'loss': 0.7956, 'lr': 5.070899107981033e-05, 'epoch': 19.95}\n","{'loss': 0.7959, 'lr': 5.055678599760079e-05, 'epoch': 19.97}\n","{'loss': 0.7746, 'lr': 5.0404732368231646e-05, 'epoch': 19.98}\n","{'loss': 0.7848, 'grad_norm': 4.977991104125977, 'learning_rate': 5.0252830657468556e-05, 'epoch': 20.0}\n"," 67% 1200/1800 [05:31<02:36,  3.84it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.8463, 'lr': 5.0252830657468556e-05, 'epoch': 20.0}\n","{'loss': 0.7625, 'lr': 5.010108133061193e-05, 'epoch': 20.02}\n","{'loss': 0.7652, 'lr': 4.9949484852495344e-05, 'epoch': 20.03}\n","{'loss': 0.7396, 'lr': 4.979804168748413e-05, 'epoch': 20.05}\n","{'loss': 0.7557, 'lr': 4.964675229947414e-05, 'epoch': 20.07}\n","{'loss': 0.828, 'lr': 4.949561715189e-05, 'epoch': 20.08}\n","{'loss': 0.8243, 'lr': 4.934463670768407e-05, 'epoch': 20.1}\n","{'loss': 0.8423, 'lr': 4.9193811429334614e-05, 'epoch': 20.12}\n","{'loss': 0.7955, 'grad_norm': 3.6350350379943848, 'learning_rate': 4.904314177884476e-05, 'epoch': 20.13}\n","{'loss': 0.7949, 'lr': 4.904314177884476e-05, 'epoch': 20.13}\n","{'loss': 0.7309, 'lr': 4.8892628217740936e-05, 'epoch': 20.15}\n","{'loss': 0.7626, 'lr': 4.874227120707122e-05, 'epoch': 20.17}\n","{'loss': 0.7915, 'lr': 4.859207120740443e-05, 'epoch': 20.18}\n","{'loss': 0.7152, 'lr': 4.844202867882821e-05, 'epoch': 20.2}\n","{'loss': 0.7425, 'lr': 4.829214408094803e-05, 'epoch': 20.22}\n","{'loss': 0.6811, 'lr': 4.8142417872885424e-05, 'epoch': 20.23}\n","{'loss': 0.7567, 'lr': 4.7992850513276856e-05, 'epoch': 20.25}\n","{'loss': 0.7469, 'grad_norm': 8.993084907531738, 'learning_rate': 4.784344246027223e-05, 'epoch': 20.27}\n","{'loss': 0.7405, 'lr': 4.784344246027223e-05, 'epoch': 20.27}\n","{'loss': 0.688, 'lr': 4.769419417153338e-05, 'epoch': 20.28}\n","{'loss': 0.8775, 'lr': 4.7545106104232794e-05, 'epoch': 20.3}\n","{'loss': 0.7389, 'lr': 4.7396178715052155e-05, 'epoch': 20.32}\n","{'loss': 0.7479, 'lr': 4.724741246018103e-05, 'epoch': 20.33}\n","{'loss': 0.7606, 'lr': 4.70988077953153e-05, 'epoch': 20.35}\n","{'loss': 0.7976, 'lr': 4.695036517565597e-05, 'epoch': 20.37}\n","{'loss': 0.6996, 'lr': 4.680208505590766e-05, 'epoch': 20.38}\n","{'loss': 0.7563, 'grad_norm': 0.8302904367446899, 'learning_rate': 4.665396789027712e-05, 'epoch': 20.4}\n","{'loss': 0.9024, 'lr': 4.665396789027712e-05, 'epoch': 20.4}\n","{'loss': 0.7964, 'lr': 4.650601413247214e-05, 'epoch': 20.42}\n","{'loss': 0.7324, 'lr': 4.635822423569969e-05, 'epoch': 20.43}\n","{'loss': 0.7361, 'lr': 4.621059865266506e-05, 'epoch': 20.45}\n","{'loss': 0.7326, 'lr': 4.606313783557008e-05, 'epoch': 20.47}\n","{'loss': 0.8151, 'lr': 4.591584223611192e-05, 'epoch': 20.48}\n","{'loss': 0.735, 'lr': 4.57687123054817e-05, 'epoch': 20.5}\n","{'loss': 0.8179, 'lr': 4.562174849436296e-05, 'epoch': 20.52}\n","{'loss': 0.7835, 'grad_norm': 1.5432143211364746, 'learning_rate': 4.547495125293053e-05, 'epoch': 20.53}\n","{'loss': 0.8045, 'lr': 4.547495125293053e-05, 'epoch': 20.53}\n","{'loss': 0.7722, 'lr': 4.5328321030848874e-05, 'epoch': 20.55}\n","{'loss': 0.7671, 'lr': 4.5181858277270916e-05, 'epoch': 20.57}\n","{'loss': 0.7614, 'lr': 4.503556344083656e-05, 'epoch': 20.58}\n","{'loss': 0.7882, 'lr': 4.4889436969671395e-05, 'epoch': 20.6}\n","{'loss': 0.7998, 'lr': 4.4743479311385316e-05, 'epoch': 20.62}\n","{'loss': 0.8345, 'lr': 4.4597690913070964e-05, 'epoch': 20.63}\n","{'loss': 0.7639, 'lr': 4.445207222130271e-05, 'epoch': 20.65}\n","{'loss': 0.7864, 'grad_norm': 1.192978858947754, 'learning_rate': 4.4306623682134873e-05, 'epoch': 20.67}\n","{'loss': 0.7381, 'lr': 4.4306623682134873e-05, 'epoch': 20.67}\n","{'loss': 0.8146, 'lr': 4.416134574110077e-05, 'epoch': 20.68}\n","{'loss': 0.7307, 'lr': 4.401623884321101e-05, 'epoch': 20.7}\n","{'loss': 0.7957, 'lr': 4.387130343295227e-05, 'epoch': 20.72}\n","{'loss': 0.7617, 'lr': 4.372653995428605e-05, 'epoch': 20.73}\n","{'loss': 0.7225, 'lr': 4.3581948850647035e-05, 'epoch': 20.75}\n","{'loss': 0.7749, 'lr': 4.3437530564942056e-05, 'epoch': 20.77}\n","{'loss': 0.7633, 'lr': 4.3293285539548414e-05, 'epoch': 20.78}\n","{'loss': 0.7627, 'grad_norm': 1.4266704320907593, 'learning_rate': 4.314921421631285e-05, 'epoch': 20.8}\n","{'loss': 0.7478, 'lr': 4.314921421631285e-05, 'epoch': 20.8}\n","{'loss': 0.7088, 'lr': 4.30053170365499e-05, 'epoch': 20.82}\n","{'loss': 0.7402, 'lr': 4.286159444104068e-05, 'epoch': 20.83}\n","{'loss': 0.7948, 'lr': 4.2718046870031605e-05, 'epoch': 20.85}\n","{'loss': 0.7207, 'lr': 4.257467476323287e-05, 'epoch': 20.87}\n","{'loss': 0.7726, 'lr': 4.2431478559817304e-05, 'epoch': 20.88}\n","{'loss': 0.7104, 'lr': 4.228845869841875e-05, 'epoch': 20.9}\n","{'loss': 0.8365, 'lr': 4.2145615617131095e-05, 'epoch': 20.92}\n","{'loss': 0.754, 'grad_norm': 1.6623469591140747, 'learning_rate': 4.2002949753506495e-05, 'epoch': 20.93}\n","{'loss': 0.7266, 'lr': 4.2002949753506495e-05, 'epoch': 20.93}\n","{'loss': 0.8062, 'lr': 4.186046154455446e-05, 'epoch': 20.95}\n","{'loss': 0.7718, 'lr': 4.171815142674018e-05, 'epoch': 20.97}\n","{'loss': 0.9139, 'lr': 4.157601983598335e-05, 'epoch': 20.98}\n"," 70% 1260/1800 [05:47<02:08,  4.21it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.6936, 'lr': 4.143406720765687e-05, 'epoch': 21.0}\n","{'loss': 0.7567, 'lr': 4.129229397658534e-05, 'epoch': 21.02}\n","{'loss': 0.7295, 'lr': 4.1150700577043966e-05, 'epoch': 21.03}\n","{'loss': 0.7386, 'lr': 4.1009287442756974e-05, 'epoch': 21.05}\n","{'loss': 0.7671, 'grad_norm': 1.3418024778366089, 'learning_rate': 4.086805500689651e-05, 'epoch': 21.07}\n","{'loss': 0.6955, 'lr': 4.086805500689651e-05, 'epoch': 21.07}\n","{'loss': 0.6944, 'lr': 4.072700370208115e-05, 'epoch': 21.08}\n","{'loss': 0.8004, 'lr': 4.058613396037459e-05, 'epoch': 21.1}\n","{'loss': 0.6927, 'lr': 4.044544621328452e-05, 'epoch': 21.12}\n","{'loss': 0.7711, 'lr': 4.0304940891760987e-05, 'epoch': 21.13}\n","{'loss': 0.744, 'lr': 4.0164618426195375e-05, 'epoch': 21.15}\n","{'loss': 0.7193, 'lr': 4.0024479246418824e-05, 'epoch': 21.17}\n","{'loss': 0.7347, 'lr': 3.9884523781701144e-05, 'epoch': 21.18}\n","{'loss': 0.7315, 'grad_norm': 1.3587833642959595, 'learning_rate': 3.974475246074929e-05, 'epoch': 21.2}\n","{'loss': 0.7384, 'lr': 3.974475246074929e-05, 'epoch': 21.2}\n","{'loss': 0.671, 'lr': 3.960516571170629e-05, 'epoch': 21.22}\n","{'loss': 0.6392, 'lr': 3.946576396214968e-05, 'epoch': 21.23}\n","{'loss': 0.7779, 'lr': 3.9326547639090315e-05, 'epoch': 21.25}\n","{'loss': 0.8432, 'lr': 3.918751716897118e-05, 'epoch': 21.27}\n","{'loss': 0.7262, 'lr': 3.904867297766578e-05, 'epoch': 21.28}\n","{'loss': 0.7376, 'lr': 3.8910015490477206e-05, 'epoch': 21.3}\n","{'loss': 0.7256, 'lr': 3.877154513213646e-05, 'epoch': 21.32}\n","{'loss': 0.7324, 'grad_norm': 1.4328696727752686, 'learning_rate': 3.863326232680148e-05, 'epoch': 21.33}\n","{'loss': 0.6521, 'lr': 3.863326232680148e-05, 'epoch': 21.33}\n","{'loss': 0.6685, 'lr': 3.8495167498055705e-05, 'epoch': 21.35}\n","{'loss': 0.8425, 'lr': 3.835726106890657e-05, 'epoch': 21.37}\n","{'loss': 0.7714, 'lr': 3.821954346178467e-05, 'epoch': 21.38}\n","{'loss': 0.816, 'lr': 3.808201509854199e-05, 'epoch': 21.4}\n","{'loss': 0.7439, 'lr': 3.794467640045102e-05, 'epoch': 21.42}\n","{'loss': 0.7321, 'lr': 3.780752778820309e-05, 'epoch': 21.43}\n","{'loss': 0.7337, 'lr': 3.767056968190742e-05, 'epoch': 21.45}\n","{'loss': 0.745, 'grad_norm': 1.6995025873184204, 'learning_rate': 3.753380250108961e-05, 'epoch': 21.47}\n","{'loss': 0.7619, 'lr': 3.753380250108961e-05, 'epoch': 21.47}\n","{'loss': 0.7029, 'lr': 3.739722666469043e-05, 'epoch': 21.48}\n","{'loss': 0.7055, 'lr': 3.7260842591064506e-05, 'epoch': 21.5}\n","{'loss': 0.7878, 'lr': 3.7124650697979045e-05, 'epoch': 21.52}\n","{'loss': 0.6879, 'lr': 3.698865140261266e-05, 'epoch': 21.53}\n","{'loss': 0.7182, 'lr': 3.685284512155398e-05, 'epoch': 21.55}\n","{'loss': 0.7341, 'lr': 3.671723227080032e-05, 'epoch': 21.57}\n","{'loss': 0.6717, 'lr': 3.658181326575659e-05, 'epoch': 21.58}\n","{'loss': 0.7212, 'grad_norm': 5.598485946655273, 'learning_rate': 3.644658852123382e-05, 'epoch': 21.6}\n","{'loss': 0.6871, 'lr': 3.644658852123382e-05, 'epoch': 21.6}\n","{'loss': 0.6669, 'lr': 3.631155845144812e-05, 'epoch': 21.62}\n","{'loss': 0.7663, 'lr': 3.617672347001908e-05, 'epoch': 21.63}\n","{'loss': 0.7356, 'lr': 3.604208398996887e-05, 'epoch': 21.65}\n","{'loss': 0.6987, 'lr': 3.590764042372079e-05, 'epoch': 21.67}\n","{'loss': 0.7661, 'lr': 3.577339318309793e-05, 'epoch': 21.68}\n","{'loss': 0.7427, 'lr': 3.5639342679322095e-05, 'epoch': 21.7}\n","{'loss': 0.7151, 'lr': 3.5505489323012386e-05, 'epoch': 21.72}\n","{'loss': 0.7223, 'grad_norm': 1.2274971008300781, 'learning_rate': 3.537183352418409e-05, 'epoch': 21.73}\n","{'loss': 0.7993, 'lr': 3.537183352418409e-05, 'epoch': 21.73}\n","{'loss': 0.7904, 'lr': 3.523837569224725e-05, 'epoch': 21.75}\n","{'loss': 0.7556, 'lr': 3.510511623600552e-05, 'epoch': 21.77}\n","{'loss': 0.6818, 'lr': 3.497205556365499e-05, 'epoch': 21.78}\n","{'loss': 0.7642, 'lr': 3.483919408278269e-05, 'epoch': 21.8}\n","{'loss': 0.7492, 'lr': 3.470653220036565e-05, 'epoch': 21.82}\n","{'loss': 0.6803, 'lr': 3.457407032276935e-05, 'epoch': 21.83}\n","{'loss': 0.6669, 'lr': 3.444180885574676e-05, 'epoch': 21.85}\n","{'loss': 0.736, 'grad_norm': 0.8843344449996948, 'learning_rate': 3.430974820443683e-05, 'epoch': 21.87}\n","{'loss': 0.7715, 'lr': 3.430974820443683e-05, 'epoch': 21.87}\n","{'loss': 0.6915, 'lr': 3.41778887733635e-05, 'epoch': 21.88}\n","{'loss': 0.6712, 'lr': 3.4046230966434243e-05, 'epoch': 21.9}\n","{'loss': 0.7772, 'lr': 3.391477518693894e-05, 'epoch': 21.92}\n","{'loss': 0.7416, 'lr': 3.37835218375487e-05, 'epoch': 21.93}\n","{'loss': 0.8086, 'lr': 3.365247132031444e-05, 'epoch': 21.95}\n","{'loss': 0.7465, 'lr': 3.35216240366659e-05, 'epoch': 21.97}\n","{'loss': 0.6601, 'lr': 3.3390980387410134e-05, 'epoch': 21.98}\n","{'loss': 0.7335, 'grad_norm': 1.2012859582901, 'learning_rate': 3.3260540772730574e-05, 'epoch': 22.0}\n"," 73% 1320/1800 [06:04<01:53,  4.22it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.7009, 'lr': 3.3260540772730574e-05, 'epoch': 22.0}\n","{'loss': 0.6592, 'lr': 3.313030559218553e-05, 'epoch': 22.02}\n","{'loss': 0.6925, 'lr': 3.300027524470724e-05, 'epoch': 22.03}\n","{'loss': 0.6486, 'lr': 3.2870450128600364e-05, 'epoch': 22.05}\n","{'loss': 0.6992, 'lr': 3.2740830641540944e-05, 'epoch': 22.07}\n","{'loss': 0.6537, 'lr': 3.261141718057523e-05, 'epoch': 22.08}\n","{'loss': 0.6915, 'lr': 3.248221014211825e-05, 'epoch': 22.1}\n","{'loss': 0.6965, 'lr': 3.2353209921952854e-05, 'epoch': 22.12}\n","{'loss': 0.6803, 'grad_norm': 1.8875709772109985, 'learning_rate': 3.222441691522825e-05, 'epoch': 22.13}\n","{'loss': 0.6882, 'lr': 3.222441691522825e-05, 'epoch': 22.13}\n","{'loss': 0.7009, 'lr': 3.209583151645902e-05, 'epoch': 22.15}\n","{'loss': 0.6509, 'lr': 3.1967454119523744e-05, 'epoch': 22.17}\n","{'loss': 0.5969, 'lr': 3.183928511766385e-05, 'epoch': 22.18}\n","{'loss': 0.6437, 'lr': 3.17113249034825e-05, 'epoch': 22.2}\n","{'loss': 0.6937, 'lr': 3.158357386894318e-05, 'epoch': 22.22}\n","{'loss': 0.6609, 'lr': 3.1456032405368765e-05, 'epoch': 22.23}\n","{'loss': 0.6861, 'lr': 3.1328700903440046e-05, 'epoch': 22.25}\n","{'loss': 0.6652, 'grad_norm': 1.2934094667434692, 'learning_rate': 3.120157975319477e-05, 'epoch': 22.27}\n","{'loss': 0.6792, 'lr': 3.120157975319477e-05, 'epoch': 22.27}\n","{'loss': 0.6546, 'lr': 3.107466934402625e-05, 'epoch': 22.28}\n","{'loss': 0.6692, 'lr': 3.094797006468238e-05, 'epoch': 22.3}\n","{'loss': 0.6422, 'lr': 3.0821482303264216e-05, 'epoch': 22.32}\n","{'loss': 0.6636, 'lr': 3.069520644722492e-05, 'epoch': 22.33}\n","{'loss': 0.6585, 'lr': 3.056914288336863e-05, 'epoch': 22.35}\n","{'loss': 0.6682, 'lr': 3.0443291997849077e-05, 'epoch': 22.37}\n","{'loss': 0.5981, 'lr': 3.0317654176168674e-05, 'epoch': 22.38}\n","{'loss': 0.6542, 'grad_norm': 1.6357780694961548, 'learning_rate': 3.0192229803177008e-05, 'epoch': 22.4}\n","{'loss': 0.6249, 'lr': 3.0192229803177008e-05, 'epoch': 22.4}\n","{'loss': 0.6954, 'lr': 3.0067019263069972e-05, 'epoch': 22.42}\n","{'loss': 0.701, 'lr': 2.994202293938847e-05, 'epoch': 22.43}\n","{'loss': 0.6549, 'lr': 2.9817241215017046e-05, 'epoch': 22.45}\n","{'loss': 0.6722, 'lr': 2.969267447218308e-05, 'epoch': 22.47}\n","{'loss': 0.7286, 'lr': 2.956832309245531e-05, 'epoch': 22.48}\n","{'loss': 0.6959, 'lr': 2.9444187456742855e-05, 'epoch': 22.5}\n","{'loss': 0.7118, 'lr': 2.9320267945293978e-05, 'epoch': 22.52}\n","{'loss': 0.6856, 'grad_norm': 8.867138862609863, 'learning_rate': 2.9196564937694816e-05, 'epoch': 22.53}\n","{'loss': 0.6369, 'lr': 2.9196564937694816e-05, 'epoch': 22.53}\n","{'loss': 0.6644, 'lr': 2.9073078812868447e-05, 'epoch': 22.55}\n","{'loss': 0.7066, 'lr': 2.8949809949073526e-05, 'epoch': 22.57}\n","{'loss': 0.6701, 'lr': 2.882675872390319e-05, 'epoch': 22.58}\n","{'loss': 0.663, 'lr': 2.8703925514283924e-05, 'epoch': 22.6}\n","{'loss': 0.6996, 'lr': 2.858131069647445e-05, 'epoch': 22.62}\n","{'loss': 0.6904, 'lr': 2.845891464606448e-05, 'epoch': 22.63}\n","{'loss': 0.7009, 'lr': 2.8336737737973573e-05, 'epoch': 22.65}\n","{'loss': 0.679, 'grad_norm': 1.7170652151107788, 'learning_rate': 2.8214780346450087e-05, 'epoch': 22.67}\n","{'loss': 0.7155, 'lr': 2.8214780346450087e-05, 'epoch': 22.67}\n","{'loss': 0.6355, 'lr': 2.8093042845069872e-05, 'epoch': 22.68}\n","{'loss': 0.7095, 'lr': 2.797152560673537e-05, 'epoch': 22.7}\n","{'loss': 0.7227, 'lr': 2.785022900367409e-05, 'epoch': 22.72}\n","{'loss': 0.7019, 'lr': 2.772915340743789e-05, 'epoch': 22.73}\n","{'loss': 0.6554, 'lr': 2.760829918890163e-05, 'epoch': 22.75}\n","{'loss': 0.6452, 'lr': 2.7487666718261938e-05, 'epoch': 22.77}\n","{'loss': 0.8001, 'lr': 2.736725636503633e-05, 'epoch': 22.78}\n","{'loss': 0.6982, 'grad_norm': 1.6956417560577393, 'learning_rate': 2.72470684980618e-05, 'epoch': 22.8}\n","{'loss': 0.6223, 'lr': 2.72470684980618e-05, 'epoch': 22.8}\n","{'loss': 0.6726, 'lr': 2.7127103485493977e-05, 'epoch': 22.82}\n","{'loss': 0.6078, 'lr': 2.7007361694805733e-05, 'epoch': 22.83}\n","{'loss': 0.7502, 'lr': 2.6887843492786203e-05, 'epoch': 22.85}\n","{'loss': 0.6157, 'lr': 2.6768549245539676e-05, 'epoch': 22.87}\n","{'loss': 0.6783, 'lr': 2.664947931848436e-05, 'epoch': 22.88}\n","{'loss': 0.7572, 'lr': 2.6530634076351434e-05, 'epoch': 22.9}\n","{'loss': 0.6738, 'lr': 2.6412013883183696e-05, 'epoch': 22.92}\n","{'loss': 0.6722, 'grad_norm': 1.4104626178741455, 'learning_rate': 2.629361910233472e-05, 'epoch': 22.93}\n","{'loss': 0.7083, 'lr': 2.629361910233472e-05, 'epoch': 22.93}\n","{'loss': 0.6908, 'lr': 2.6175450096467468e-05, 'epoch': 22.95}\n","{'loss': 0.7158, 'lr': 2.6057507227553445e-05, 'epoch': 22.97}\n","{'loss': 0.6934, 'lr': 2.5939790856871382e-05, 'epoch': 22.98}\n"," 77% 1380/1800 [06:20<01:45,  4.00it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.6069, 'lr': 2.5822301345006194e-05, 'epoch': 23.0}\n","{'loss': 0.6392, 'lr': 2.5705039051847966e-05, 'epoch': 23.02}\n","{'loss': 0.6316, 'lr': 2.5588004336590687e-05, 'epoch': 23.03}\n","{'loss': 0.5863, 'lr': 2.547119755773133e-05, 'epoch': 23.05}\n","{'loss': 0.659, 'grad_norm': 1.1695420742034912, 'learning_rate': 2.5354619073068543e-05, 'epoch': 23.07}\n","{'loss': 0.5785, 'lr': 2.5354619073068543e-05, 'epoch': 23.07}\n","{'loss': 0.744, 'lr': 2.5238269239701817e-05, 'epoch': 23.08}\n","{'loss': 0.5742, 'lr': 2.512214841403012e-05, 'epoch': 23.1}\n","{'loss': 0.6267, 'lr': 2.500625695175095e-05, 'epoch': 23.12}\n","{'loss': 0.568, 'lr': 2.4890595207859325e-05, 'epoch': 23.13}\n","{'loss': 0.6347, 'lr': 2.477516353664645e-05, 'epoch': 23.15}\n","{'loss': 0.6106, 'lr': 2.4659962291698933e-05, 'epoch': 23.17}\n","{'loss': 0.5849, 'lr': 2.454499182589739e-05, 'epoch': 23.18}\n","{'loss': 0.6152, 'grad_norm': 1.0900444984436035, 'learning_rate': 2.4430252491415672e-05, 'epoch': 23.2}\n","{'loss': 0.6029, 'lr': 2.4430252491415672e-05, 'epoch': 23.2}\n","{'loss': 0.729, 'lr': 2.4315744639719518e-05, 'epoch': 23.22}\n","{'loss': 0.7271, 'lr': 2.4201468621565683e-05, 'epoch': 23.23}\n","{'loss': 0.6338, 'lr': 2.4087424787000712e-05, 'epoch': 23.25}\n","{'loss': 0.6123, 'lr': 2.3973613485359947e-05, 'epoch': 23.27}\n","{'loss': 0.5921, 'lr': 2.3860035065266485e-05, 'epoch': 23.28}\n","{'loss': 0.6651, 'lr': 2.3746689874630003e-05, 'epoch': 23.3}\n","{'loss': 0.5872, 'lr': 2.363357826064585e-05, 'epoch': 23.32}\n","{'loss': 0.6437, 'grad_norm': 1.44223153591156, 'learning_rate': 2.352070056979375e-05, 'epoch': 23.33}\n","{'loss': 0.6233, 'lr': 2.352070056979375e-05, 'epoch': 23.33}\n","{'loss': 0.6256, 'lr': 2.3408057147837027e-05, 'epoch': 23.35}\n","{'loss': 0.6425, 'lr': 2.3295648339821364e-05, 'epoch': 23.37}\n","{'loss': 0.6352, 'lr': 2.3183474490073664e-05, 'epoch': 23.38}\n","{'loss': 0.613, 'lr': 2.3071535942201273e-05, 'epoch': 23.4}\n","{'loss': 0.613, 'lr': 2.295983303909065e-05, 'epoch': 23.42}\n","{'loss': 0.5782, 'lr': 2.284836612290654e-05, 'epoch': 23.43}\n","{'loss': 0.6942, 'lr': 2.2737135535090705e-05, 'epoch': 23.45}\n","{'loss': 0.6281, 'grad_norm': 3.4776456356048584, 'learning_rate': 2.2626141616361084e-05, 'epoch': 23.47}\n","{'loss': 0.636, 'lr': 2.2626141616361084e-05, 'epoch': 23.47}\n","{'loss': 0.7396, 'lr': 2.251538470671066e-05, 'epoch': 23.48}\n","{'loss': 0.6729, 'lr': 2.2404865145406352e-05, 'epoch': 23.5}\n","{'loss': 0.637, 'lr': 2.2294583270988088e-05, 'epoch': 23.52}\n","{'loss': 0.6345, 'lr': 2.218453942126766e-05, 'epoch': 23.53}\n","{'loss': 0.6099, 'lr': 2.2074733933327872e-05, 'epoch': 23.55}\n","{'loss': 0.6092, 'lr': 2.1965167143521246e-05, 'epoch': 23.57}\n","{'loss': 0.6491, 'lr': 2.1855839387469233e-05, 'epoch': 23.58}\n","{'loss': 0.6485, 'grad_norm': 1.7424299716949463, 'learning_rate': 2.174675100006107e-05, 'epoch': 23.6}\n","{'loss': 0.603, 'lr': 2.174675100006107e-05, 'epoch': 23.6}\n","{'loss': 0.6641, 'lr': 2.163790231545271e-05, 'epoch': 23.62}\n","{'loss': 0.5821, 'lr': 2.1529293667065952e-05, 'epoch': 23.63}\n","{'loss': 0.7258, 'lr': 2.1420925387587178e-05, 'epoch': 23.65}\n","{'loss': 0.6028, 'lr': 2.131279780896662e-05, 'epoch': 23.67}\n","{'loss': 0.6919, 'lr': 2.1204911262417117e-05, 'epoch': 23.68}\n","{'loss': 0.6053, 'lr': 2.1097266078413247e-05, 'epoch': 23.7}\n","{'loss': 0.6109, 'lr': 2.0989862586690245e-05, 'epoch': 23.72}\n","{'loss': 0.6357, 'grad_norm': 2.0421595573425293, 'learning_rate': 2.088270111624293e-05, 'epoch': 23.73}\n","{'loss': 0.6191, 'lr': 2.088270111624293e-05, 'epoch': 23.73}\n","{'loss': 0.5953, 'lr': 2.0775781995324882e-05, 'epoch': 23.75}\n","{'loss': 0.6138, 'lr': 2.0669105551447243e-05, 'epoch': 23.77}\n","{'loss': 0.5919, 'lr': 2.05626721113778e-05, 'epoch': 23.78}\n","{'loss': 0.6094, 'lr': 2.0456482001139975e-05, 'epoch': 23.8}\n","{'loss': 0.7197, 'lr': 2.0350535546011885e-05, 'epoch': 23.82}\n","{'loss': 0.7722, 'lr': 2.024483307052526e-05, 'epoch': 23.83}\n","{'loss': 0.6148, 'lr': 2.0139374898464447e-05, 'epoch': 23.85}\n","{'loss': 0.642, 'grad_norm': 2.287813663482666, 'learning_rate': 2.0034161352865522e-05, 'epoch': 23.87}\n","{'loss': 0.6445, 'lr': 2.0034161352865522e-05, 'epoch': 23.87}\n","{'loss': 0.5947, 'lr': 1.9929192756015137e-05, 'epoch': 23.88}\n","{'loss': 0.6156, 'lr': 1.982446942944972e-05, 'epoch': 23.9}\n","{'loss': 0.6593, 'lr': 1.971999169395432e-05, 'epoch': 23.92}\n","{'loss': 0.6419, 'lr': 1.961575986956171e-05, 'epoch': 23.93}\n","{'loss': 0.6189, 'lr': 1.9511774275551463e-05, 'epoch': 23.95}\n","{'loss': 0.6657, 'lr': 1.940803523044882e-05, 'epoch': 23.97}\n","{'loss': 0.5999, 'lr': 1.930454305202386e-05, 'epoch': 23.98}\n","{'loss': 0.6301, 'grad_norm': 1.000767707824707, 'learning_rate': 1.920129805729043e-05, 'epoch': 24.0}\n"," 80% 1440/1800 [06:37<01:26,  4.17it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.5635, 'lr': 1.920129805729043e-05, 'epoch': 24.0}\n","{'loss': 0.5832, 'lr': 1.9098300562505266e-05, 'epoch': 24.02}\n","{'loss': 0.6464, 'lr': 1.8995550883166913e-05, 'epoch': 24.03}\n","{'loss': 0.6643, 'lr': 1.88930493340148e-05, 'epoch': 24.05}\n","{'loss': 0.6403, 'lr': 1.8790796229028397e-05, 'epoch': 24.07}\n","{'loss': 0.6, 'lr': 1.8688791881426017e-05, 'epoch': 24.08}\n","{'loss': 0.593, 'lr': 1.858703660366411e-05, 'epoch': 24.1}\n","{'loss': 0.517, 'lr': 1.848553070743606e-05, 'epoch': 24.12}\n","{'loss': 0.601, 'grad_norm': 1.339833378791809, 'learning_rate': 1.8384274503671484e-05, 'epoch': 24.13}\n","{'loss': 0.6395, 'lr': 1.8384274503671484e-05, 'epoch': 24.13}\n","{'loss': 0.6002, 'lr': 1.8283268302535016e-05, 'epoch': 24.15}\n","{'loss': 0.5721, 'lr': 1.8182512413425625e-05, 'epoch': 24.17}\n","{'loss': 0.567, 'lr': 1.808200714497543e-05, 'epoch': 24.18}\n","{'loss': 0.5925, 'lr': 1.7981752805048858e-05, 'epoch': 24.2}\n","{'loss': 0.5463, 'lr': 1.788174970074179e-05, 'epoch': 24.22}\n","{'loss': 0.565, 'lr': 1.7781998138380428e-05, 'epoch': 24.23}\n","{'loss': 0.5258, 'lr': 1.7682498423520543e-05, 'epoch': 24.25}\n","{'loss': 0.576, 'grad_norm': 1.2649085521697998, 'learning_rate': 1.7583250860946376e-05, 'epoch': 24.27}\n","{'loss': 0.6105, 'lr': 1.7583250860946376e-05, 'epoch': 24.27}\n","{'loss': 0.5815, 'lr': 1.7484255754669878e-05, 'epoch': 24.28}\n","{'loss': 0.5449, 'lr': 1.7385513407929587e-05, 'epoch': 24.3}\n","{'loss': 0.5703, 'lr': 1.728702412318982e-05, 'epoch': 24.32}\n","{'loss': 0.6798, 'lr': 1.7188788202139792e-05, 'epoch': 24.33}\n","{'loss': 0.5477, 'lr': 1.7090805945692536e-05, 'epoch': 24.35}\n","{'loss': 0.613, 'lr': 1.6993077653984136e-05, 'epoch': 24.37}\n","{'loss': 0.5759, 'lr': 1.6895603626372658e-05, 'epoch': 24.38}\n","{'loss': 0.5905, 'grad_norm': 2.2562344074249268, 'learning_rate': 1.679838416143743e-05, 'epoch': 24.4}\n","{'loss': 0.5758, 'lr': 1.679838416143743e-05, 'epoch': 24.4}\n","{'loss': 0.5171, 'lr': 1.6701419556977883e-05, 'epoch': 24.42}\n","{'loss': 0.6467, 'lr': 1.6604710110012878e-05, 'epoch': 24.43}\n","{'loss': 0.5483, 'lr': 1.6508256116779618e-05, 'epoch': 24.45}\n","{'loss': 0.5934, 'lr': 1.64120578727328e-05, 'epoch': 24.47}\n","{'loss': 0.5428, 'lr': 1.6316115672543807e-05, 'epoch': 24.48}\n","{'loss': 0.554, 'lr': 1.62204298100996e-05, 'epoch': 24.5}\n","{'loss': 0.6902, 'lr': 1.6125000578502048e-05, 'epoch': 24.52}\n","{'loss': 0.5835, 'grad_norm': 15.160595893859863, 'learning_rate': 1.6029828270066806e-05, 'epoch': 24.53}\n","{'loss': 0.6405, 'lr': 1.6029828270066806e-05, 'epoch': 24.53}\n","{'loss': 0.5975, 'lr': 1.5934913176322608e-05, 'epoch': 24.55}\n","{'loss': 0.5264, 'lr': 1.5840255588010287e-05, 'epoch': 24.57}\n","{'loss': 0.6049, 'lr': 1.5745855795081887e-05, 'epoch': 24.58}\n","{'loss': 0.5278, 'lr': 1.5651714086699744e-05, 'epoch': 24.6}\n","{'loss': 0.5833, 'lr': 1.555783075123566e-05, 'epoch': 24.62}\n","{'loss': 0.5863, 'lr': 1.5464206076270038e-05, 'epoch': 24.63}\n","{'loss': 0.5906, 'lr': 1.53708403485909e-05, 'epoch': 24.65}\n","{'loss': 0.5822, 'grad_norm': 1.3444029092788696, 'learning_rate': 1.527773385419311e-05, 'epoch': 24.67}\n","{'loss': 0.5616, 'lr': 1.527773385419311e-05, 'epoch': 24.67}\n","{'loss': 0.5612, 'lr': 1.5184886878277471e-05, 'epoch': 24.68}\n","{'loss': 0.6126, 'lr': 1.5092299705249768e-05, 'epoch': 24.7}\n","{'loss': 0.5615, 'lr': 1.499997261872007e-05, 'epoch': 24.72}\n","{'loss': 0.6072, 'lr': 1.4907905901501618e-05, 'epoch': 24.73}\n","{'loss': 0.6582, 'lr': 1.4816099835610209e-05, 'epoch': 24.75}\n","{'loss': 0.5972, 'lr': 1.4724554702263226e-05, 'epoch': 24.77}\n","{'loss': 0.5321, 'lr': 1.4633270781878672e-05, 'epoch': 24.78}\n","{'loss': 0.5865, 'grad_norm': 1.4384186267852783, 'learning_rate': 1.454224835407454e-05, 'epoch': 24.8}\n","{'loss': 0.5356, 'lr': 1.454224835407454e-05, 'epoch': 24.8}\n","{'loss': 0.5648, 'lr': 1.4451487697667698e-05, 'epoch': 24.82}\n","{'loss': 0.614, 'lr': 1.4360989090673283e-05, 'epoch': 24.83}\n","{'loss': 0.5777, 'lr': 1.4270752810303657e-05, 'epoch': 24.85}\n","{'loss': 0.5713, 'lr': 1.4180779132967614e-05, 'epoch': 24.87}\n","{'loss': 0.5957, 'lr': 1.4091068334269652e-05, 'epoch': 24.88}\n","{'loss': 0.6207, 'lr': 1.4001620689008898e-05, 'epoch': 24.9}\n","{'loss': 0.5615, 'lr': 1.3912436471178526e-05, 'epoch': 24.92}\n","{'loss': 0.5802, 'grad_norm': 1.7258740663528442, 'learning_rate': 1.3823515953964672e-05, 'epoch': 24.93}\n","{'loss': 0.5608, 'lr': 1.3823515953964672e-05, 'epoch': 24.93}\n","{'loss': 0.5977, 'lr': 1.3734859409745792e-05, 'epoch': 24.95}\n","{'loss': 0.576, 'lr': 1.3646467110091698e-05, 'epoch': 24.97}\n","{'loss': 0.7678, 'lr': 1.3558339325762836e-05, 'epoch': 24.98}\n"," 83% 1500/1800 [06:53<01:11,  4.19it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.5703, 'lr': 1.3470476326709336e-05, 'epoch': 25.0}\n","{'loss': 0.5394, 'lr': 1.338287838207024e-05, 'epoch': 25.02}\n","{'loss': 0.4936, 'lr': 1.3295545760172778e-05, 'epoch': 25.03}\n","{'loss': 0.5265, 'lr': 1.3208478728531338e-05, 'epoch': 25.05}\n","{'loss': 0.579, 'grad_norm': 1.244186282157898, 'learning_rate': 1.3121677553846845e-05, 'epoch': 25.07}\n","{'loss': 0.5652, 'lr': 1.3121677553846845e-05, 'epoch': 25.07}\n","{'loss': 0.6186, 'lr': 1.3035142502005792e-05, 'epoch': 25.08}\n","{'loss': 0.5204, 'lr': 1.2948873838079567e-05, 'epoch': 25.1}\n","{'loss': 0.5012, 'lr': 1.2862871826323497e-05, 'epoch': 25.12}\n","{'loss': 0.5268, 'lr': 1.2777136730176143e-05, 'epoch': 25.13}\n","{'loss': 0.5217, 'lr': 1.2691668812258472e-05, 'epoch': 25.15}\n","{'loss': 0.5195, 'lr': 1.2606468334373001e-05, 'epoch': 25.17}\n","{'loss': 0.5239, 'lr': 1.2521535557503072e-05, 'epoch': 25.18}\n","{'loss': 0.5372, 'grad_norm': 1.1379019021987915, 'learning_rate': 1.243687074181198e-05, 'epoch': 25.2}\n","{'loss': 0.5381, 'lr': 1.243687074181198e-05, 'epoch': 25.2}\n","{'loss': 0.5176, 'lr': 1.2352474146642245e-05, 'epoch': 25.22}\n","{'loss': 0.5417, 'lr': 1.2268346030514743e-05, 'epoch': 25.23}\n","{'loss': 0.551, 'lr': 1.2184486651128013e-05, 'epoch': 25.25}\n","{'loss': 0.5704, 'lr': 1.2100896265357342e-05, 'epoch': 25.27}\n","{'loss': 0.5402, 'lr': 1.2017575129254055e-05, 'epoch': 25.28}\n","{'loss': 0.5277, 'lr': 1.193452349804478e-05, 'epoch': 25.3}\n","{'loss': 0.5117, 'lr': 1.185174162613052e-05, 'epoch': 25.32}\n","{'loss': 0.5373, 'grad_norm': 1.3354686498641968, 'learning_rate': 1.1769229767086054e-05, 'epoch': 25.33}\n","{'loss': 0.5358, 'lr': 1.1769229767086054e-05, 'epoch': 25.33}\n","{'loss': 0.5459, 'lr': 1.1686988173658975e-05, 'epoch': 25.35}\n","{'loss': 0.5636, 'lr': 1.1605017097769077e-05, 'epoch': 25.37}\n","{'loss': 0.4641, 'lr': 1.1523316790507488e-05, 'epoch': 25.38}\n","{'loss': 0.576, 'lr': 1.1441887502135884e-05, 'epoch': 25.4}\n","{'loss': 0.5642, 'lr': 1.1360729482085853e-05, 'epoch': 25.42}\n","{'loss': 0.5334, 'lr': 1.1279842978957956e-05, 'epoch': 25.43}\n","{'loss': 0.563, 'lr': 1.119922824052113e-05, 'epoch': 25.45}\n","{'loss': 0.5432, 'grad_norm': 2.835257053375244, 'learning_rate': 1.1118885513711774e-05, 'epoch': 25.47}\n","{'loss': 0.506, 'lr': 1.1118885513711774e-05, 'epoch': 25.47}\n","{'loss': 0.637, 'lr': 1.1038815044633144e-05, 'epoch': 25.48}\n","{'loss': 0.5389, 'lr': 1.0959017078554457e-05, 'epoch': 25.5}\n","{'loss': 0.5634, 'lr': 1.0879491859910262e-05, 'epoch': 25.52}\n","{'loss': 0.6245, 'lr': 1.0800239632299614e-05, 'epoch': 25.53}\n","{'loss': 0.4717, 'lr': 1.072126063848532e-05, 'epoch': 25.55}\n","{'loss': 0.5825, 'lr': 1.0642555120393305e-05, 'epoch': 25.57}\n","{'loss': 0.5693, 'lr': 1.0564123319111706e-05, 'epoch': 25.58}\n","{'loss': 0.5617, 'grad_norm': 2.0919575691223145, 'learning_rate': 1.0485965474890291e-05, 'epoch': 25.6}\n","{'loss': 0.5819, 'lr': 1.0485965474890291e-05, 'epoch': 25.6}\n","{'loss': 0.5087, 'lr': 1.0408081827139581e-05, 'epoch': 25.62}\n","{'loss': 0.4855, 'lr': 1.0330472614430241e-05, 'epoch': 25.63}\n","{'loss': 0.5345, 'lr': 1.025313807449233e-05, 'epoch': 25.65}\n","{'loss': 0.5278, 'lr': 1.017607844421441e-05, 'epoch': 25.67}\n","{'loss': 0.5249, 'lr': 1.0099293959643075e-05, 'epoch': 25.68}\n","{'loss': 0.5061, 'lr': 1.0022784855982014e-05, 'epoch': 25.7}\n","{'loss': 0.5798, 'lr': 9.946551367591438e-06, 'epoch': 25.72}\n","{'loss': 0.5312, 'grad_norm': 10.11001968383789, 'learning_rate': 9.870593727987321e-06, 'epoch': 25.73}\n","{'loss': 0.5431, 'lr': 9.870593727987321e-06, 'epoch': 25.73}\n","{'loss': 0.5549, 'lr': 9.794912169840565e-06, 'epoch': 25.75}\n","{'loss': 0.5566, 'lr': 9.719506924976496e-06, 'epoch': 25.77}\n","{'loss': 0.5612, 'lr': 9.644378224373985e-06, 'epoch': 25.78}\n","{'loss': 0.5282, 'lr': 9.56952629816481e-06, 'epoch': 25.8}\n","{'loss': 0.5254, 'lr': 9.494951375632943e-06, 'epoch': 25.82}\n","{'loss': 0.5291, 'lr': 9.420653685213855e-06, 'epoch': 25.83}\n","{'loss': 0.5736, 'lr': 9.346633454493847e-06, 'epoch': 25.85}\n","{'loss': 0.5465, 'grad_norm': 1.9059478044509888, 'learning_rate': 9.27289091020922e-06, 'epoch': 25.87}\n","{'loss': 0.4607, 'lr': 9.27289091020922e-06, 'epoch': 25.87}\n","{'loss': 0.5685, 'lr': 9.199426278245781e-06, 'epoch': 25.88}\n","{'loss': 0.498, 'lr': 9.126239783637946e-06, 'epoch': 25.9}\n","{'loss': 0.5151, 'lr': 9.053331650568265e-06, 'epoch': 25.92}\n","{'loss': 0.4707, 'lr': 8.980702102366467e-06, 'epoch': 25.93}\n","{'loss': 0.5276, 'lr': 8.90835136150906e-06, 'epoch': 25.95}\n","{'loss': 0.5382, 'lr': 8.836279649618485e-06, 'epoch': 25.97}\n","{'loss': 0.564, 'lr': 8.764487187462433e-06, 'epoch': 25.98}\n","{'loss': 0.5179, 'grad_norm': 3.2050864696502686, 'learning_rate': 8.692974194953263e-06, 'epoch': 26.0}\n"," 87% 1560/1800 [07:10<00:59,  4.00it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.5267, 'lr': 8.692974194953263e-06, 'epoch': 26.0}\n","{'loss': 0.4584, 'lr': 8.621740891147201e-06, 'epoch': 26.02}\n","{'loss': 0.4886, 'lr': 8.550787494243817e-06, 'epoch': 26.03}\n","{'loss': 0.5296, 'lr': 8.480114221585234e-06, 'epoch': 26.05}\n","{'loss': 0.5013, 'lr': 8.409721289655492e-06, 'epoch': 26.07}\n","{'loss': 0.494, 'lr': 8.339608914079944e-06, 'epoch': 26.08}\n","{'loss': 0.4868, 'lr': 8.269777309624515e-06, 'epoch': 26.1}\n","{'loss': 0.4766, 'lr': 8.200226690195133e-06, 'epoch': 26.12}\n","{'loss': 0.4952, 'grad_norm': 1.5624232292175293, 'learning_rate': 8.130957268836947e-06, 'epoch': 26.13}\n","{'loss': 0.5105, 'lr': 8.130957268836947e-06, 'epoch': 26.13}\n","{'loss': 0.5488, 'lr': 8.061969257733825e-06, 'epoch': 26.15}\n","{'loss': 0.5143, 'lr': 7.993262868207552e-06, 'epoch': 26.17}\n","{'loss': 0.5026, 'lr': 7.924838310717341e-06, 'epoch': 26.18}\n","{'loss': 0.5183, 'lr': 7.856695794859037e-06, 'epoch': 26.2}\n","{'loss': 0.5188, 'lr': 7.788835529364568e-06, 'epoch': 26.22}\n","{'loss': 0.5219, 'lr': 7.721257722101316e-06, 'epoch': 26.23}\n","{'loss': 0.5211, 'lr': 7.653962580071384e-06, 'epoch': 26.25}\n","{'loss': 0.5196, 'grad_norm': 1.6041374206542969, 'learning_rate': 7.586950309411078e-06, 'epoch': 26.27}\n","{'loss': 0.4983, 'lr': 7.586950309411078e-06, 'epoch': 26.27}\n","{'loss': 0.4998, 'lr': 7.52022111539018e-06, 'epoch': 26.28}\n","{'loss': 0.5032, 'lr': 7.453775202411395e-06, 'epoch': 26.3}\n","{'loss': 0.5136, 'lr': 7.387612774009689e-06, 'epoch': 26.32}\n","{'loss': 0.4811, 'lr': 7.321734032851612e-06, 'epoch': 26.33}\n","{'loss': 0.5322, 'lr': 7.256139180734822e-06, 'epoch': 26.35}\n","{'loss': 0.5119, 'lr': 7.190828418587292e-06, 'epoch': 26.37}\n","{'loss': 0.5013, 'lr': 7.125801946466848e-06, 'epoch': 26.38}\n","{'loss': 0.5052, 'grad_norm': 1.7096678018569946, 'learning_rate': 7.061059963560435e-06, 'epoch': 26.4}\n","{'loss': 0.454, 'lr': 7.061059963560435e-06, 'epoch': 26.4}\n","{'loss': 0.4645, 'lr': 6.996602668183605e-06, 'epoch': 26.42}\n","{'loss': 0.4677, 'lr': 6.932430257779821e-06, 'epoch': 26.43}\n","{'loss': 0.4641, 'lr': 6.868542928919941e-06, 'epoch': 26.45}\n","{'loss': 0.534, 'lr': 6.804940877301536e-06, 'epoch': 26.47}\n","{'loss': 0.705, 'lr': 6.741624297748317e-06, 'epoch': 26.48}\n","{'loss': 0.6795, 'lr': 6.678593384209597e-06, 'epoch': 26.5}\n","{'loss': 0.5002, 'lr': 6.615848329759588e-06, 'epoch': 26.52}\n","{'loss': 0.5336, 'grad_norm': 8.764435768127441, 'learning_rate': 6.5533893265969145e-06, 'epoch': 26.53}\n","{'loss': 0.5388, 'lr': 6.5533893265969145e-06, 'epoch': 26.53}\n","{'loss': 0.5578, 'lr': 6.491216566043945e-06, 'epoch': 26.55}\n","{'loss': 0.5038, 'lr': 6.429330238546238e-06, 'epoch': 26.57}\n","{'loss': 0.5115, 'lr': 6.367730533672034e-06, 'epoch': 26.58}\n","{'loss': 0.5109, 'lr': 6.306417640111462e-06, 'epoch': 26.6}\n","{'loss': 0.4697, 'lr': 6.2453917456762325e-06, 'epoch': 26.62}\n","{'loss': 0.5, 'lr': 6.1846530372988375e-06, 'epoch': 26.63}\n","{'loss': 0.4776, 'lr': 6.12420170103214e-06, 'epoch': 26.65}\n","{'loss': 0.5088, 'grad_norm': 1.2680177688598633, 'learning_rate': 6.0640379220486595e-06, 'epoch': 26.67}\n","{'loss': 0.4954, 'lr': 6.0640379220486595e-06, 'epoch': 26.67}\n","{'loss': 0.5289, 'lr': 6.004161884640158e-06, 'epoch': 26.68}\n","{'loss': 0.5284, 'lr': 5.944573772216966e-06, 'epoch': 26.7}\n","{'loss': 0.5467, 'lr': 5.88527376730742e-06, 'epoch': 26.72}\n","{'loss': 0.5096, 'lr': 5.826262051557374e-06, 'epoch': 26.73}\n","{'loss': 0.4914, 'lr': 5.767538805729578e-06, 'epoch': 26.75}\n","{'loss': 0.4621, 'lr': 5.709104209703176e-06, 'epoch': 26.77}\n","{'loss': 0.5122, 'lr': 5.650958442473109e-06, 'epoch': 26.78}\n","{'loss': 0.5093, 'grad_norm': 1.6280478239059448, 'learning_rate': 5.593101682149582e-06, 'epoch': 26.8}\n","{'loss': 0.4521, 'lr': 5.593101682149582e-06, 'epoch': 26.8}\n","{'loss': 0.4778, 'lr': 5.535534105957585e-06, 'epoch': 26.82}\n","{'loss': 0.5086, 'lr': 5.478255890236184e-06, 'epoch': 26.83}\n","{'loss': 0.4855, 'lr': 5.421267210438186e-06, 'epoch': 26.85}\n","{'loss': 0.5167, 'lr': 5.364568241129409e-06, 'epoch': 26.87}\n","{'loss': 0.4656, 'lr': 5.308159155988335e-06, 'epoch': 26.88}\n","{'loss': 0.5831, 'lr': 5.252040127805391e-06, 'epoch': 26.9}\n","{'loss': 0.5122, 'lr': 5.196211328482559e-06, 'epoch': 26.92}\n","{'loss': 0.5002, 'grad_norm': 1.7748942375183105, 'learning_rate': 5.140672929032841e-06, 'epoch': 26.93}\n","{'loss': 0.4655, 'lr': 5.140672929032841e-06, 'epoch': 26.93}\n","{'loss': 0.5565, 'lr': 5.085425099579599e-06, 'epoch': 26.95}\n","{'loss': 0.5379, 'lr': 5.030468009356215e-06, 'epoch': 26.97}\n","{'loss': 0.468, 'lr': 4.97580182670544e-06, 'epoch': 26.98}\n"," 90% 1620/1800 [07:27<00:44,  4.04it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.4735, 'lr': 4.921426719078948e-06, 'epoch': 27.0}\n","{'loss': 0.456, 'lr': 4.867342853036772e-06, 'epoch': 27.02}\n","{'loss': 0.4303, 'lr': 4.813550394246869e-06, 'epoch': 27.03}\n","{'loss': 0.4723, 'lr': 4.760049507484543e-06, 'epoch': 27.05}\n","{'loss': 0.4825, 'grad_norm': 0.977051317691803, 'learning_rate': 4.706840356631958e-06, 'epoch': 27.07}\n","{'loss': 0.4382, 'lr': 4.706840356631958e-06, 'epoch': 27.07}\n","{'loss': 0.4785, 'lr': 4.653923104677671e-06, 'epoch': 27.08}\n","{'loss': 0.4897, 'lr': 4.601297913716052e-06, 'epoch': 27.1}\n","{'loss': 0.4873, 'lr': 4.548964944946887e-06, 'epoch': 27.12}\n","{'loss': 0.4507, 'lr': 4.496924358674815e-06, 'epoch': 27.13}\n","{'loss': 0.5419, 'lr': 4.4451763143088524e-06, 'epoch': 27.15}\n","{'loss': 0.4862, 'lr': 4.393720970361948e-06, 'epoch': 27.17}\n","{'loss': 0.4491, 'lr': 4.342558484450387e-06, 'epoch': 27.18}\n","{'loss': 0.4777, 'grad_norm': 1.3263002634048462, 'learning_rate': 4.291689013293476e-06, 'epoch': 27.2}\n","{'loss': 0.4963, 'lr': 4.291689013293476e-06, 'epoch': 27.2}\n","{'loss': 0.4748, 'lr': 4.241112712712891e-06, 'epoch': 27.22}\n","{'loss': 0.4868, 'lr': 4.190829737632318e-06, 'epoch': 27.23}\n","{'loss': 0.5159, 'lr': 4.140840242076926e-06, 'epoch': 27.25}\n","{'loss': 0.5513, 'lr': 4.091144379172895e-06, 'epoch': 27.27}\n","{'loss': 0.4314, 'lr': 4.041742301146989e-06, 'epoch': 27.28}\n","{'loss': 0.4905, 'lr': 3.992634159326014e-06, 'epoch': 27.3}\n","{'loss': 0.4991, 'lr': 3.943820104136464e-06, 'epoch': 27.32}\n","{'loss': 0.4933, 'grad_norm': 1.9526230096817017, 'learning_rate': 3.895300285103931e-06, 'epoch': 27.33}\n","{'loss': 0.5545, 'lr': 3.895300285103931e-06, 'epoch': 27.33}\n","{'loss': 0.4643, 'lr': 3.8470748508527725e-06, 'epoch': 27.35}\n","{'loss': 0.5049, 'lr': 3.7991439491055215e-06, 'epoch': 27.37}\n","{'loss': 0.4448, 'lr': 3.7515077266826017e-06, 'epoch': 27.38}\n","{'loss': 0.546, 'lr': 3.7041663295017126e-06, 'epoch': 27.4}\n","{'loss': 0.4573, 'lr': 3.657119902577466e-06, 'epoch': 27.42}\n","{'loss': 0.4576, 'lr': 3.6103685900209628e-06, 'epoch': 27.43}\n","{'loss': 0.4717, 'lr': 3.563912535039282e-06, 'epoch': 27.45}\n","{'loss': 0.4876, 'grad_norm': 1.3925766944885254, 'learning_rate': 3.517751879935105e-06, 'epoch': 27.47}\n","{'loss': 0.5055, 'lr': 3.517751879935105e-06, 'epoch': 27.47}\n","{'loss': 0.4734, 'lr': 3.471886766106247e-06, 'epoch': 27.48}\n","{'loss': 0.5225, 'lr': 3.4263173340452257e-06, 'epoch': 27.5}\n","{'loss': 0.4606, 'lr': 3.3810437233388392e-06, 'epoch': 27.52}\n","{'loss': 0.5191, 'lr': 3.3360660726677316e-06, 'epoch': 27.53}\n","{'loss': 0.5161, 'lr': 3.2913845198059957e-06, 'epoch': 27.55}\n","{'loss': 0.4923, 'lr': 3.2469992016206595e-06, 'epoch': 27.57}\n","{'loss': 0.5029, 'lr': 3.202910254071434e-06, 'epoch': 27.58}\n","{'loss': 0.4991, 'grad_norm': 1.6642590761184692, 'learning_rate': 3.1591178122101105e-06, 'epoch': 27.6}\n","{'loss': 0.4497, 'lr': 3.1591178122101105e-06, 'epoch': 27.6}\n","{'loss': 0.4975, 'lr': 3.1156220101802968e-06, 'epoch': 27.62}\n","{'loss': 0.4829, 'lr': 3.0724229812168826e-06, 'epoch': 27.63}\n","{'loss': 0.5149, 'lr': 3.02952085764574e-06, 'epoch': 27.65}\n","{'loss': 0.4809, 'lr': 2.986915770883281e-06, 'epoch': 27.67}\n","{'loss': 0.4326, 'lr': 2.9446078514359653e-06, 'epoch': 27.68}\n","{'loss': 0.5083, 'lr': 2.9025972289000725e-06, 'epoch': 27.7}\n","{'loss': 0.4748, 'lr': 2.860884031961142e-06, 'epoch': 27.72}\n","{'loss': 0.4802, 'grad_norm': 1.5177171230316162, 'learning_rate': 2.819468388393709e-06, 'epoch': 27.73}\n","{'loss': 0.4986, 'lr': 2.819468388393709e-06, 'epoch': 27.73}\n","{'loss': 0.5351, 'lr': 2.778350425060794e-06, 'epoch': 27.75}\n","{'loss': 0.5029, 'lr': 2.737530267913613e-06, 'epoch': 27.77}\n","{'loss': 0.5212, 'lr': 2.6970080419911446e-06, 'epoch': 27.78}\n","{'loss': 0.4655, 'lr': 2.6567838714197545e-06, 'epoch': 27.8}\n","{'loss': 0.4717, 'lr': 2.616857879412793e-06, 'epoch': 27.82}\n","{'loss': 0.4692, 'lr': 2.577230188270263e-06, 'epoch': 27.83}\n","{'loss': 0.4702, 'lr': 2.537900919378422e-06, 'epoch': 27.85}\n","{'loss': 0.4918, 'grad_norm': 1.262417197227478, 'learning_rate': 2.4988701932093794e-06, 'epoch': 27.87}\n","{'loss': 0.4897, 'lr': 2.4988701932093794e-06, 'epoch': 27.87}\n","{'loss': 0.5087, 'lr': 2.4601381293208104e-06, 'epoch': 27.88}\n","{'loss': 0.4957, 'lr': 2.4217048463555104e-06, 'epoch': 27.9}\n","{'loss': 0.474, 'lr': 2.383570462041029e-06, 'epoch': 27.92}\n","{'loss': 0.4922, 'lr': 2.345735093189416e-06, 'epoch': 27.93}\n","{'loss': 0.4866, 'lr': 2.308198855696686e-06, 'epoch': 27.95}\n","{'loss': 0.4618, 'lr': 2.2709618645426534e-06, 'epoch': 27.97}\n","{'loss': 0.4747, 'lr': 2.2340242337904328e-06, 'epoch': 27.98}\n","{'loss': 0.4854, 'grad_norm': 1.7908660173416138, 'learning_rate': 2.197386076586183e-06, 'epoch': 28.0}\n"," 93% 1680/1800 [07:44<00:28,  4.21it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.4405, 'lr': 2.197386076586183e-06, 'epoch': 28.0}\n","{'loss': 0.4873, 'lr': 2.16104750515872e-06, 'epoch': 28.02}\n","{'loss': 0.4672, 'lr': 2.1250086308191475e-06, 'epoch': 28.03}\n","{'loss': 0.4655, 'lr': 2.089269563960594e-06, 'epoch': 28.05}\n","{'loss': 0.4675, 'lr': 2.053830414057789e-06, 'epoch': 28.07}\n","{'loss': 0.4691, 'lr': 2.018691289666774e-06, 'epoch': 28.08}\n","{'loss': 0.4909, 'lr': 1.983852298424582e-06, 'epoch': 28.1}\n","{'loss': 0.4408, 'lr': 1.94931354704887e-06, 'epoch': 28.12}\n","{'loss': 0.4661, 'grad_norm': 1.2616584300994873, 'learning_rate': 1.915075141337619e-06, 'epoch': 28.13}\n","{'loss': 0.4793, 'lr': 1.915075141337619e-06, 'epoch': 28.13}\n","{'loss': 0.4494, 'lr': 1.8811371861687799e-06, 'epoch': 28.15}\n","{'loss': 0.452, 'lr': 1.8474997855000176e-06, 'epoch': 28.17}\n","{'loss': 0.4617, 'lr': 1.8141630423682888e-06, 'epoch': 28.18}\n","{'loss': 0.4939, 'lr': 1.7811270588896534e-06, 'epoch': 28.2}\n","{'loss': 0.4687, 'lr': 1.7483919362588309e-06, 'epoch': 28.22}\n","{'loss': 0.4452, 'lr': 1.7159577747489886e-06, 'epoch': 28.23}\n","{'loss': 0.4104, 'lr': 1.6838246737113983e-06, 'epoch': 28.25}\n","{'loss': 0.4576, 'grad_norm': 0.8858250975608826, 'learning_rate': 1.6519927315751138e-06, 'epoch': 28.27}\n","{'loss': 0.4821, 'lr': 1.6519927315751138e-06, 'epoch': 28.27}\n","{'loss': 0.5368, 'lr': 1.6204620458467158e-06, 'epoch': 28.28}\n","{'loss': 0.461, 'lr': 1.589232713109956e-06, 'epoch': 28.3}\n","{'loss': 0.5338, 'lr': 1.5583048290255364e-06, 'epoch': 28.32}\n","{'loss': 0.4913, 'lr': 1.5276784883307082e-06, 'epoch': 28.33}\n","{'loss': 0.4387, 'lr': 1.4973537848391061e-06, 'epoch': 28.35}\n","{'loss': 0.4509, 'lr': 1.4673308114403595e-06, 'epoch': 28.37}\n","{'loss': 0.43, 'lr': 1.437609660099859e-06, 'epoch': 28.38}\n","{'loss': 0.4781, 'grad_norm': 1.2473177909851074, 'learning_rate': 1.4081904218584685e-06, 'epoch': 28.4}\n","{'loss': 0.5255, 'lr': 1.4081904218584685e-06, 'epoch': 28.4}\n","{'loss': 0.4138, 'lr': 1.3790731868322471e-06, 'epoch': 28.42}\n","{'loss': 0.4446, 'lr': 1.3502580442121492e-06, 'epoch': 28.43}\n","{'loss': 0.469, 'lr': 1.3217450822637811e-06, 'epoch': 28.45}\n","{'loss': 0.4823, 'lr': 1.2935343883271334e-06, 'epoch': 28.47}\n","{'loss': 0.4377, 'lr': 1.2656260488162709e-06, 'epoch': 28.48}\n","{'loss': 0.4319, 'lr': 1.238020149219099e-06, 'epoch': 28.5}\n","{'loss': 0.4508, 'lr': 1.2107167740971203e-06, 'epoch': 28.52}\n","{'loss': 0.4569, 'grad_norm': 1.3997442722320557, 'learning_rate': 1.183716007085145e-06, 'epoch': 28.53}\n","{'loss': 0.4467, 'lr': 1.183716007085145e-06, 'epoch': 28.53}\n","{'loss': 0.4619, 'lr': 1.1570179308910356e-06, 'epoch': 28.55}\n","{'loss': 0.4992, 'lr': 1.1306226272954634e-06, 'epoch': 28.57}\n","{'loss': 0.4497, 'lr': 1.1045301771516747e-06, 'epoch': 28.58}\n","{'loss': 0.476, 'lr': 1.0787406603852135e-06, 'epoch': 28.6}\n","{'loss': 0.4787, 'lr': 1.053254155993666e-06, 'epoch': 28.62}\n","{'loss': 0.4849, 'lr': 1.028070742046483e-06, 'epoch': 28.63}\n","{'loss': 0.448, 'lr': 1.0031904956846694e-06, 'epoch': 28.65}\n","{'loss': 0.4681, 'grad_norm': 2.692514657974243, 'learning_rate': 9.786134931205726e-07, 'epoch': 28.67}\n","{'loss': 0.4929, 'lr': 9.786134931205726e-07, 'epoch': 28.67}\n","{'loss': 0.495, 'lr': 9.543398096376833e-07, 'epoch': 28.68}\n","{'loss': 0.4771, 'lr': 9.303695195903462e-07, 'epoch': 28.7}\n","{'loss': 0.5364, 'lr': 9.067026964035608e-07, 'epoch': 28.72}\n","{'loss': 0.4923, 'lr': 8.833394125728034e-07, 'epoch': 28.73}\n","{'loss': 0.4619, 'lr': 8.602797396636942e-07, 'epoch': 28.75}\n","{'loss': 0.427, 'lr': 8.375237483118636e-07, 'epoch': 28.77}\n","{'loss': 0.4992, 'lr': 8.150715082227534e-07, 'epoch': 28.78}\n","{'loss': 0.4852, 'grad_norm': 1.3479628562927246, 'learning_rate': 7.929230881713045e-07, 'epoch': 28.8}\n","{'loss': 0.4674, 'lr': 7.929230881713045e-07, 'epoch': 28.8}\n","{'loss': 0.4673, 'lr': 7.710785560018474e-07, 'epoch': 28.82}\n","{'loss': 0.4779, 'lr': 7.495379786278456e-07, 'epoch': 28.83}\n","{'loss': 0.4634, 'lr': 7.283014220316742e-07, 'epoch': 28.85}\n","{'loss': 0.4429, 'lr': 7.073689512644976e-07, 'epoch': 28.87}\n","{'loss': 0.4416, 'lr': 6.867406304459367e-07, 'epoch': 28.88}\n","{'loss': 0.449, 'lr': 6.664165227640018e-07, 'epoch': 28.9}\n","{'loss': 0.5025, 'lr': 6.463966904748486e-07, 'epoch': 28.92}\n","{'loss': 0.464, 'grad_norm': 1.1673486232757568, 'learning_rate': 6.266811949025675e-07, 'epoch': 28.93}\n","{'loss': 0.4378, 'lr': 6.266811949025675e-07, 'epoch': 28.93}\n","{'loss': 0.44, 'lr': 6.072700964390277e-07, 'epoch': 28.95}\n","{'loss': 0.4251, 'lr': 5.881634545436554e-07, 'epoch': 28.97}\n","{'loss': 0.4453, 'lr': 5.693613277433119e-07, 'epoch': 28.98}\n"," 97% 1740/1800 [08:00<00:15,  3.96it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","{'loss': 0.4753, 'lr': 5.508637736320488e-07, 'epoch': 29.0}\n","{'loss': 0.4685, 'lr': 5.326708488709753e-07, 'epoch': 29.02}\n","{'loss': 0.4749, 'lr': 5.147826091880581e-07, 'epoch': 29.03}\n","{'loss': 0.5142, 'lr': 4.97199109377977e-07, 'epoch': 29.05}\n","{'loss': 0.4602, 'grad_norm': 1.9731210470199585, 'learning_rate': 4.799204033019367e-07, 'epoch': 29.07}\n","{'loss': 0.489, 'lr': 4.799204033019367e-07, 'epoch': 29.07}\n","{'loss': 0.4291, 'lr': 4.6294654388748804e-07, 'epoch': 29.08}\n","{'loss': 0.4232, 'lr': 4.462775831284294e-07, 'epoch': 29.1}\n","{'loss': 0.4591, 'lr': 4.299135720845615e-07, 'epoch': 29.12}\n","{'loss': 0.4316, 'lr': 4.1385456088159913e-07, 'epoch': 29.13}\n","{'loss': 0.4912, 'lr': 3.9810059871095985e-07, 'epoch': 29.15}\n","{'loss': 0.51, 'lr': 3.8265173382968644e-07, 'epoch': 29.17}\n","{'loss': 0.4795, 'lr': 3.675080135602471e-07, 'epoch': 29.18}\n","{'loss': 0.4641, 'grad_norm': 1.0387288331985474, 'learning_rate': 3.5266948429036885e-07, 'epoch': 29.2}\n","{'loss': 0.4563, 'lr': 3.5266948429036885e-07, 'epoch': 29.2}\n","{'loss': 0.4805, 'lr': 3.3813619147295973e-07, 'epoch': 29.22}\n","{'loss': 0.4542, 'lr': 3.2390817962592024e-07, 'epoch': 29.23}\n","{'loss': 0.4624, 'lr': 3.0998549233205443e-07, 'epoch': 29.25}\n","{'loss': 0.4694, 'lr': 2.9636817223887005e-07, 'epoch': 29.27}\n","{'loss': 0.4608, 'lr': 2.8305626105848973e-07, 'epoch': 29.28}\n","{'loss': 0.4457, 'lr': 2.7004979956753995e-07, 'epoch': 29.3}\n","{'loss': 0.4641, 'lr': 2.5734882760697353e-07, 'epoch': 29.32}\n","{'loss': 0.4617, 'grad_norm': 1.6282163858413696, 'learning_rate': 2.4495338408201394e-07, 'epoch': 29.33}\n","{'loss': 0.4439, 'lr': 2.4495338408201394e-07, 'epoch': 29.33}\n","{'loss': 0.5137, 'lr': 2.328635069619556e-07, 'epoch': 29.35}\n","{'loss': 0.4349, 'lr': 2.2107923328014147e-07, 'epoch': 29.37}\n","{'loss': 0.4384, 'lr': 2.096005991337524e-07, 'epoch': 29.38}\n","{'loss': 0.4965, 'lr': 1.9842763968377364e-07, 'epoch': 29.4}\n","{'loss': 0.4315, 'lr': 1.8756038915486163e-07, 'epoch': 29.42}\n","{'loss': 0.4562, 'lr': 1.7699888083521077e-07, 'epoch': 29.43}\n","{'loss': 0.4626, 'lr': 1.6674314707648687e-07, 'epoch': 29.45}\n","{'loss': 0.4597, 'grad_norm': 1.477760672569275, 'learning_rate': 1.5679321929372714e-07, 'epoch': 29.47}\n","{'loss': 0.4486, 'lr': 1.5679321929372714e-07, 'epoch': 29.47}\n","{'loss': 0.4677, 'lr': 1.4714912796521818e-07, 'epoch': 29.48}\n","{'loss': 0.4558, 'lr': 1.3781090263242924e-07, 'epoch': 29.5}\n","{'loss': 0.4878, 'lr': 1.2877857189992348e-07, 'epoch': 29.52}\n","{'loss': 0.501, 'lr': 1.2005216343521363e-07, 'epoch': 29.53}\n","{'loss': 0.4826, 'lr': 1.11631703968762e-07, 'epoch': 29.55}\n","{'loss': 0.4748, 'lr': 1.0351721929384716e-07, 'epoch': 29.57}\n","{'loss': 0.4718, 'lr': 9.570873426649752e-08, 'epoch': 29.58}\n","{'loss': 0.4738, 'grad_norm': 1.5386779308319092, 'learning_rate': 8.820627280540228e-08, 'epoch': 29.6}\n","{'loss': 0.4985, 'lr': 8.820627280540228e-08, 'epoch': 29.6}\n","{'loss': 0.4766, 'lr': 8.100985789185611e-08, 'epoch': 29.62}\n","{'loss': 0.4654, 'lr': 7.411951156969243e-08, 'epoch': 29.63}\n","{'loss': 0.4196, 'lr': 6.753525494518354e-08, 'epoch': 29.65}\n","{'loss': 0.4863, 'lr': 6.125710818701835e-08, 'epoch': 29.67}\n","{'loss': 0.4657, 'lr': 5.528509052621367e-08, 'epoch': 29.68}\n","{'loss': 0.4729, 'lr': 4.961922025603638e-08, 'epoch': 29.7}\n","{'loss': 0.4208, 'lr': 4.4259514732025717e-08, 'epoch': 29.72}\n","{'loss': 0.4632, 'grad_norm': 1.0251131057739258, 'learning_rate': 3.9205990371837806e-08, 'epoch': 29.73}\n","{'loss': 0.5385, 'lr': 3.9205990371837806e-08, 'epoch': 29.73}\n","{'loss': 0.4925, 'lr': 3.4458662655267873e-08, 'epoch': 29.75}\n","{'loss': 0.4623, 'lr': 3.001754612418362e-08, 'epoch': 29.77}\n","{'loss': 0.4124, 'lr': 2.5882654382447525e-08, 'epoch': 29.78}\n","{'loss': 0.466, 'lr': 2.2054000095950155e-08, 'epoch': 29.8}\n","{'loss': 0.4434, 'lr': 1.8531594992488022e-08, 'epoch': 29.82}\n","{'loss': 0.4452, 'lr': 1.5315449861774688e-08, 'epoch': 29.83}\n","{'loss': 0.5046, 'lr': 1.2405574555407473e-08, 'epoch': 29.85}\n","{'loss': 0.4706, 'grad_norm': 1.8115235567092896, 'learning_rate': 9.80197798682303e-09, 'epoch': 29.87}\n","{'loss': 0.4432, 'lr': 9.80197798682303e-09, 'epoch': 29.87}\n","{'loss': 0.4413, 'lr': 7.504668131264047e-09, 'epoch': 29.88}\n","{'loss': 0.4282, 'lr': 5.513652025779248e-09, 'epoch': 29.9}\n","{'loss': 0.4195, 'lr': 3.828935769190078e-09, 'epoch': 29.92}\n","{'loss': 0.4494, 'lr': 2.450524522057407e-09, 'epoch': 29.93}\n","{'loss': 0.4989, 'lr': 1.3784225066926226e-09, 'epoch': 29.95}\n","{'loss': 0.4459, 'lr': 6.12633007113228e-10, 'epoch': 29.97}\n","{'loss': 0.4823, 'lr': 1.531583690650429e-10, 'epoch': 29.98}\n","{'loss': 0.4511, 'grad_norm': 1.5929765701293945, 'learning_rate': 0.0, 'epoch': 30.0}\n","{'train_runtime': 499.5728, 'train_samples_per_second': 7.206, 'train_steps_per_second': 3.603, 'train_loss': 1.397122914923562, 'epoch': 30.0}\n","100% 1800/1800 [08:19<00:00,  3.60it/s]\n","******model_save_path is model7b_M2_family_epoch_30_r_64_moreData/adapter_model.safetensors******\n"]}],"source":["!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 30 \\\n","    --json_path \"./dataset/family_k1k2.json\" \\\n","    --output_dir \"model7b_M1_family_epoch_30_r_64_moreData\" \\\n","    --save_steps 40 \\\n","    --LORA_R 64\n","\n","!python ./ft_gemma/train_7b_save.py \\\n","    --epochs 30 \\\n","    --json_path \"./dataset/family_k1k2k3.json\" \\\n","    --output_dir \"model7b_M2_family_epoch_30_r_64_moreData\" \\\n","    --save_steps 60 \\\n","    --LORA_R 64"]},{"cell_type":"markdown","metadata":{"id":"BobqXEOvQ54f"},"source":["## Predict on M1"]},{"cell_type":"markdown","metadata":{"id":"VPZ7o-uLQ54f"},"source":["### k1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQJ3ZfZJ6OT9"},"outputs":[],"source":["!chmod a+x *.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TQ3RETIZQ54f","outputId":"39c4f09a-8f27-4cf5-cd9d-74022fc033c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.58s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.18s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.16s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.18s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n"]}],"source":["!./predict_M1k1.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3rl2W3AbN3N"},"outputs":[],"source":["%pwd"]},{"cell_type":"markdown","metadata":{"id":"0g2lCytAQ54f"},"source":["### k2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2862510,"status":"ok","timestamp":1724484635970,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"1kYMOvMzxpWr","outputId":"68fa0927-7706-4f81-ed12-f882e5cd5391"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.16s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.16s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.24s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n"]}],"source":["!./predict_M1k2.sh"]},{"cell_type":"markdown","metadata":{"id":"3hvo5bVfQ54g"},"source":["### k3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2730808,"status":"ok","timestamp":1724487366755,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"Vo_IteQMxpWr","outputId":"199b8a77-0b78-4ecc-9d61-5b0be49d6f23"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.16s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.18s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n"]}],"source":["!./predict_M1k3.sh"]},{"cell_type":"markdown","metadata":{"id":"6h7EEnS6Q54g"},"source":["## Predict on M2"]},{"cell_type":"markdown","metadata":{"id":"9w7wLzRSQ54g"},"source":["### k1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1636194,"status":"ok","timestamp":1724489002920,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"BQq1k9RdQ54g","outputId":"8f616fc7-ee78-4163-dd73-b551822581fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.10s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n"]}],"source":["!./predict_M2k1.sh"]},{"cell_type":"markdown","metadata":{"id":"BNefcbH_Q54g"},"source":["### k2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1768252,"status":"ok","timestamp":1724490771142,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"wr2VDthrxpWr","outputId":"77b636bd-a4ca-45ba-ab07-d0f0f1988efa"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.22s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.21s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.23s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.16s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n"]}],"source":["!./predict_M2k2.sh"]},{"cell_type":"markdown","metadata":{"id":"_HIcaMveQ54h"},"source":["### k3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1490,"status":"ok","timestamp":1724476328487,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"EaAZYtPhxpWs","outputId":"da44c37a-612a-4a90-e1e5-ca30bf5d1e37"},"outputs":[{"name":"stdout","output_type":"stream","text":["for i in $(seq 60 60 1800) \n","do\n","    python ./ft_gemma/p.py \\\n","        --model_dir \"./model7b_M2_family_epoch_30_r_64_moreData/checkpoint-$i\" \\\n","        --data_dir './dataset/family_k3_q_short.txt' \\\n","        --q_num 1 --LORA_R 64 > \"./Results/M2_family_epoch_30_r_64_moreData/k3_checkpoint_$i.txt\"\n","done\n"]}],"source":["%cat ./predict_M2k3.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"toAKu3xcxpWs","outputId":"91b1e2e7-7310-4ed1-bc73-f7dd7b2adcfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.11s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.07s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.20s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.12s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.08s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.06s/it]\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.09s/it]\n"]}],"source":["!./predict_M2k3.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsoNUTwzQ54h"},"outputs":[],"source":["%cp ./Results/M1_family_epoch_30_r_64_moreData/* ./Results/raw_M1_family_epoch_30_r_64_moreData/\n","%cp ./Results/M2_family_epoch_30_r_64_moreData/* ./Results/raw_M2_family_epoch_30_r_64_moreData/"]},{"cell_type":"markdown","metadata":{"id":"VI0QOqJbA6Om"},"source":["# prob -- confidence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skxhzL_ZzYDz"},"outputs":[],"source":["print(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOxHxOZmzzwN"},"outputs":[],"source":["%cat | grep \"generation_output\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z49A8si-A6Om"},"outputs":[],"source":["!python ./ft_gemma/p_with_probability.py \\\n","    --model_dir \"./model7b_M1_family_epoch_15_r_72_moreData/checkpoint-120\" \\\n","    --data_dir './dataset/family_k3_q_short.txt' \\\n","    --q_num 3 --LORA_R 32 > \"./Results/M1_family_epoch_15_r_72_moreData/checkpoint_500_probability_7.txt\""]},{"cell_type":"markdown","metadata":{"id":"-csbXrwTnZHw"},"source":["## Train LoRA_R = 32 lr ="]},{"cell_type":"markdown","metadata":{"id":"ACf98GfLA6On"},"source":["# Cleanse data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwGZ4Ft7QBZo"},"outputs":[],"source":["import os\n","import re\n","\n","folder_path = './Results/M1_family_epoch_15_r_32_moreData/'\n","\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.txt'):# and filename.__contains__('what') == False:\n","        file_path = os.path.join(folder_path, filename)\n","\n","        with open(file_path, 'r') as file:\n","            content = file.readlines()\n","#         if len(content) != 80:\n","#             print(file_path)\n","\n","        modified_content = [line for line in content if not re.match(r'dict.*\\n', line)]\n","#         modified_content = []\n","#         for i in range(len(content) - 1):\n","\n","#             if '<start_of_turn--LORA_R 32 >model' in content[i]:\n","#                 modified_content.append(content[i + 1])\n","\n","        with open(file_path, 'w') as file:\n","            file.writelines(modified_content)\n","\n","\n","\n","folder_path = './Results/M2_family_epoch_15_r_32_moreData/'\n","\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.txt'):# and filename.__contains__('what') == False:\n","        file_path = os.path.join(folder_path, filename)\n","\n","        with open(file_path, 'r') as file:\n","            content = file.readlines()\n","#         if len(content) != 80:\n","#             print(file_path)\n","\n","        modified_content = [line for line in content if not re.match(r'dict.*\\n', line)]\n","#         modified_content = []\n","#         for i in range(len(content) - 1):\n","\n","#             if '<start_of_turn--LORA_R 32 >model' in content[i]:\n","#                 modified_content.append(content[i + 1])\n","\n","        with open(file_path, 'w') as file:\n","            file.writelines(modified_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Gzskw13QBZp"},"outputs":[],"source":["folder_path = './Results/M1_family_epoch_15_r_32_moreData/'\n","\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.txt'):# and filename.__contains__('what') == False:\n","        file_path = os.path.join(folder_path, filename)\n","\n","        with open(file_path, 'r') as file:\n","            content = file.readlines()\n","#         if len(content) != 80:\n","#             print(file_path)\n","\n","        # modified_content = [line for line in content if not re.match(r'dict.*\\n', line)]\n","        modified_content = []\n","        for i in range(len(content) - 1):\n","            if '<start_of_turn--LORA_R 32 >model' in content[i]:\n","                modified_content.append(content[i + 1])\n","\n","        with open(file_path, 'w') as file:\n","            file.writelines(modified_content)\n","\n","folder_path = './Results/M2_family_epoch_15_r_32_moreData/'\n","\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.txt'):# and filename.__contains__('what') == False:\n","        file_path = os.path.join(folder_path, filename)\n","\n","        with open(file_path, 'r') as file:\n","            content = file.readlines()\n","#         if len(content) != 80:\n","#             print(file_path)\n","\n","        # modified_content = [line for line in content if not re.match(r'dict.*\\n', line)]\n","        modified_content = []\n","        for i in range(len(content) - 1):\n","            if '<start_of_turn--LORA_R 32 >model' in content[i]:\n","                modified_content.append(content[i + 1])\n","\n","        with open(file_path, 'w') as file:\n","            file.writelines(modified_content)\n"]},{"cell_type":"markdown","metadata":{"id":"zgS48qQRNOK_"},"source":["# test histogram"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2693KCqkIBfU"},"outputs":[{"name":"stderr","output_type":"stream","text":["Gemma's activation function should be approximate GeLU and not exact GeLU.\n","Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1409fb803df74133a6e189c4280b40d3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/chalmers/users/chenzhao/miniconda3/envs/tofu/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'logits'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids)\n\u001b[0;32m---> 17\u001b[0m     next_token_logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(outputs)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# s = outputs.sequences[0]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(s)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Convert logits to probabilities\u001b[39;00m\n\u001b[1;32m     28\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"]}],"source":["import torch\n","from transformers import GemmaForCausalLM, GemmaTokenizer\n","import matplotlib.pyplot as plt\n","\n","# Initialize the model and tokenizer\n","model_name = 'google/gemma-7b-it'  # Replace with the actual model name\n","model = GemmaForCausalLM.from_pretrained(model_name)#.cuda()\n","tokenizer = GemmaTokenizer.from_pretrained(model_name)\n","\n","# Input text\n","input_text = \"Who is William Shakespeare?\"\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","\n","# Get the model's output probabilities for the next word\n","with torch.no_grad():\n","    outputs = model.generate(input_ids)\n","    next_token_logits = outputs.logits[:, -1, :]\n","\n","# print(outputs)\n","# s = outputs.sequences[0]\n","# print(s)\n","# ss = tokenizer.decode(s)\n","# print(ss)\n","\n","\n","\n","# Convert logits to probabilities\n","probabilities = torch.softmax(next_token_logits, dim=-1).squeeze()\n","\n","# Get the top k words and their probabilities\n","top_k = 10  # Number of top words to display\n","top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n","top_k_words = [tokenizer.decode([idx]) for idx in top_k_indices]\n","\n","# Plot the histogram\n","plt.figure(figsize=(10, 5))\n","plt.bar(top_k_words, top_k_probs.numpy())\n","plt.xlabel('Words')\n","plt.ylabel('Probability')\n","plt.title('Top Words and Their Probabilities')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":748,"referenced_widgets":["88ae5cd505824f208a7d085711a32154","ff9030e6e810422b8ac51d9dcd8d9c82","f25b43bdb5ee42cbbfab0e2a2f5562f8","90b4b44386d24e3eb3f6f6689b194dbf","14a673efa18d40bbaf34412e88c00890","19d74576b9a147ce98d4d6329520d11a","cff4667043ce41729a5c7ebb31e0d709","93425b4fd2694f939ccc91725c85a56b","6863f1fba52441b38d5e5d6413f37a92","96fd7a4258b24b9d8c0769c7cf22e328","42362c2d25ba425897c1d309d7a9b859"]},"executionInfo":{"elapsed":11561,"status":"ok","timestamp":1724766403611,"user":{"displayName":"ZY ZHANG (Z)","userId":"05340926196879469750"},"user_tz":-480},"id":"w3exy8FXNQL4","outputId":"e0a4bd29-5051-495a-bfeb-eed5bb910825"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88ae5cd505824f208a7d085711a32154","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 1000x500 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import GemmaForCausalLM, GemmaTokenizer\n","import matplotlib.pyplot as plt\n","\n","# Initialize the model and tokenizer\n","model_name = 'google/gemma-7b-it'  # Replace with the actual model name\n","model = GemmaForCausalLM.from_pretrained(model_name)\n","tokenizer = GemmaTokenizer.from_pretrained(model_name)\n","\n","# Input text\n","input_text = \"Who is William Shakespeare?\"\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","\n","# Get the model's output probabilities for the next word\n","with torch.no_grad():\n","    outputs = model(input_ids)\n","    next_token_logits = outputs.logits[:, -1, :]\n","\n","# Convert logits to probabilities\n","probabilities = torch.softmax(next_token_logits, dim=-1).squeeze()\n","\n","# Get the top k words and their probabilities\n","top_k = 10  # Number of top words to display\n","top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n","top_k_words = [tokenizer.decode([idx]) for idx in top_k_indices]\n","print(top_k_words)\n","\n","# Plot the histogram\n","plt.figure(figsize=(10, 5))\n","plt.bar(top_k_words, top_k_probs.numpy())\n","plt.xlabel('Words')\n","plt.ylabel('Probability')\n","plt.title('Top Words and Their Probabilities')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xTNe0hZKQBZp"},"source":["# count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPmmnV92A6Oo"},"outputs":[],"source":["import os\n","filenames = []\n","\n","for filename in os.listdir(\"./Results/M1_family_epoch_15_r_32_moreData/\"):\n","    if filename.endswith(\".txt\") and filename.__contains__(\"prob\") == False:\n","        filenames.append(filename)\n","\n","filenames.sort(key=lambda x: int(x.split('_')[1].split(\".\")[0]))\n","\n","for filename in filenames:\n","    a = []\n","    file_path = os.path.join(\"./Results/M1_family_epoch_15_r_32_moreData/\", filename)\n","    print(f'Processing file: {file_path}')\n","\n","    with open(file_path, 'r') as file:\n","        content = [line.rstrip('\\n') for line in file]\n","\n","    c = []\n","    aaa = 0\n","    count = 0\n","    for i in range(20):\n","        if content[i].lower().__contains__(family_k3[i]) or content[i].lower().__contains__(num_en[i]):\n","        # if content[i].__contains__(brand[i]):\n","            count += 1\n","\n","    print(f\"{count}/20\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t3dnvJcA6Oo"},"outputs":[],"source":["filenames = []\n","\n","for filename in os.listdir(\"./Results/M2_family_epoch_15_r_32_moreData/\"):\n","    if filename.endswith(\".txt\"):\n","        filenames.append(filename)\n","\n","filenames.sort(key=lambda x: int(x.split('_')[1].split(\".\")[0]))\n","\n","for filename in filenames:\n","    a = []\n","    file_path = os.path.join(\"./Results/M2_family_epoch_15_r_32_moreData/\", filename)\n","    print(f'Processing file: {file_path}')\n","\n","    with open(file_path, 'r') as file:\n","        content = [line.rstrip('\\n') for line in file]\n","\n","    c = []\n","    aaa = 0\n","    count = 0\n","    for i in range(20):\n","        if content[i].lower().__contains__(family_k3[i]) or content[i].lower().__contains__(num_en[i]):\n","        # if content[i].__contains__(brand[i]):\n","            count += 1\n","\n","    print(f\"{count}/20\")"]},{"cell_type":"markdown","metadata":{"id":"QtbewJVsA6Oo"},"source":["### 3 times"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C8isYnjOA6Oo"},"outputs":[],"source":["import os\n","\n","filenames = []\n","\n","for filename in os.listdir(\"./Results/M1_family_epoch_30_q3/\"):\n","    if filename.endswith(\".txt\"):\n","        filenames.append(filename)\n","\n","filenames.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n","\n","for filename in filenames:\n","    a = []\n","    file_path = os.path.join(\"./Results/M1_family_epoch_30_q3/\", filename)\n","    print(f'Processing file: {file_path}')\n","\n","    with open(file_path, 'r') as file:\n","        content = [line.rstrip('\\n') for line in file]\n","\n","    c = []\n","    aaa = 0\n","    for i in range(20):\n","        count = 0\n","        for j in range(3):\n","            inde = i * 3 + j\n","            if content[inde].lower().__contains__(family_k3[i]) or content[inde].lower().__contains__(num_en[i]):\n","                count += 1\n","                # print(content[inde])\n","        c.append(count)\n","        if count --LORA_R 32 >= 2:\n","            aaa += 1\n","    print(f\"{aaa}/20\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I25oTM0pA6Oo"},"outputs":[],"source":["import os\n","\n","filenames = []\n","\n","for filename in os.listdir(\"./Results/M2_family_epoch_30_q3/\"):\n","    if filename.endswith(\".txt\"):\n","        filenames.append(filename)\n","\n","filenames.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n","\n","for filename in filenames:\n","    a = []\n","    file_path = os.path.join(\"./Results/M2_family_epoch_30_q3/\", filename)\n","    print(f'Processing file: {file_path}')\n","\n","    with open(file_path, 'r') as file:\n","        content = [line.rstrip('\\n') for line in file]\n","\n","    c = []\n","    aaa = 0\n","    for i in range(20):\n","        count = 0\n","        for j in range(3):\n","            inde = i * 3 + j\n","            if content[inde].lower().__contains__(family_k3[i]) or content[inde].lower().__contains__(num_en[i]):\n","                count += 1\n","                # print(content[inde])\n","        c.append(count)\n","        if count --LORA_R 32 >= 2:\n","            aaa += 1\n","    print(f\"{aaa}/20\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeZiMgAqQBZp"},"outputs":[],"source":["# filenames = []\n","\n","# for filename in os.listdir(\"./Results/M1_family_epoch_15_r_32_moreData/\"):\n","#     if filename.endswith(\".txt\"):\n","#         filenames.append(filename)\n","\n","# filenames.sort(key=lambda x: int(x.split('_')[1]))\n","\n","# for filename in filenames:\n","#     a = []\n","#     file_path = os.path.join(\"./Results/M1_family_epoch_15_r_32_moreData/\", filename)\n","#     print(f'Processing file: {file_path}')\n","\n","#     with open(file_path, 'r') as file:\n","#         content = [line.rstrip('\\n') for line in file]\n","\n","#     c = []\n","#     aaa = 0\n","#     for i in range(40):\n","#         count = 0\n","#         for j in range(3):\n","#             inde = i * 3 + j\n","#             if content[inde].lower().__contains__(num[i]) or content[inde].lower().__contains__(num_en[i]):\n","#                 count += 1\n","#         c.append(count)\n","#         if count --LORA_R 32 >= 2:\n","#             aaa += 1\n","#     print(f\"{aaa}/40\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","history_visible":true,"machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00a747c024c54472858b380e81a97b50":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0376b90aec5948aa8810b76c2cff3fbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"095e84efdc3e4bc49954bb5736a88432":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f1810c30baa4dd680f285fb677120aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14a673efa18d40bbaf34412e88c00890":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19d74576b9a147ce98d4d6329520d11a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"223bd8de45a849eca8de2886fa559c04":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b35389507774dab8d7e5fe21948d72b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_095e84efdc3e4bc49954bb5736a88432","placeholder":"​","style":"IPY_MODEL_0f1810c30baa4dd680f285fb677120aa","value":"Your token has been saved in your configured git credential helpers (store)."}},"2d77e9986f86453baa63c9dffd22393e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42362c2d25ba425897c1d309d7a9b859":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4571ed87e0ab49c3882fc8f619ab49ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2bae5fe023c49128552755de5ba6bcf","placeholder":"​","style":"IPY_MODEL_9ec0f3718e5e47bd9b97898072c54292","value":"Login successful"}},"4a399876ed7e41d7ba150b327f2e4512":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"523e485f33a74a8aabcb02dcd6b72291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e06176592b584b9c900ab4beec18fb1d","placeholder":"​","style":"IPY_MODEL_4a399876ed7e41d7ba150b327f2e4512","value":"Your token has been saved to /root/.cache/huggingface/token"}},"5be8606eb49040ee9327d91894dfa678":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_5e78460f61404e57811d2b4b515e0c07","placeholder":"​","style":"IPY_MODEL_c5bab406a62e4ec59773adbd4d0052f2","value":""}},"5e78460f61404e57811d2b4b515e0c07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6863f1fba52441b38d5e5d6413f37a92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7fb9c41cc3f24f75b030392f6afd2c16":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"83bd9ab2c79c49178c3c54d1ae9fb585":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88ae5cd505824f208a7d085711a32154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff9030e6e810422b8ac51d9dcd8d9c82","IPY_MODEL_f25b43bdb5ee42cbbfab0e2a2f5562f8","IPY_MODEL_90b4b44386d24e3eb3f6f6689b194dbf"],"layout":"IPY_MODEL_14a673efa18d40bbaf34412e88c00890"}},"8ab8cf9cd23f429c9c2bcb24376b612b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b4b44386d24e3eb3f6f6689b194dbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96fd7a4258b24b9d8c0769c7cf22e328","placeholder":"​","style":"IPY_MODEL_42362c2d25ba425897c1d309d7a9b859","value":" 4/4 [00:04&lt;00:00,  1.04s/it]"}},"93425b4fd2694f939ccc91725c85a56b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96fd7a4258b24b9d8c0769c7cf22e328":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98e17435921c4d889c5d31d65695bc20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_2d77e9986f86453baa63c9dffd22393e","style":"IPY_MODEL_c49f4d627fda4e1ab69292a0b0895a6b","tooltip":""}},"9d6d9a7423224785808b00d625b93daf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_8ab8cf9cd23f429c9c2bcb24376b612b","style":"IPY_MODEL_e7f216f1893c4e53a3e387cdf86bb8f3","value":true}},"9ec0f3718e5e47bd9b97898072c54292":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f96f9fce7f84fa7910a2ba3cb8a1bdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0376b90aec5948aa8810b76c2cff3fbd","placeholder":"​","style":"IPY_MODEL_83bd9ab2c79c49178c3c54d1ae9fb585","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a7b2f77000c5447284cac89dc6fd2936":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c49f4d627fda4e1ab69292a0b0895a6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"c5bab406a62e4ec59773adbd4d0052f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c832875e93344b7dbe2da02210671949":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00a747c024c54472858b380e81a97b50","placeholder":"​","style":"IPY_MODEL_f27bc90580884f658d1e42c58aea52dd","value":"Connecting..."}},"ce3840a016b4442bb52d42a11a4be97f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e6e9f922e52e4c39b54b8f7ae2810529","IPY_MODEL_2b35389507774dab8d7e5fe21948d72b","IPY_MODEL_523e485f33a74a8aabcb02dcd6b72291","IPY_MODEL_4571ed87e0ab49c3882fc8f619ab49ae"],"layout":"IPY_MODEL_7fb9c41cc3f24f75b030392f6afd2c16"}},"cff4667043ce41729a5c7ebb31e0d709":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5432dc3f2374345b55c68fbe32fdf45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db1a34d2d94b4be2805d7e69ac525b9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e06176592b584b9c900ab4beec18fb1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2bae5fe023c49128552755de5ba6bcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6e9f922e52e4c39b54b8f7ae2810529":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_223bd8de45a849eca8de2886fa559c04","placeholder":"​","style":"IPY_MODEL_db1a34d2d94b4be2805d7e69ac525b9a","value":"Token is valid (permission: write)."}},"e7f216f1893c4e53a3e387cdf86bb8f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f25b43bdb5ee42cbbfab0e2a2f5562f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93425b4fd2694f939ccc91725c85a56b","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6863f1fba52441b38d5e5d6413f37a92","value":4}},"f27bc90580884f658d1e42c58aea52dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd72554b5d8d489db8ab24925478e4ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7b2f77000c5447284cac89dc6fd2936","placeholder":"​","style":"IPY_MODEL_d5432dc3f2374345b55c68fbe32fdf45","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"ff9030e6e810422b8ac51d9dcd8d9c82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19d74576b9a147ce98d4d6329520d11a","placeholder":"​","style":"IPY_MODEL_cff4667043ce41729a5c7ebb31e0d709","value":"Loading checkpoint shards: 100%"}}}}},"nbformat":4,"nbformat_minor":0}
